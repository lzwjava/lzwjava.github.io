<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Neural Netwrok</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Neural Netwrok | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Neural Netwrok" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="es" />
<meta name="description" content="Hablemos directamente del núcleo del funcionamiento de las redes neuronales. Es decir, el algoritmo de retropropagación:" />
<meta property="og:description" content="Hablemos directamente del núcleo del funcionamiento de las redes neuronales. Es decir, el algoritmo de retropropagación:" />
<link rel="canonical" href="https://lzwjava.github.io/neural-network-es" />
<meta property="og:url" content="https://lzwjava.github.io/neural-network-es" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-05-30T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Neural Netwrok" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2023-05-30T00:00:00+08:00","datePublished":"2023-05-30T00:00:00+08:00","description":"Hablemos directamente del núcleo del funcionamiento de las redes neuronales. Es decir, el algoritmo de retropropagación:","headline":"Neural Netwrok","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/neural-network-es"},"url":"https://lzwjava.github.io/neural-network-es"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=4e66ccd5eaa16d3f69bddcabd21279148eda46d8">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=4e66ccd5eaa16d3f69bddcabd21279148eda46d8" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Neural Netwrok | Original, traducido por IA
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="_posts/es/2023-05-30-neural-network-es.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>_postses2023-05-30-neural-network-es.md</span> -->
      

      <!-- <span>2023-05-30-neural-network-es.md</span> -->

      
        

        
          
          <a href="#" class="button">2023.05</a>
        

      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/neural-network-en" >English</option>
        <option value="/neural-network-zh" >中文</option>
        <option value="/neural-network-ja" >日本語</option>
        <option value="/neural-network-es" selected>Español</option>
        <option value="/neural-network-hi" >हिंदी</option>
        <option value="/neural-network-fr" >Français</option>
        <option value="/neural-network-de" >Deutsch</option>
        <option value="/neural-network-ar" >العربية</option>
        <option value="/neural-network-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Hablemos directamente del núcleo del funcionamiento de las redes neuronales. Es decir, el algoritmo de retropropagación:</p>

<ol>
  <li><strong>Entrada x</strong>: Establece la activación correspondiente \(a^{1}\) para la capa de entrada.</li>
  <li><strong>Propagación hacia adelante</strong>: Para cada l=2,3,…,L, calcula \(z^{l} = w^l a^{l-1}+b^l\) y \(a^{l} = \sigma(z^{l})\).</li>
  <li><strong>Error de salida \(\delta^{L}\)</strong>: Calcula el vector \(\delta^{L} = \nabla_a C \odot \sigma'(z^L)\).</li>
  <li><strong>Retropropagación del error</strong>: Para cada l=L−1,L−2,…,2, calcula \(\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\).</li>
  <li><strong>Salida</strong>: El gradiente de la función de costo está dado por \(\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\) y \(\frac{\partial C}{\partial b^l_j} = \delta^l_j\).</li>
</ol>

<p>Esto está copiado del libro <em>Neural Networks and Deep Learning</em> de Michael Nelson. ¿Te parece abrumador? Puede serlo la primera vez que lo ves. Pero no lo será después de un mes de estudio alrededor de ello. Permíteme explicar.</p>

<h2 id="entrada">Entrada</h2>

<p>Hay 5 fases. La primera fase es la Entrada. Aquí utilizamos dígitos escritos a mano como entrada. Nuestra tarea es reconocerlos. Un dígito escrito a mano tiene 784 píxeles, que es 28*28. En cada píxel, hay un valor de escala de grises que va de 0 a 255. La activación significa que utilizamos alguna función para activarlo, para cambiar su valor original a un nuevo valor con el fin de facilitar el procesamiento.</p>

<p>Digamos que ahora tenemos 1000 imágenes de 784 píxeles. Ahora entrenamos el programa para que reconozca qué dígito muestran. Tenemos 100 imágenes para probar ese efecto de aprendizaje. Si el programa puede reconocer los dígitos de 97 imágenes, decimos que su precisión es del 97%.</p>

<p>Entonces, iteraríamos sobre las 1000 imágenes para entrenar los pesos y los sesgos. Hacemos que los pesos y los sesgos sean más correctos cada vez que le damos una nueva imagen para que aprenda.</p>

<p>El resultado de un entrenamiento por lotes debe reflejarse en 10 neuronas. Aquí, las 10 neuronas representan los números del 0 al 9, y su valor varía entre 0 y 1 para indicar su confianza en la precisión de la predicción.</p>

<p>Y la entrada es de 784 neuronas. ¿Cómo podemos reducir 784 neuronas a 10 neuronas? Aquí está la cosa. Supongamos que tenemos dos capas. ¿Qué significa la capa? Esa es la primera capa, tenemos 784 neuronas. En la segunda capa, tenemos 10 neuronas.</p>

<p>Le damos a cada neurona en las 784 neuronas un peso, digamos,</p>

\[w_1, w_2, w_3, w_4, ... , w_{784}\]

<p>Y dale a la primera capa un sesgo, es decir, \(b_1\).</p>

<p>Y así, para la primera neurona en la segunda capa, su valor es:</p>

\[w_1*a_1 + w_2*a_2+...+ w_{784}*a_{784}+b_1\]

<p>Pero estos pesos y un sesgo son para \(neuron^2_{1}\) (el primero en la segunda capa). Para \(neuron^2_{2}\), necesitamos otro conjunto de pesos y un sesgo.</p>

<p>¿Qué tal la función sigmoide? Usamos la función sigmoide para mapear el valor anterior de 0 a 1.</p>

\[\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}
\end{eqnarray}\]

\[\begin{eqnarray} 
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray}\]

<p>También utilizamos la función sigmoide para activar la primera capa. Dicho esto, transformamos ese valor de escala de grises al rango de 0 a 1. Así que ahora, cada neurona en cada capa tiene un valor entre 0 y 1.</p>

<p>Entonces, para nuestra red de dos capas, la primera capa tiene 784 neuronas y la segunda capa tiene 10 neuronas. La entrenamos para obtener los pesos y los sesgos.</p>

<p>Tenemos 784 * 10 pesos y 10 sesgos. En la segunda capa, para cada neurona, utilizaremos 784 pesos y 1 sesgo para calcular su valor. El código aquí es como,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="n">sizes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</code></pre></div></div>

<h2 id="feedforward">Feedforward</h2>

<p>El término <strong>feedforward</strong> se refiere a un proceso en el que la información fluye en una dirección, desde la entrada hacia la salida, sin retroalimentación. Este concepto es ampliamente utilizado en diversos campos, como la ingeniería, la inteligencia artificial y la teoría de control.</p>

<p>En el contexto de las redes neuronales, el <strong>feedforward</strong> es un tipo de arquitectura en la que las señales se mueven en una sola dirección, desde la capa de entrada, a través de las capas ocultas, hasta la capa de salida. Este tipo de red se conoce como <strong>red neuronal feedforward</strong> y es la base de muchas aplicaciones de aprendizaje automático.</p>

<h3 id="ejemplo-de-una-red-neuronal-feedforward">Ejemplo de una Red Neuronal Feedforward</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Definir la función de activación (sigmoide)
</span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Definir la derivada de la función sigmoide
</span><span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Datos de entrada
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Salidas esperadas
</span><span class="n">expected_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Inicializar pesos aleatoriamente
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Tasa de aprendizaje
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Entrenamiento de la red
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># Feedforward
</span>    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>

    <span class="c1"># Cálculo del error
</span>    <span class="n">error</span> <span class="o">=</span> <span class="n">expected_output</span> <span class="o">-</span> <span class="n">outputs</span>

    <span class="c1"># Ajuste de pesos usando el gradiente descendente
</span>    <span class="n">adjustments</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">input_layer</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">adjustments</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Salidas después del entrenamiento:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</code></pre></div></div>

<p>En este ejemplo, la red neuronal feedforward aprende a realizar la operación XOR a través del proceso de entrenamiento. La información fluye en una sola dirección, desde la entrada hasta la salida, sin retroalimentación.</p>

<h3 id="aplicaciones-del-feedforward">Aplicaciones del Feedforward</h3>

<ul>
  <li><strong>Reconocimiento de patrones</strong>: Las redes feedforward son ampliamente utilizadas en tareas de reconocimiento de imágenes y voz.</li>
  <li><strong>Predicción</strong>: Se utilizan para predecir resultados basados en datos de entrada, como en modelos de regresión.</li>
  <li><strong>Clasificación</strong>: Son eficaces en tareas de clasificación, como la detección de spam o la categorización de textos.</li>
</ul>

<p>El feedforward es un concepto fundamental en el diseño de sistemas que requieren un flujo de información unidireccional y es la base de muchas tecnologías modernas.</p>

<blockquote>
  <p>Propagación hacia adelante: Para cada l=2,3,…,L calcula \(z^{l} = w^l a^{l-1}+b^l\) y \(a^{l} = \sigma(z^{l})\)</p>
</blockquote>

<p>Observa aquí que utilizamos el valor de la última capa, es decir, \(a^{l-1}\), junto con el peso de la capa actual, \(w^l\), y su sesgo \(b^l\), para aplicar la función sigmoide y obtener el valor de la capa actual, \(a^{l}\).</p>

<p>Código:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="c1"># propagación hacia adelante
</span>        <span class="n">activation</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> 
        <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span> 
        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
            <span class="n">zs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="error-de-salida">Error de salida</h2>

<blockquote>
  <p>Error de salida \(\delta^{L}\): Calcular el vector \(\delta^{L} = \nabla_a C \odot \sigma'(z^L)\)</p>
</blockquote>

<p>Veamos qué significa \(\nabla\).</p>

<blockquote>
  <p>Del, o nabla, es un operador utilizado en matemáticas (especialmente en cálculo vectorial) como un operador diferencial vectorial, generalmente representado por el símbolo nabla ∇.</p>
</blockquote>

\[\begin{eqnarray}
  w_k &amp; \rightarrow &amp; w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \\
  b_l &amp; \rightarrow &amp; b_l' = b_l-\eta \frac{\partial C}{\partial b_l}
\end{eqnarray}\]

<p>Aquí \(\eta\) es la tasa de aprendizaje. Utilizamos la derivada de C con respecto a los pesos y el sesgo, es decir, la tasa de cambio entre ellos. Esto es <code class="language-plaintext highlighter-rouge">sigmoid_prime</code> en el siguiente código.</p>

<p>Código:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
            <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
        <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
</code></pre></div></div>

<p><em>Nota: El código proporcionado no necesita traducción, ya que es un bloque de código en Python y los nombres de variables y funciones deben mantenerse en inglés para mantener la funcionalidad del programa.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">cost_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">output_activations</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>En este bloque de código, la función <code class="language-plaintext highlighter-rouge">cost_derivative</code> calcula la derivada de la función de costo con respecto a las activaciones de salida. La derivada se obtiene restando el valor objetivo <code class="language-plaintext highlighter-rouge">y</code> de las activaciones de salida <code class="language-plaintext highlighter-rouge">output_activations</code>. Este resultado se utiliza comúnmente en el proceso de retropropagación para ajustar los pesos de la red neuronal.</p>

<h2 id="retropropagar-el-error">Retropropagar el error</h2>

<blockquote>
  <p>Retropropagar el error: Para cada l=L−1,L−2,…,2, calcular \(\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
            <span class="n">sp</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sp</span>
            <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
            <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="salida">Salida</h2>

<blockquote>
  <p>Salida: El gradiente de la función de costo está dado por \(\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\)
y \(\frac{\partial C}{\partial b^l_j} = \delta^l_j\)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">update_mini_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
            <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">nb</span><span class="o">+</span><span class="n">dnb</span> <span class="k">for</span> <span class="n">nb</span><span class="p">,</span> <span class="n">dnb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_b</span><span class="p">)]</span>
            <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">nw</span><span class="o">+</span><span class="n">dnw</span> <span class="k">for</span> <span class="n">nw</span><span class="p">,</span> <span class="n">dnw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_w</span><span class="p">,</span> <span class="n">delta_nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nw</span>
                        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">nw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nb</span>
                       <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">nabla_b</span><span class="p">)]</span>
</code></pre></div></div>

<h2 id="final">Final</h2>

<p>Es un artículo corto. Y en su mayor parte, solo muestra el código y la fórmula matemática. Pero para mí está bien. Antes de escribirlo, no lo entendía claramente. Después de escribirlo o simplemente copiar fragmentos de código y del libro, entiendo la mayor parte. Después de ganar confianza gracias al profesor Yin Wang, leer alrededor del 30% del libro <em>Neural Networks and Deep Learning</em>, escuchar las conferencias de Andrej Karpathy en Stanford y los cursos de Andrew Ng, discutir con mi amigo Qi, y ajustar las bibliotecas de Anaconda, numpy y Theano para hacer funcionar el código de hace años, ahora lo entiendo.</p>

<p>Uno de los puntos clave son las dimensiones. Debemos conocer las dimensiones de cada símbolo y variable. Y simplemente realiza el cálculo diferenciable. Terminemos con las citas de Yin Wang:</p>

<blockquote>
  <p>El aprendizaje automático es realmente útil, incluso se podría decir que es una teoría hermosa, ¡porque simplemente es cálculo después de un cambio de imagen! Es la antigua y gran teoría de Newton y Leibniz, pero en una forma más simple, elegante y poderosa. Básicamente, el aprendizaje automático es el uso del cálculo para derivar y ajustar algunas funciones, y el aprendizaje profundo es el ajuste de funciones más complejas.</p>
</blockquote>

<blockquote>
  <p>No hay ‘inteligencia’ en la inteligencia artificial, no hay ‘neural’ en las redes neuronales, no hay ‘aprendizaje’ en el aprendizaje automático, y no hay ‘profundidad’ en el aprendizaje profundo. No hay ‘profundidad’ en el aprendizaje profundo. Lo que realmente funciona en este campo se llama ‘cálculo’. Por eso prefiero llamar a este campo ‘computación diferenciable’, y el proceso de construir modelos se llama ‘programación diferenciable’.</p>
</blockquote>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-es" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
