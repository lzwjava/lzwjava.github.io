<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>كيف تعمل الشبكات العصبية</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>كيف تعمل الشبكات العصبية | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="كيف تعمل الشبكات العصبية" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="ar" />
<meta name="description" content="لنناقش مباشرة جوهر العمل العصبي. بمعنى آخر، خوارزمية الانتشار العكسي (backpropagation):" />
<meta property="og:description" content="لنناقش مباشرة جوهر العمل العصبي. بمعنى آخر، خوارزمية الانتشار العكسي (backpropagation):" />
<link rel="canonical" href="https://lzwjava.github.io/neural-network-ar" />
<meta property="og:url" content="https://lzwjava.github.io/neural-network-ar" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-05-30T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="كيف تعمل الشبكات العصبية" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2023-05-30T00:00:00+08:00","datePublished":"2023-05-30T00:00:00+08:00","description":"لنناقش مباشرة جوهر العمل العصبي. بمعنى آخر، خوارزمية الانتشار العكسي (backpropagation):","headline":"كيف تعمل الشبكات العصبية","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/neural-network-ar"},"url":"https://lzwjava.github.io/neural-network-ar"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=81a98c6470e973a72fe7e3ac15e34a1310b7d25c">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=81a98c6470e973a72fe7e3ac15e34a1310b7d25c" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       كيف تعمل الشبكات العصبية | أصلي، ترجم بواسطة AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="_posts/ar/2023-05-30-neural-network-ar.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>_postsar2023-05-30-neural-network-ar.md</span> -->
      

      <!-- <span>2023-05-30-neural-network-ar.md</span> -->

      
        

        
          
          <a href="#" class="button">2023.05</a>
        

      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/neural-network-en" >English</option>
        <option value="/neural-network-zh" >中文</option>
        <option value="/neural-network-ja" >日本語</option>
        <option value="/neural-network-es" >Español</option>
        <option value="/neural-network-hi" >हिंदी</option>
        <option value="/neural-network-fr" >Français</option>
        <option value="/neural-network-de" >Deutsch</option>
        <option value="/neural-network-ar" selected>العربية</option>
        <option value="/neural-network-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>لنناقش مباشرة جوهر العمل العصبي. بمعنى آخر، خوارزمية الانتشار العكسي (backpropagation):</p>

<ol>
  <li><strong>إدخال x</strong>: قم بتعيين التنشيط المقابل \(a^{1}\) للطبقة المدخلة.</li>
  <li><strong>التغذية الأمامية</strong>: لكل l=2,3,…,L احسب \(z^{l} = w^l a^{l-1}+b^l\) و \(a^{l} = \sigma(z^{l})\)</li>
  <li><strong>خطأ الإخراج \(\delta^{L}\)</strong>: احسب المتجه \(\delta^{L} = \nabla_a C \odot \sigma'(z^L)\)</li>
  <li><strong>نشر الخطأ للخلف</strong>: لكل l=L−1,L−2,…,2، احسب \(\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\)</li>
  <li><strong>الإخراج</strong>: يتم إعطاء تدرج دالة التكلفة بواسطة \(\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\) و \(\frac{\partial C}{\partial b^l_j} = \delta^l_j\)</li>
</ol>

<p>هذا مأخوذ من كتاب مايكل نيلسون <em>الشبكات العصبية والتعلم العميق</em>. هل يبدو هذا مربكًا؟ قد يكون كذلك في المرة الأولى التي تراه فيها. لكنه لن يكون كذلك بعد شهر من الدراسة حوله. دعني أشرح.</p>

<h2 id="الإدخال">الإدخال</h2>

<p>هناك 5 مراحل. المرحلة الأولى هي <strong>الإدخال</strong>. هنا نستخدم الأرقام المكتوبة بخط اليد كمدخلات. مهمتنا هي التعرف عليها. كل رقم مكتوب بخط اليد يحتوي على 784 بكسل، أي 28*28. في كل بكسل، هناك قيمة تدرج رمادي تتراوح من 0 إلى 255. لذا، فإن التفعيل يعني أننا نستخدم بعض الدوال لتفعيلها، لتغيير قيمتها الأصلية إلى قيمة جديدة لتسهيل المعالجة.</p>

<p>لنفترض أن لدينا الآن 1000 صورة، كل صورة تحتوي على 784 بكسل. نقوم الآن بتدريب البرنامج على التعرف على الرقم الذي تعرضه هذه الصور. لدينا الآن 100 صورة لاختبار تأثير هذا التعلم. إذا استطاع البرنامج التعرف على أرقام 97 صورة، نقول إن دقته هي 97%.</p>

<p>لذلك سنقوم بالتكرار على 1000 صورة، لتدريب الأوزان والانحيازات. نجعل الأوزان والانحيازات أكثر دقة في كل مرة نعطيها صورة جديدة لتتعلم منها.</p>

<p>نتيجة تدريب دفعة واحدة يجب أن تنعكس على 10 خلايا عصبية. هنا، تمثل الخلايا العصبية العشرة الأرقام من 0 إلى 9، وقيمتها تتراوح من 0 إلى 1 للإشارة إلى مدى ثقتها في دقتها.</p>

<p>والإدخال يتكون من 784 خلية عصبية. كيف يمكننا تقليل 784 خلية عصبية إلى 10 خلايا عصبية؟ إليكم الأمر. لنفترض أن لدينا طبقتين. ماذا تعني الطبقة؟ هذه هي الطبقة الأولى، لدينا فيها 784 خلية عصبية. وفي الطبقة الثانية، لدينا 10 خلايا عصبية.</p>

<p>نعطي كل خلية عصبية في الـ784 خلية عصبية وزنًا، لنقل،</p>

\[w_1, w_2, w_3, w_4, ... , w_{784}\]

<p>وأعطِ الطبقة الأولى تحيزًا، أي \(b_1\).</p>

<p>وبالتالي، بالنسبة للعصبون الأول في الطبقة الثانية، تكون قيمته:</p>

\[w_1*a_1 + w_2*a_2+...+ w_{784}*a_{784}+b_1\]

<p>لكن هذه الأوزان والانحياز هي لـ \(neuron^2_{1}\) (العصبون الأول في الطبقة الثانية). بالنسبة لـ \(neuron^2_{2}\)، نحتاج إلى مجموعة أخرى من الأوزان والانحياز.</p>

<p>ماذا عن دالة السيجمويد؟ نستخدم دالة السيجمويد لتحويل القيمة السابقة إلى نطاق من 0 إلى 1.</p>

\[\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}
\end{eqnarray}\]

\[\begin{eqnarray} 
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray}\]

<p>نستخدم أيضًا دالة السيجمويد (sigmoid) لتفعيل الطبقة الأولى. بهذا، نقوم بتحويل قيمة التدرج الرمادي إلى نطاق من 0 إلى 1. لذا الآن، كل عصبون في كل طبقة له قيمة تتراوح من 0 إلى 1.</p>

<p>إذن الآن بالنسبة لشبكتنا المكونة من طبقتين، تحتوي الطبقة الأولى على 784 خلية عصبية، والطبقة الثانية تحتوي على 10 خلايا عصبية. نقوم بتدريبها للحصول على الأوزان والتحيزات.</p>

<p>لدينا 784 * 10 أوزان و10 انحيازات. في الطبقة الثانية، لكل خلية عصبية، سنستخدم 784 وزنًا وانحيازًا واحدًا لحساب قيمتها. الكود هنا يشبه،</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="n">sizes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</code></pre></div></div>

<h2 id="التغذية-الأمامية-feedforward">التغذية الأمامية (Feedforward)</h2>

<blockquote>
  <p>التغذية الأمامية: لكل ( l = 2, 3, \ldots, L ) احسب<br />
[ z^{l} = w^l a^{l-1} + b^l ]<br />
و<br />
[ a^{l} = \sigma(z^{l}) ]</p>
</blockquote>

<p>لاحظ هنا أننا نستخدم قيمة الطبقة الأخيرة، وهي \(a^{l-1}\)، بالإضافة إلى وزن الطبقة الحالية \(w^l\) والانحياز الخاص بها \(b^l\)، لإجراء دالة السيجمويد للحصول على قيمة الطبقة الحالية \(a^{l}\).</p>

<p>الكود:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="c1"># التغذية الأمامية
</span>        <span class="n">activation</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> 
        <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span> 
        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
            <span class="n">zs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="خطأ-الإخراج">خطأ الإخراج</h2>

<blockquote>
  <p>خطأ الإخراج \(\delta^{L}\): حساب المتجه \(\delta^{L} = \nabla_a C \odot \sigma'(z^L)\)</p>
</blockquote>

<p>لنرى ما يعنيه الرمز \(\nabla\).</p>

<blockquote>
  <p>دِل، أو نابلا، هو عامل يستخدم في الرياضيات (وخاصة في حساب المتجهات) كعامل تفاضلي متجه، وعادة ما يتم تمثيله بالرمز نابلا ∇.</p>
</blockquote>

\[\begin{eqnarray}
  w_k &amp; \rightarrow &amp; w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \\
  b_l &amp; \rightarrow &amp; b_l' = b_l-\eta \frac{\partial C}{\partial b_l}
\end{eqnarray}\]

<p>هنا \(\eta\) هو معدل التعلم. نستخدم المشتقة التي تربط C بالأوزان والانحياز، وهي معدل التغير بينهما. وهذا هو <code class="language-plaintext highlighter-rouge">sigmoid_prime</code> في الأسفل.</p>

<p>الكود:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
            <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
        <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
</code></pre></div></div>

<p>تم ترجمة الكود أعلاه إلى العربية كما يلي:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
            <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
        <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
</code></pre></div></div>

<p>ملاحظة: الكود لم يتم ترجمته لأنه يحتوي على أسماء دوال ومتغيرات بالإنجليزية، وهي مصطلحات تقنية تُستخدم كما هي في البرمجة.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">cost_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">output_activations</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>في الكود أعلاه، يتم تعريف دالة تُسمى <code class="language-plaintext highlighter-rouge">cost_derivative</code> تأخذ مُدخلين: <code class="language-plaintext highlighter-rouge">output_activations</code> و <code class="language-plaintext highlighter-rouge">y</code>. الدالة تُرجع الفرق بين <code class="language-plaintext highlighter-rouge">output_activations</code> و <code class="language-plaintext highlighter-rouge">y</code>. هذا الكود يُستخدم عادةً في سياق الشبكات العصبية لحساب مشتقة دالة التكلفة بالنسبة إلى مُخرجات الطبقة الأخيرة.</p>

<h2 id="نشر-الخطأ-للخلف-backpropagate-the-error">نشر الخطأ للخلف (Backpropagate the error)</h2>

<blockquote>
  <p>انتشار الخطأ للخلف: لكل طبقة ( l = L-1, L-2, \ldots, 2 )، احسب 
\(\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
            <span class="n">sp</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sp</span>
            <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
            <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>ملاحظة:</strong> الكود أعلاه مكتوب بلغة Python ويستخدم مكتبة NumPy. لا يتم ترجمة الكود البرمجي، حيث أنه يجب أن يبقى كما هو لضمان استمرارية العمل بشكل صحيح.</p>

<h2 id="الإخراج">الإخراج</h2>

<blockquote>
  <p>الناتج: يتم إعطاء تدرج دالة التكلفة بواسطة
\(\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\)
و
\(\frac{\partial C}{\partial b^l_j} = \delta^l_j\)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">update_mini_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
            <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">nb</span><span class="o">+</span><span class="n">dnb</span> <span class="k">for</span> <span class="n">nb</span><span class="p">,</span> <span class="n">dnb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_b</span><span class="p">)]</span>
            <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">nw</span><span class="o">+</span><span class="n">dnw</span> <span class="k">for</span> <span class="n">nw</span><span class="p">,</span> <span class="n">dnw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_w</span><span class="p">,</span> <span class="n">delta_nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nw</span>
                        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">nw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nb</span>
                       <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">nabla_b</span><span class="p">)]</span>
</code></pre></div></div>

<h2 id="النهائي">النهائي</h2>

<p>إنها مقالة قصيرة. وفي معظمها، تعرض فقط الكود والصيغ الرياضية. ولكن هذا جيد بالنسبة لي. قبل كتابتها، لم أكن أفهمها بوضوح. بعد الكتابة أو مجرد نسخ المقاطع من الكود والكتاب، فهمت معظمها. بعد اكتساب الثقة من المعلم Yin Wang، وقراءة حوالي 30% من كتاب <em>الشبكات العصبية والتعلم العميق</em>، والاستماع إلى محاضرات Andrej Karpathy في ستانفورد ودورات Andrew Ng، والمناقشة مع صديقي Qi، والتعديل على مكتبات Anaconda وnumpy وTheano لجعل الكود يعمل منذ سنوات، فهمتها الآن.</p>

<p>إحدى النقاط الرئيسية هي الأبعاد. يجب أن نعرف أبعاد كل رمز ومتغير. وهي تقوم فقط بالحسابات القابلة للتفاضل. لننهي باقتباس من Yin Wang:</p>

<blockquote>
  <p>تعلم الآلة مفيد حقًا، بل قد يقول البعض إنه نظرية جميلة، لأنه ببساطة حساب التفاضل والتكامل بعد إعادة صياغة! إنها النظرية القديمة العظيمة لنيوتن ولايبنيز، ولكن بشكل أبسط وأنيق وقوي. تعلم الآلة هو في الأساس استخدام حساب التفاضل والتكامل لاشتقاق وتناسب بعض الدوال، بينما التعلم العميق هو تناسب دوال أكثر تعقيدًا.</p>
</blockquote>

<blockquote>
  <p>لا يوجد ‘ذكاء’ في الذكاء الاصطناعي، ولا ‘عصبي’ في الشبكات العصبية، ولا ‘تعلم’ في تعلم الآلة، ولا ‘عمق’ في التعلم العميق. ما يعمل حقًا في هذا المجال يُسمى ‘حساب التفاضل’. لذلك أفضل أن أسمي هذا المجال ‘الحوسبة التفاضلية’، وعملية بناء النماذج تُسمى ‘البرمجة التفاضلية’.</p>
</blockquote>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-ar" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
