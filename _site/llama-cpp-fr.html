<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Essayer llama.cpp</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Essayer llama.cpp | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Essayer llama.cpp" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="fr" />
<meta name="description" content="llama.cpp" />
<meta property="og:description" content="llama.cpp" />
<link rel="canonical" href="https://lzwjava.github.io/llama-cpp-fr" />
<meta property="og:url" content="https://lzwjava.github.io/llama-cpp-fr" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-01-25T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Essayer llama.cpp" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2025-01-25T00:00:00+08:00","datePublished":"2025-01-25T00:00:00+08:00","description":"llama.cpp","headline":"Essayer llama.cpp","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/llama-cpp-fr"},"url":"https://lzwjava.github.io/llama-cpp-fr"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=f02b3274dc91ac8765e796987e3a52da5d886bd7">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=f02b3274dc91ac8765e796987e3a52da5d886bd7" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Essayer llama.cpp | Original, traduit par l'IA
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="_posts/fr/2025-01-25-llama-cpp-fr.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>_postsfr2025-01-25-llama-cpp-fr.md</span> -->
      

      <!-- <span>2025-01-25-llama-cpp-fr.md</span> -->

      
        

        
          
          <a href="#" class="button">2025.01</a>
        

      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/llama-cpp-en" >English</option>
        <option value="/llama-cpp-zh" >中文</option>
        <option value="/llama-cpp-ja" >日本語</option>
        <option value="/llama-cpp-es" >Español</option>
        <option value="/llama-cpp-hi" >हिंदी</option>
        <option value="/llama-cpp-fr" selected>Français</option>
        <option value="/llama-cpp-de" >Deutsch</option>
        <option value="/llama-cpp-ar" >العربية</option>
        <option value="/llama-cpp-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h2 id="llamacpp">llama.cpp</h2>

<p>Lors de l’exécution de <code class="language-plaintext highlighter-rouge">llama.cpp</code> avec un modèle, vous pourriez rencontrer une erreur comme celle-ci :</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>% ./main <span class="nt">-m</span> models/7B/Phi-3-mini-4k-instruct-q4.gguf
main: build <span class="o">=</span> 964 <span class="o">(</span>f3c3b4b<span class="o">)</span>
main: seed  <span class="o">=</span> 1737736417
llama.cpp: loading model from models/7B/Phi-3-mini-4k-instruct-q4.gguf
error loading model: unknown <span class="o">(</span>magic, version<span class="o">)</span> combination: 46554747, 00000003<span class="p">;</span> is this really a GGML file?
llama_load_model_from_file: failed to load model
llama_init_from_gpt_params: error: failed to load model <span class="s1">'models/7B/Phi-3-mini-4k-instruct-q4.gguf'</span>
main: error: unable to load model
</code></pre></div></div>

<p>Cette erreur se produit parce que vous exécutez le programme <code class="language-plaintext highlighter-rouge">main</code>. L’exécution des programmes <code class="language-plaintext highlighter-rouge">llama-cli</code> ou <code class="language-plaintext highlighter-rouge">llama-server</code> situés sous <code class="language-plaintext highlighter-rouge">build/bin</code> devrait résoudre le problème.</p>

<p>Le programme <code class="language-plaintext highlighter-rouge">main</code> a été créé le 8 août 2023, ce qui signifie qu’il n’est pas la version actuelle.</p>

<p>Une autre solution consiste à installer <code class="language-plaintext highlighter-rouge">llama.cpp</code> en utilisant Homebrew :</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>llama.cpp
</code></pre></div></div>

<p>Cela garantit que vous disposez d’une version compatible de la bibliothèque.</p>

<h2 id="servir-le-modèle">Servir le Modèle</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/lzw/Projects/llama.cpp/build/bin/llama-server <span class="nt">-m</span> /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-Distill-Qwen-14B-Q5_K_M.gguf <span class="nt">--port</span> 8000  <span class="nt">--ctx-size</span> 2048 <span class="nt">--batch-size</span> 512 <span class="nt">--n-gpu-layers</span> 49 <span class="nt">--threads</span> 8 <span class="nt">--parallel</span> 1
</code></pre></div></div>

<p>Les paramètres <code class="language-plaintext highlighter-rouge">--ctx-size 2048 --batch-size 512 --n-gpu-layers 49 --threads 8 --parallel 1</code> sont importants. Ils augmenteront la vitesse.</p>

<h2 id="llm-farm">LLM Farm</h2>

<p>C’est une excellente application iOS. Dans les paramètres, il y a environ 20 modèles. Lors de l’importation d’un modèle GGUF par nous-mêmes, qui sont téléchargés depuis Hugging Face, cela peut entraîner un plantage.</p>

<h2 id="avantages">Avantages</h2>

<p>L’auto-hébergement de ces modèles LLM vous permet de les exécuter localement sans avoir besoin d’un accès réseau. Par exemple, lors du téléchargement de grands fichiers qui congestionnent le réseau, l’exécution d’un modèle local peut être bénéfique.</p>

<h2 id="ressources">Ressources</h2>

<ul>
  <li><a href="https://huggingface.co/ggml-org?sort_models=downloads#models">Modèles GGML de Hugging Face</a></li>
  <li><a href="https://github.com/ggerganov/llama.cpp">Dépôt GitHub de llama.cpp</a></li>
  <li><a href="https://github.com/ggerganov/ggml">Dépôt GitHub de ggml</a></li>
  <li><a href="https://ollama.com">Ollama</a></li>
  <li><a href="https://github.com/kevinhermawan/Ollamac">Ollamac</a></li>
</ul>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-fr" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
