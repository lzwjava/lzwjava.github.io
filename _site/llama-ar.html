<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Nvidia Driver، LLaMA و ChatGPT</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Nvidia Driver، LLaMA و ChatGPT | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Nvidia Driver، LLaMA و ChatGPT" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="ar" />
<meta name="description" content="LLaMA (نموذج اللغة الكبيرة Meta AI) هي عائلة من نماذج اللغة الكبيرة (LLMs)، تم إصدارها بواسطة Meta AI بدءًا من فبراير 2023." />
<meta property="og:description" content="LLaMA (نموذج اللغة الكبيرة Meta AI) هي عائلة من نماذج اللغة الكبيرة (LLMs)، تم إصدارها بواسطة Meta AI بدءًا من فبراير 2023." />
<link rel="canonical" href="https://lzwjava.github.io/llama-ar" />
<meta property="og:url" content="https://lzwjava.github.io/llama-ar" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-18T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Nvidia Driver، LLaMA و ChatGPT" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2023-08-18T00:00:00+08:00","datePublished":"2023-08-18T00:00:00+08:00","description":"LLaMA (نموذج اللغة الكبيرة Meta AI) هي عائلة من نماذج اللغة الكبيرة (LLMs)، تم إصدارها بواسطة Meta AI بدءًا من فبراير 2023.","headline":"Nvidia Driver، LLaMA و ChatGPT","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/llama-ar"},"url":"https://lzwjava.github.io/llama-ar"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=5401deb2b2b9c21cc97f9940a3d32e2005cc24be">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=5401deb2b2b9c21cc97f9940a3d32e2005cc24be" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Nvidia Driver، LLaMA و ChatGPT | أصلي، ترجم بواسطة AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="_posts/ar/2023-08-18-llama-ar.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>_postsar2023-08-18-llama-ar.md</span> -->
      

      <!-- <span>2023-08-18-llama-ar.md</span> -->

      
        

        
          
          <a href="#" class="button">2023.08</a>
        

      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/llama-en" >English</option>
        <option value="/llama-zh" >中文</option>
        <option value="/llama-ja" >日本語</option>
        <option value="/llama-es" >Español</option>
        <option value="/llama-hi" >हिंदी</option>
        <option value="/llama-fr" >Français</option>
        <option value="/llama-de" >Deutsch</option>
        <option value="/llama-ar" selected>العربية</option>
        <option value="/llama-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>LLaMA (نموذج اللغة الكبيرة Meta AI) هي عائلة من نماذج اللغة الكبيرة (LLMs)، تم إصدارها بواسطة Meta AI بدءًا من فبراير 2023.</p>

<p>لقد قمت مؤخرًا ببناء جهاز الكمبيوتر الخاص بي مع وحدة معالجة الرسومات من Nvidia. يمكنك التحقق من ذلك هنا، كيفية بناء جهاز كمبيوتر، <a href="https://lzwjava.github.io/computer">https://lzwjava.github.io/computer</a>.</p>

<p>بعد ذلك، بدأت في تشغيل مشروع LLaMA. عنوان GitHub لمشروع LLaMA هو <a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a>.</p>

<h2 id="تثبيت-برنامج-تشغيل-nvidia">تثبيت برنامج تشغيل Nvidia</h2>

<p>عندما تقوم بتنفيذ الأمر،</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torchrun</span> <span class="o">--</span><span class="n">nproc_per_node</span> <span class="mi">1</span> <span class="n">example_text_completion</span><span class="p">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">ckpt_dir</span> <span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">/</span> \
    <span class="o">--</span><span class="n">tokenizer_path</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">model</span> \
    <span class="o">--</span><span class="n">max_seq_len</span> <span class="mi">128</span> <span class="o">--</span><span class="n">max_batch_size</span> <span class="mi">4</span>
</code></pre></div></div>

<p>يظهر الخطأ التالي: “RuntimeError: الحزمة الموزعة لا تحتوي على NCCL مدمجًا”. دعونا نتعرف على NCCL.</p>

<blockquote>
  <p>مكتبة الاتصال الجماعي من NVIDIA (NCCL) تنفذ بدائيات الاتصال متعددة وحدات معالجة الرسوميات (GPUs) ومتعددة العقد (multi-node) المُحسنة لوحدات معالجة الرسوميات من NVIDIA والشبكات.
أنا أشير إلى المواقع التالية لتثبيت برامج تشغيل NVIDIA.</p>
</blockquote>

<ul>
  <li>تنزيلات CUDA Toolkit 12.2 التحديث 1، <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></li>
  <li>NVIDIA NCCL، <a href="https://developer.nvidia.com/nccl">https://developer.nvidia.com/nccl</a></li>
  <li>وثائق NVIDIA Deep Learning NCCL، <a href="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html">https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html</a></li>
  <li>دليل تثبيت NVIDIA CUDA لنظام Linux، <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></li>
  <li>بعد تثبيت Ubuntu، تواجه إدارة MOK، <a href="https://www.cnblogs.com/yutian-blogs/p/13019226.html">https://www.cnblogs.com/yutian-blogs/p/13019226.html</a></li>
  <li>Ubuntu 22.04 للتعلم العميق، <a href="https://gist.github.com/amir-saniyan/b3d8e06145a8569c0d0e030af6d60bea">https://gist.github.com/amir-saniyan/b3d8e06145a8569c0d0e030af6d60bea</a></li>
  <li>ملاحظات Ubuntu 22.04، <a href="https://github.com/kmcminn/thinkpad/tree/main/extreme3g">https://github.com/kmcminn/thinkpad/tree/main/extreme3g</a></li>
</ul>

<p>عندما نقوم بتثبيت برنامج تشغيل NVIDIA بنجاح لبطاقة الرسومات الخاصة بنا، ثم نستخدم الأمر <code class="language-plaintext highlighter-rouge">nvidia-smi</code> لعرض تفاصيلها، يمكن أن تظهر المعلومات التالية.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) lzw@lzw-MS-7E01:~$ nvidia-smi
الخميس 17 أغسطس 2023 04:15:43       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.10              إصدار السائق: 535.86.10    إصدار CUDA: 12.2         |
|-----------------------------------------+----------------------+----------------------+
| GPU  الاسم                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| المروحة  الحرارة   الأداء          الطاقة: الاستخدام/السعة |         استخدام الذاكرة | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        تشغيل  | 00000000:01:00.0  تشغيل |                  N/A |
|  0%   34C    P8               9W / 215W |    666MiB / 12282MiB |     15%      افتراضي |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| العمليات:                                                                            |
|  GPU   GI   CI        PID   النوع   اسم العملية                            استخدام ذاكرة GPU |
|        ID   ID                                                             الاستخدام      |
|=======================================================================================|
|    0   N/A  N/A      1926      G   /usr/lib/xorg/Xorg                          381MiB |
|    0   N/A  N/A      2065      G   /usr/bin/gnome-shell                        120MiB |
|    0   N/A  N/A      3482      G   gnome-control-center                          2MiB |
|    0   N/A  N/A      3803      G   ...irefox/2987/usr/lib/firefox/firefox      149MiB |
+---------------------------------------------------------------------------------------+
</code></pre></div></div>

<p>في الواقع، من الصعب الوصول إلى هذه المرحلة. يرجى الرجوع إلى الرابط هنا بعناية، ملاحظات Ubuntu 22.04، <a href="https://github.com/kmcminn/thinkpad/tree/main/extreme3g">https://github.com/kmcminn/thinkpad/tree/main/extreme3g</a>.</p>

<h2 id="تعلم-llama">تعلم LLaMA</h2>

<p>LLaMA (Large Language Model Meta AI) هي مجموعة من نماذج اللغة الكبيرة التي طورتها Meta (المعروفة سابقًا باسم Facebook). هذه النماذج مصممة لتكون فعالة من حيث الحجم وقادرة على أداء مهام معالجة اللغة الطبيعية (NLP) بشكل جيد. إذا كنت ترغب في تعلم المزيد عن LLaMA وكيفية استخدامها، إليك بعض الخطوات التي يمكنك اتباعها:</p>

<h3 id="1-فهم-أساسيات-llama">1. <strong>فهم أساسيات LLaMA</strong></h3>
<ul>
  <li>ابدأ بقراءة الأوراق البحثية والمستندات الرسمية التي تصف LLaMA. سيساعدك هذا على فهم كيفية عمل النموذج وما هي المهام التي يمكنه القيام بها.</li>
  <li>تعرف على الفرق بين LLaMA والنماذج اللغوية الأخرى مثل GPT وBERT.</li>
</ul>

<h3 id="2-إعداد-البيئة">2. <strong>إعداد البيئة</strong></h3>
<ul>
  <li>تأكد من أن لديك بيئة برمجية مناسبة لتشغيل النماذج اللغوية. يمكنك استخدام Python مع مكتبات مثل PyTorch أو TensorFlow.</li>
  <li>قم بتثبيت المكتبات اللازمة مثل <code class="language-plaintext highlighter-rouge">transformers</code> من Hugging Face والتي تدعم تشغيل نماذج LLaMA.</li>
</ul>

<h3 id="3-تنزيل-النموذج">3. <strong>تنزيل النموذج</strong></h3>
<ul>
  <li>يمكنك تنزيل نماذج LLaMA من مصادر موثوقة مثل Hugging Face Model Hub.</li>
  <li>تأكد من أن لديك مساحة تخزين كافية وحساب GPU إذا كنت تخطط لتشغيل النموذج على جهازك.</li>
</ul>

<h3 id="4-التدريب-والضبط">4. <strong>التدريب والضبط</strong></h3>
<ul>
  <li>إذا كنت ترغب في تحسين أداء النموذج لمهمة معينة، يمكنك ضبط النموذج (fine-tuning) باستخدام مجموعة بيانات خاصة بك.</li>
  <li>استخدم أدوات مثل <code class="language-plaintext highlighter-rouge">Trainer</code> من Hugging Face لتسهيل عملية التدريب.</li>
</ul>

<h3 id="5-التقييم-والاختبار">5. <strong>التقييم والاختبار</strong></h3>
<ul>
  <li>بعد تدريب النموذج، قم بتقييم أدائه باستخدام مقاييس مثل الدقة (accuracy) أو F1-score.</li>
  <li>قم باختبار النموذج على بيانات جديدة لضمان أنه يعمل بشكل جيد في العالم الحقيقي.</li>
</ul>

<h3 id="6-التطبيقات-العملية">6. <strong>التطبيقات العملية</strong></h3>
<ul>
  <li>استخدم LLaMA في تطبيقات عملية مثل توليد النصوص، الترجمة الآلية، الإجابة على الأسئلة، وغيرها.</li>
  <li>يمكنك أيضًا دمج LLaMA في تطبيقاتك الخاصة باستخدام واجهات برمجية (APIs).</li>
</ul>

<h3 id="7-الموارد-الإضافية">7. <strong>الموارد الإضافية</strong></h3>
<ul>
  <li>انضم إلى مجتمعات المطورين والمهتمين بالذكاء الاصطناعي على منصات مثل GitHub وReddit وDiscord.</li>
  <li>تابع الدورات التعليمية والبرامج التعليمية المتاحة على الإنترنت لتعميق فهمك.</li>
</ul>

<p>باتباع هذه الخطوات، ستتمكن من تعلم LLaMA واستخدامها بشكل فعال في مشاريعك الخاصة.</p>

<p>بعد تنزيل النماذج، ومحاولة تشغيل الأمر، سنواجه الخطأ التالي،</p>

<blockquote>
  <p>torch.cuda.OutOfMemoryError: نفاد ذاكرة CUDA. تمت محاولة تخصيص 86.00 ميجابايت (GPU 0؛ سعة إجمالية 11.69 جيجابايت؛ تم تخصيص 9.70 جيجابايت بالفعل؛ 64.81 ميجابايت متاحة؛ 9.70 جيجابايت محجوزة إجمالًا بواسطة PyTorch). إذا كانت الذاكرة المحجوزة أكبر بكثير من الذاكرة المخصصة، حاول ضبط <code class="language-plaintext highlighter-rouge">max_split_size_mb</code> لتجنب التجزئة.</p>
</blockquote>

<p>نظرًا لأن ذاكرة بطاقة الرسوميات لدينا هي 12 جيجابايت فقط، وحجم نموذج llama-2-7b يبلغ حوالي 13 جيجابايت، فإننا لا نستطيع تشغيله باستخدام بطاقة الرسوميات الخاصة بنا.</p>

<p>نحاول استخدام المشروع الآخر، open-llama-3b، <a href="https://huggingface.co/openlm-research/open_llama_3b">https://huggingface.co/openlm-research/open_llama_3b</a>.</p>

<p>نواجه الخطأ التالي:</p>

<blockquote>
  <p>RuntimeError: توقع أن تكون جميع الموترات على نفس الجهاز، ولكن تم العثور على جهازين على الأقل، cuda:0 و cpu! (عند التحقق من الوسيط index في الدالة wrapper_CUDA__index_select)</p>
</blockquote>

<p>ونحن نسأل ChatGPT عن هذا.</p>

<p>يوفر لنا ChatGPT إصلاحًا جميلًا جدًا. نحتاج إلى إضافة الكود التالي.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>تم تحويل <code class="language-plaintext highlighter-rouge">input_ids</code> إلى الجهاز الذي يعمل عليه النموذج (<code class="language-plaintext highlighter-rouge">model.device</code>).</p>

<p>أخيرًا، يمكننا تشغيله.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llama) lzw@lzw-MS-7E01:~/Projects/open_llama_3b$ python run.py 
س: ما هو أكبر حيوان؟   
ج: الحوت الأزرق.   
س: ما هو أكبر حيوان؟    
ج: الحوت الأزرق. إنه أكبر حيوان على وجه الأرض. وهو أيضًا  
</code></pre></div></div>

<p>لقد جربنا أيضًا المشروع llama2.c، <a href="https://github.com/karpathy/llama2.c">https://github.com/karpathy/llama2.c</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) lzw@lzw-MS-7E01:~/Projects/llama2.c$ ./run stories15M.bin 
في يوم من الأيام، كان هناك أسد كبير وأرنب صغير في الغابة. قال الأسد الكبير: "لنتسابق إلى النهر ونشرب بعض الماء." وافق الأرنب الصغير، وبدأوا في السباق.
كان الأسد سريعًا، ولكنه كان حريصًا على ألا يؤذي الأرنب الصغير. كان الأرنب يقترب من النهر، لكن الأسد الكبير لم يره. عندما عبر الأسد النهر، انزلق وسقط في النهر.
رأى الأرنب الصغير الأسد في الماء وفكر: "يجب أن أخبر الأسد ألا يشرب." قال: "يا أسد، لا تشرب من النهر. إنه ليس آمنًا." لكن الأسد لم يستمع وشرب من النهر.
حاول الأرنب الصغير مساعدة الأسد، لكنه لم يستطع. شرب الأسد كثيرًا وبدأت معدته تؤلمه. كان الأسد الكبير حزينًا جدًا وقال: "لقد أخبرتك ألا تشرب من النهر." ذهب الأرنب الصغير إلى المنزل وهو يبكي، وما زال الأسد الكبير غير قادر على المساعدة.
تحقيق tok/s: 185.527329
</code></pre></div></div>

<h2 id="فهم-الكود-بواسطة-chatgpt">فهم الكود بواسطة ChatGPT</h2>

<p>Zhiwei: هل يمكنك شرح الأكواد التالية؟</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">max_gen_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">echo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]]:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">params</span>
        <span class="n">bsz</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">bsz</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_prompt_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>
<span class="n">max_prompt_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">max_prompt_len</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span>
<span class="n">total_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_gen_len</span> <span class="o">+</span> <span class="n">max_prompt_len</span><span class="p">)</span>
</code></pre></div></div>

<p>تم ترجمة الكود أعلاه إلى:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_prompt_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>  <span class="c1"># الحد الأدنى لطول النص المطلوب
</span><span class="n">max_prompt_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>  <span class="c1"># الحد الأقصى لطول النص المطلوب
</span><span class="k">assert</span> <span class="n">max_prompt_len</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span>  <span class="c1"># التأكد من أن الحد الأقصى لا يتجاوز الطول المسموح به
</span><span class="n">total_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_gen_len</span> <span class="o">+</span> <span class="n">max_prompt_len</span><span class="p">)</span>  <span class="c1"># الطول الكلي المقبول
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pad_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_id</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">total_len</span><span class="p">),</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">):</span>
    <span class="n">tokens</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
    <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prev_pos</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">eos_reached</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">input_text_mask</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">!=</span> <span class="n">pad_id</span>
<span class="k">for</span> <span class="n">cur_pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_prompt_len</span><span class="p">,</span> <span class="n">total_len</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span><span class="p">:</span><span class="n">cur_pos</span><span class="p">],</span> <span class="n">prev_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
        <span class="n">token_logprobs</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">target</span><span class="o">=</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">reduction</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="n">pad_id</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">sample_top_p</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># استبدال الرمز فقط إذا تم إنشاء النص المطلوب بالفعل
</span>            <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">next_token</span>
            <span class="p">)</span>
            <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span>
            <span class="n">eos_reached</span> <span class="o">|=</span> <span class="p">(</span><span class="o">~</span><span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span>
                <span class="n">next_token</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span>
            <span class="p">)</span>
            <span class="n">prev_pos</span> <span class="o">=</span> <span class="n">cur_pos</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">eos_reached</span><span class="p">):</span>
                <span class="k">break</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
            <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">token_logprobs</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">toks</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="c1"># قص إلى الحد الأقصى لطول التوليد
</span>            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">echo</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">toks</span> <span class="o">=</span> <span class="n">toks</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">max_gen_len</span><span class="p">]</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">token_logprobs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">start</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">max_gen_len</span><span class="p">]</span>
            <span class="c1"># قص إلى رمز نهاية الجملة إذا وجد
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span> <span class="ow">in</span> <span class="n">toks</span><span class="p">:</span>
                <span class="n">eos_idx</span> <span class="o">=</span> <span class="n">toks</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span><span class="p">)</span>
                <span class="n">toks</span> <span class="o">=</span> <span class="n">toks</span><span class="p">[:</span><span class="n">eos_idx</span><span class="p">]</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:</span><span class="n">eos_idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">logprobs</span> <span class="k">else</span> <span class="bp">None</span>
            <span class="n">out_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">toks</span><span class="p">)</span>
            <span class="n">out_logprobs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="k">if</span> <span class="n">logprobs</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>بالطبع، إليك شرحًا للكود المقدم:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">max_gen_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">echo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]]:</span>
</code></pre></div></div>

<p>هذه الوظيفة تُسمى <code class="language-plaintext highlighter-rouge">generate</code> وتقبل عدة معاملات:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt_tokens</code> هي قائمة تحتوي على تسلسلات من الرموز (tokens) للطلبات (prompts).</li>
  <li><code class="language-plaintext highlighter-rouge">max_gen_len</code> هو الطول الأقصى للنص الذي يتم إنشاؤه.</li>
  <li><code class="language-plaintext highlighter-rouge">temperature</code> هي معلمة تُستخدم للتحكم في العشوائية أثناء عملية الإنشاء.</li>
  <li><code class="language-plaintext highlighter-rouge">top_p</code> هو عتبة احتمالية بين 0 و1 تُستخدم للتحكم في التنوع أثناء عملية أخذ العينات.</li>
  <li><code class="language-plaintext highlighter-rouge">logprobs</code> هي قيمة منطقية (boolean) تشير إلى ما إذا كان يجب تسجيل اللوغاريتمات الاحتمالية (log probabilities) لعملية الإنشاء.</li>
  <li><code class="language-plaintext highlighter-rouge">echo</code> هي قيمة منطقية (boolean) تشير إلى ما إذا كان يجب تكرار المدخلات أثناء عملية الإنشاء.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">params</span>
<span class="n">bsz</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">bsz</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></div>

<p>تم ترجمة الكود أعلاه إلى:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">params</span>
<span class="n">bsz</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">bsz</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></div>

<p>ملاحظة: الكود لم يتم ترجمته لأنه يحتوي على أسماء متغيرات ودوال بالإنجليزية، والتي عادةً ما تبقى كما هي في البرمجة.</p>

<p>هنا، تقوم الدالة بالحصول على معلمات النموذج وحساب حجم الدُفعة (batch size) للطلبات. تضمن الدالة أن حجم الدُفعة لا يتجاوز الحد الأقصى لحجم الدُفعة المحدد في معلمات النموذج.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_prompt_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>  <span class="c1"># الحد الأدنى لطول النص المطلوب
</span><span class="n">max_prompt_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>  <span class="c1"># الحد الأقصى لطول النص المطلوب
</span><span class="k">assert</span> <span class="n">max_prompt_len</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span>  <span class="c1"># التأكد من أن الحد الأقصى لا يتجاوز الطول المسموح به
</span><span class="n">total_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_gen_len</span> <span class="o">+</span> <span class="n">max_prompt_len</span><span class="p">)</span>  <span class="c1"># الطول الكلي المقبول
</span></code></pre></div></div>

<p>هذه الأسطر تحسب الحد الأدنى والحد الأقصى لأطوال النصوص المقدمة (prompts) وتضمن ألا يتجاوز الحد الأقصى لطول النص المقدم الحد الأقصى لطول التسلسل المحدد في معلمات النموذج. يتم حساب الطول الإجمالي بناءً على الطول الأقصى الذي يمكن إنشاؤه والطول الأقصى للنص المقدم.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pad_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_id</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">total_len</span><span class="p">),</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">):</span>
    <span class="n">tokens</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
</code></pre></div></div>

<p>يقوم هذا القسم بتهيئة موتر <code class="language-plaintext highlighter-rouge">tokens</code> برموز الحشو (padding tokens) ويملؤه بتسلسلات الرموز (token sequences) المستخرجة من النصوص المقدمة (prompts).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
    <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p>إذا كانت <code class="language-plaintext highlighter-rouge">logprobs</code> تساوي <code class="language-plaintext highlighter-rouge">True</code>، يتم إنشاء موتر <code class="language-plaintext highlighter-rouge">token_logprobs</code> لتخزين اللوغاريتمات الاحتمالية.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prev_pos</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">eos_reached</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">input_text_mask</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">!=</span> <span class="n">pad_id</span>
</code></pre></div></div>

<p>تم ترجمة الكود أعلاه إلى:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prev_pos</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">eos_reached</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">input_text_mask</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">!=</span> <span class="n">pad_id</span>
</code></pre></div></div>

<p>ملاحظة: الكود لم يتم ترجمته لأنه يحتوي على أسماء متغيرات ودوال بلغة Python، والتي لا يتم ترجمتها عادةً.</p>

<p>يتم تهيئة المتغيرات <code class="language-plaintext highlighter-rouge">prev_pos</code> (الموضع السابق)، <code class="language-plaintext highlighter-rouge">eos_reached</code> (ما إذا تم الوصول إلى رموز نهاية التسلسل)، و <code class="language-plaintext highlighter-rouge">input_text_mask</code> (المواضع التي لم يتم ملؤها بالحشو).</p>

<p>الحلقة التالية تولد الرموز بدءًا من الحد الأدنى لطول المطالبة وصولاً إلى الطول الكلي.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">cur_pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_prompt_len</span><span class="p">,</span> <span class="n">total_len</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span><span class="p">:</span><span class="n">cur_pos</span><span class="p">],</span> <span class="n">prev_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
        <span class="n">token_logprobs</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">target</span><span class="o">=</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">reduction</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="n">pad_id</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">sample_top_p</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">next_token</span>
    <span class="p">)</span>
    <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span>
    <span class="n">eos_reached</span> <span class="o">|=</span> <span class="p">(</span><span class="o">~</span><span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span>
        <span class="n">next_token</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span>
    <span class="p">)</span>
    <span class="n">prev_pos</span> <span class="o">=</span> <span class="n">cur_pos</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">eos_reached</span><span class="p">):</span>
        <span class="k">break</span>
</code></pre></div></div>

<p>تمت ترجمة الكود أعلاه إلى:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">next_token</span>
    <span class="p">)</span>
    <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span>
    <span class="n">eos_reached</span> <span class="o">|=</span> <span class="p">(</span><span class="o">~</span><span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span>
        <span class="n">next_token</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span>
    <span class="p">)</span>
    <span class="n">prev_pos</span> <span class="o">=</span> <span class="n">cur_pos</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">eos_reached</span><span class="p">):</span>
        <span class="k">break</span>
</code></pre></div></div>

<p>ملاحظة: الكود لم يتم ترجمته لأنه يحتوي على أسماء ودوال محددة في لغة البرمجة Python والتي لا يتم ترجمتها عادةً.</p>

<p>داخل هذه الحلقة، يقوم النموذج بإنشاء توزيع احتمالي <code class="language-plaintext highlighter-rouge">logits</code> للرمز التالي. إذا كان <code class="language-plaintext highlighter-rouge">logprobs</code> صحيحًا، فإنه يحسب اللوغاريتمات الاحتمالية باستخدام الانتروبيا المتقاطعة. اعتمادًا على <code class="language-plaintext highlighter-rouge">temperature</code>، يستخدم softmax أو argmax لتحديد الرمز التالي. ثم يقوم بتحديث موتر <code class="language-plaintext highlighter-rouge">tokens</code> ويفحص رموز نهاية التسلسل.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
    <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">token_logprobs</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div></div>

<p>إذا كانت <code class="language-plaintext highlighter-rouge">logprobs</code> تساوي <code class="language-plaintext highlighter-rouge">True</code>، يتم تحويل موتر احتمالات اللوغاريتمات إلى قائمة Python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">toks</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">.</span><span class="n">tolist</span><span class="p">()):</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>في هذه الحلقة، لكل عينة إدخال، يتم توليد تسلسل الرموز الناتج والاحتمالات اللوغاريتمية المرتبطة بها.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="p">(</span><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="k">if</span> <span class="n">logprobs</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>أخيرًا، تُعيد الدالة تسلسلات الرموز المُنشأة والاحتمالات اللوغاريتمية المقابلة لها (إذا تم طلب ذلك).</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-ar" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
