<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>神经网络是如何工作的</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>神经网络是如何工作的 | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="神经网络是如何工作的" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="zh" />
<meta name="description" content="让我们直接讨论神经系统工作的核心。也就是说，反向传播算法：" />
<meta property="og:description" content="让我们直接讨论神经系统工作的核心。也就是说，反向传播算法：" />
<link rel="canonical" href="https://lzwjava.github.io/neural-network-zh" />
<meta property="og:url" content="https://lzwjava.github.io/neural-network-zh" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-05-30T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="神经网络是如何工作的" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2023-05-30T00:00:00+08:00","datePublished":"2023-05-30T00:00:00+08:00","description":"让我们直接讨论神经系统工作的核心。也就是说，反向传播算法：","headline":"神经网络是如何工作的","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/neural-network-zh"},"url":"https://lzwjava.github.io/neural-network-zh"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=4078a2582799c9fd02a1981696d374189b201186">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=4078a2582799c9fd02a1981696d374189b201186" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       神经网络是如何工作的 | 原创，AI翻译
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="_posts/zh/2023-05-30-neural-network-zh.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>_postszh2023-05-30-neural-network-zh.md</span> -->
      

      <!-- <span>2023-05-30-neural-network-zh.md</span> -->

      
        

        
          
          <a href="#" class="button">2023.05</a>
        

      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/neural-network-en" >English</option>
        <option value="/neural-network-zh" selected>中文</option>
        <option value="/neural-network-ja" >日本語</option>
        <option value="/neural-network-es" >Español</option>
        <option value="/neural-network-hi" >हिंदी</option>
        <option value="/neural-network-fr" >Français</option>
        <option value="/neural-network-de" >Deutsch</option>
        <option value="/neural-network-ar" >العربية</option>
        <option value="/neural-network-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>让我们直接讨论神经系统工作的核心。也就是说，反向传播算法：</p>

<ol>
  <li>输入x： 为输入层设置相应的激活值$a^{1}$。</li>
  <li>前馈式： 对于每个$l=2,3,…,L$计算$z^{l} = w^l a^{l-1}+b^l$和$a^{l} = \sigma(z^{l})$</li>
  <li>输出误差 $\delta^{L}$： 计算矢量 $\delta^{L} = \nabla_a C \odot \sigma’(z^L)$</li>
  <li>反向传播误差： 对于每个$l=L-1,L-2,…,2$，计算$\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^{l})$</li>
  <li>输出： 成本函数的梯度由 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ 和 $\frac{\partial C}{\partial b^l_j} = \delta^l_j $ 给出。
这是从迈克尔-尼尔森的《神经网络与深度学习》一书中抄来的。这是不是让人不知所措？在你第一次看到它时可能是这样。但在围绕它研究了一个月后，就不会了。让我解释一下。</li>
</ol>

<h2 id="输入">输入</h2>

<p>一共有5个阶段。第一阶段是输入。这里我们用手写的数字作为输入。我们的任务是识别它们。一个手写的数字有784个像素，即28*28。在每个像素中，都有一个灰度值，范围是0到255。因此，激活意味着我们使用一些函数来激活它，把它的原始值变成一个新的值，以便于处理。</p>

<p>例如，我们现在有1000张784像素的图片。我们现在训练它来识别它们所显示的数字。我们现在有100张图片来测试这个学习效果。如果程序能够识别97张图片的数字，我们就说它的准确率是97%。</p>

<p>因此，我们将在这1000张图片中进行循环，以训练出权重和偏差。每当我们给它一张新的图片来学习时，我们就会使权重和偏置更加正确。</p>

<p>一个批次的训练结果将反映在10个神经元中。这里，10个神经元代表从0到9，其数值范围是0到1，以表示他们对其准确性的信心。</p>

<p>而输入是784个神经元。我们怎样才能将784个神经元减少到10个神经元？事情是这样的。让我们假设我们有两个层。这个层是什么意思？那就是第一层，我们有784个神经元。在第二层，我们有10个神经元。</p>

<p>我们给784个神经元中的每个神经元一个权重，比如、</p>

<p>$w_1, w_2, w_3, w_4, … , w_{784}$</p>

<p>并给第一层一个偏置，即$b_1$。</p>

<p>于是对于第二层的第一个神经元，它的值是：</p>

<p>$w_1<em>a_1 + w_2</em>a_2+…+ w_{784}*a_{784}+b_1$</p>

<p>但这些权重和偏置是针对$neuron^2<em>{1}$(第二层的第一个)的。对于$neuron^2</em>{2}$来说，我们需要另一组权重和一个偏置。</p>

<p>那么，sigmoid函数呢？我们用sigmoid函数把上面的值从0映射到1。</p>

<p>$
\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}
\end{eqnarray}
$</p>

<p>$
\begin{eqnarray} 
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray}
$</p>

<p>我们也使用sigmoid函数来激活第一层。也就是说，我们把这个灰度值改为从0到1的范围。所以现在，每一层的每个神经元都有一个从0到1的值。</p>

<p>所以现在对于我们的两层网络，第一层有784个神经元，第二层有10个神经元。我们训练它以获得权重和偏置。</p>

<p>我们有784*10个权重和10个偏置。在第二层，对于每个神经元，我们将使用784个权重和1个偏置来计算其值。这里的代码像、</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="n">sizes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</code></pre></div></div>

<h2 id="前馈">前馈</h2>

<blockquote>
  <p>前馈： 对于每个l=2,3,…,L计算$z^{l} = w^l a^{l-1}+b^l$和$a^{l} = \sigma(z^{l})$</p>
</blockquote>

<p>注意这里，我们使用最后一层的值，即$a^{l-1}$和当前层的权重$w^l$及其偏置$b^l$来做sigmoid，得到当前层的值，$a^{l}$。</p>

<p>代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="c1"># feedforward
</span>        <span class="n">activation</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> 
        <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span> 
        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
            <span class="n">zs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="输出错误">输出错误</h2>

<blockquote>
  <p>输出误差 $\delta^{L}$： 计算矢量 $\delta^{L} = \nabla_a C \odot \sigma’(z^L)$</p>
</blockquote>

<p>让我们看看$\nabla$意味着什么。</p>

<blockquote>
  <p>Del，或称nabla，是数学中（特别是在矢量微积分中）使用的一种矢量微分算子，通常用nabla符号∇表示。</p>
</blockquote>

<p>$
\begin{eqnarray}
  w_k &amp; \rightarrow &amp; w_k’ = w_k-\eta \frac{\partial C}{\partial w_k} <br />
  b_l &amp; \rightarrow &amp; b_l’ = b_l-\eta \frac{\partial C}{\partial b_l}
\end{eqnarray}
$</p>

<p>这里$\eta$是学习率。我们使用的导数是C与权重和偏差各自的导数，也就是它们之间的速率变化。这就是下面的<code class="language-plaintext highlighter-rouge">sigmoid_prime</code>。</p>

<p>代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
            <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
        <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">cost_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">output_activations</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="反向传播误差">反向传播误差</h2>

<blockquote>
  <p>反向传播误差： 对于每个l=L-1,L-2,…,2，计算$\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^{l})$</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
            <span class="n">sp</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sp</span>
            <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
            <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">transpose</span><span class="p">())</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="输出">输出</h2>

<blockquote>
  <p>输出： 成本函数的梯度由 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ 和 $\frac{\partial C}{\partial b^l_j} = \delta^l_j $ 给出。</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">update_mini_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
            <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">nb</span><span class="o">+</span><span class="n">dnb</span> <span class="k">for</span> <span class="n">nb</span><span class="p">,</span> <span class="n">dnb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_b</span><span class="p">)]</span>
            <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">nw</span><span class="o">+</span><span class="n">dnw</span> <span class="k">for</span> <span class="n">nw</span><span class="p">,</span> <span class="n">dnw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_w</span><span class="p">,</span> <span class="n">delta_nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nw</span>
                        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">nw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nb</span>
                       <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">nabla_b</span><span class="p">)]</span>
</code></pre></div></div>

<h2 id="最后">最后</h2>

<p>这是一篇短文。而且在大多数段落里，它只是显示了代码和数学公式。但对我来说，这足够了。在写这篇文章之前，我不太明白。在写完或只是复制了代码和书中的片段后，我明白了大部分内容。在从王垠老师那里获得信心，阅读了<em>神经网络和深度学习</em>这本书的大约30%，听了Andrej Karpathy的斯坦福讲座和Andrew Ng的部分课程，与我的朋友Qi讨论，捣鼓Anaconda、numpy和Theano库使书上多年前的代码能够工作，现在我明白了。</p>

<p>其中的一个关键点是维度。我们应该知道每个符号和变量的维度。而整个代码只是做可微分的计算。让我们以王垠的引言结束吧：</p>

<blockquote>
  <p>机器学习真的很有用，甚至可以说是优美的理论，因为它根本就是改头换面之后的微积分！它是牛顿，莱布尼兹古老而伟大的理论，以更加简单，优雅而强大的形式出现。机器学习基本就是利用微积分求导，拟合一些函数，深度学习就是拟合更加复杂的函数。</p>
</blockquote>

<blockquote>
  <p>人工智能里面没有什么“智能”，神经网络里面也没有什么“神经”，机器学习里面也没有什么“学习”，深度学习里面也没有什么“深度”。这里面真正有效的东西，叫做“微积分”。所以我宁愿把这个领域叫做“可求导计算”（differentiable computing），构建模型的过程叫做“可求导编程”（differentiable programming）。</p>
</blockquote>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-zh" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
