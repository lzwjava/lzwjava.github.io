<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Nvidiaドライバー、LLaMA、そしてChatGPT</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Nvidiaドライバー、LLaMA、そしてChatGPT | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Nvidiaドライバー、LLaMA、そしてChatGPT" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="ja" />
<meta name="description" content="LLaMA（Large Language Model Meta AI）は、2023年2月からMeta AIによってリリースされた大規模言語モデル（LLM）のファミリーです。" />
<meta property="og:description" content="LLaMA（Large Language Model Meta AI）は、2023年2月からMeta AIによってリリースされた大規模言語モデル（LLM）のファミリーです。" />
<link rel="canonical" href="https://lzwjava.github.io/llama-ja" />
<meta property="og:url" content="https://lzwjava.github.io/llama-ja" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-18T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Nvidiaドライバー、LLaMA、そしてChatGPT" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2023-08-18T00:00:00+08:00","datePublished":"2023-08-18T00:00:00+08:00","description":"LLaMA（Large Language Model Meta AI）は、2023年2月からMeta AIによってリリースされた大規模言語モデル（LLM）のファミリーです。","headline":"Nvidiaドライバー、LLaMA、そしてChatGPT","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/llama-ja"},"url":"https://lzwjava.github.io/llama-ja"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=4bf9fa7886d2770858f4b8abbb5a24ea72bc774c">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=4bf9fa7886d2770858f4b8abbb5a24ea72bc774c" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Nvidiaドライバー、LLaMA、そしてChatGPT | オリジナル、AI翻訳
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="_posts/ja/2023-08-18-llama-ja.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>_postsja2023-08-18-llama-ja.md</span> -->
      

      <!-- <span>2023-08-18-llama-ja.md</span> -->

      
        

        
          
          <a href="#" class="button">2023.08</a>
        

      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/llama-en" >English</option>
        <option value="/llama-zh" >中文</option>
        <option value="/llama-ja" selected>日本語</option>
        <option value="/llama-es" >Español</option>
        <option value="/llama-hi" >हिंदी</option>
        <option value="/llama-fr" >Français</option>
        <option value="/llama-de" >Deutsch</option>
        <option value="/llama-ar" >العربية</option>
        <option value="/llama-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>LLaMA（Large Language Model Meta AI）は、2023年2月からMeta AIによってリリースされた大規模言語モデル（LLM）のファミリーです。</p>

<p>最近、Nvidia GPUを搭載したコンピュータを組み立てました。詳細はこちらをご覧ください、コンピュータの組み立て方、<a href="https://lzwjava.github.io/computer">https://lzwjava.github.io/computer</a>。</p>

<p>その後、LLaMAプロジェクトを実行し始めました。LLaMAプロジェクトのGitHub URLは<a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a>です。</p>

<h2 id="nvidiaドライバのインストール">Nvidiaドライバのインストール</h2>

<p>コマンドを実行すると、</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torchrun</span> <span class="o">--</span><span class="n">nproc_per_node</span> <span class="mi">1</span> <span class="n">example_text_completion</span><span class="p">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">ckpt_dir</span> <span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">/</span> \
    <span class="o">--</span><span class="n">tokenizer_path</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">model</span> \
    <span class="o">--</span><span class="n">max_seq_len</span> <span class="mi">128</span> <span class="o">--</span><span class="n">max_batch_size</span> <span class="mi">4</span>
</code></pre></div></div>

<p>このコードは、<code class="language-plaintext highlighter-rouge">torchrun</code>を使用して1つのプロセスで<code class="language-plaintext highlighter-rouge">example_text_completion.py</code>スクリプトを実行するコマンドです。以下のオプションが指定されています：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">--nproc_per_node 1</code>: 1つのノードで1つのプロセスを実行します。</li>
  <li><code class="language-plaintext highlighter-rouge">--ckpt_dir llama-2-7b/</code>: チェックポイントディレクトリとして<code class="language-plaintext highlighter-rouge">llama-2-7b/</code>を指定します。</li>
  <li><code class="language-plaintext highlighter-rouge">--tokenizer_path tokenizer.model</code>: トークナイザーモデルのパスとして<code class="language-plaintext highlighter-rouge">tokenizer.model</code>を指定します。</li>
  <li><code class="language-plaintext highlighter-rouge">--max_seq_len 128</code>: 最大シーケンス長を128に設定します。</li>
  <li><code class="language-plaintext highlighter-rouge">--max_batch_size 4</code>: 最大バッチサイズを4に設定します。</li>
</ul>

<p>エラーが表示されます：「RuntimeError: Distributed package doesn’t have NCCL built in」。NCCLについて学びましょう。</p>

<blockquote>
  <p>NVIDIA Collective Communication Library (NCCL) は、NVIDIA GPUとネットワーキング向けに最適化されたマルチGPUおよびマルチノード通信プリミティブを実装しています。
以下のウェブサイトを参照して、NVIDIAドライバーをインストールしました。</p>
</blockquote>

<ul>
  <li>CUDA Toolkit 12.2 Update 1 ダウンロード, <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></li>
  <li>NVIDIA NCCL, <a href="https://developer.nvidia.com/nccl">https://developer.nvidia.com/nccl</a></li>
  <li>NVIDIA Deep Learning NCCL ドキュメント, <a href="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html">https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html</a></li>
  <li>NVIDIA CUDA Linux インストールガイド, <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></li>
  <li>Ubuntu インストール後に Perform MOK Management が表示される場合, <a href="https://www.cnblogs.com/yutian-blogs/p/13019226.html">https://www.cnblogs.com/yutian-blogs/p/13019226.html</a></li>
  <li>ディープラーニング用 Ubuntu 22.04, <a href="https://gist.github.com/amir-saniyan/b3d8e06145a8569c0d0e030af6d60bea">https://gist.github.com/amir-saniyan/b3d8e06145a8569c0d0e030af6d60bea</a></li>
  <li>Ubuntu 22.04 メモ, <a href="https://github.com/kmcminn/thinkpad/tree/main/extreme3g">https://github.com/kmcminn/thinkpad/tree/main/extreme3g</a></li>
</ul>

<p>NVIDIAドライバーをグラフィックカードに正常にインストールした後、<code class="language-plaintext highlighter-rouge">nvidia-smi</code>コマンドを使用してその詳細を表示すると、以下の情報が表示されます。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) lzw@lzw-MS-7E01:~$ nvidia-smi
2023年8月17日 04:15:43       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.10              ドライバーバージョン: 535.86.10    CUDAバージョン: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  名前                 持続性-M | バス-ID        ディスプレイ.A | 揮発性未修正ECC |
| ファン  温度   パフォーマンス         電力:使用量/上限 |         メモリ使用量 | GPU使用率  計算モード |
|                                         |                      |              MIGモード |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        オン | 00000000:01:00.0 オン |                  N/A |
|  0%   34℃    P8               9W / 215W |    666MiB / 12282MiB |     15%      デフォルト |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| プロセス:                                                                             |
|  GPU   GI   CI        PID   タイプ   プロセス名                            GPUメモリ |
|        ID   ID                                                             使用量      |
|=======================================================================================|
|    0   N/A  N/A      1926      G   /usr/lib/xorg/Xorg                          381MiB |
|    0   N/A  N/A      2065      G   /usr/bin/gnome-shell                        120MiB |
|    0   N/A  N/A      3482      G   gnome-control-center                          2MiB |
|    0   N/A  N/A      3803      G   ...irefox/2987/usr/lib/firefox/firefox      149MiB |
+---------------------------------------------------------------------------------------+
</code></pre></div></div>

<p>実際、この段階に到達するのは難しいです。こちらのリンク、Ubuntu 22.04 Notes、<a href="https://github.com/kmcminn/thinkpad/tree/main/extreme3g">https://github.com/kmcminn/thinkpad/tree/main/extreme3g</a> を注意深く参照してください。</p>

<h2 id="llamaを学ぶ">LLaMAを学ぶ</h2>

<p>モデルをダウンロードし、コマンドを実行しようとすると、以下のエラーが発生します。</p>

<blockquote>
  <p>torch.cuda.OutOfMemoryError: CUDAのメモリ不足です。86.00 MiBの割り当てを試みました（GPU 0; 合計容量 11.69 GiB; 9.70 GiBが既に割り当て済み; 64.81 MiBが空き; PyTorchによって合計9.70 GiBが予約されています）。予約メモリが割り当てメモリを大幅に上回る場合、フラグメンテーションを防ぐためにmax_split_size_mbを設定してみてください。</p>
</blockquote>

<p>私たちのグラフィックカードのメモリは12GBしかなく、llama-2-7bモデルのサイズは約13GBであるため、このグラフィックカードでは実行できません。</p>

<p>私たちは、別のプロジェクトであるopen-llama-3b、<a href="https://huggingface.co/openlm-research/open_llama_3b">https://huggingface.co/openlm-research/open_llama_3b</a>を使用してみます。</p>

<p>以下のエラーが発生しました。</p>

<blockquote>
  <p>RuntimeError: すべてのテンソルが同じデバイス上にあることが期待されましたが、少なくとも2つのデバイス、cuda:0とcpuが見つかりました！（メソッドwrapper_CUDA__index_selectの引数indexをチェックしている際に発生）</p>
</blockquote>

<p>そして、これについてChatGPTに尋ねます。</p>

<p>ChatGPTは非常に美しい修正を提供してくれました。以下のコードを追加する必要があります。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input_ids = input_ids.to(model.device)
</code></pre></div></div>

<p>最後に、それを実行することができます。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llama) lzw@lzw-MS-7E01:~/Projects/open_llama_3b$ python run.py 
Q: 最も大きな動物は何ですか？   
A: シロナガスクジラです。   
Q: 最も大きな動物は何ですか？    
A: シロナガスクジラです。地球上で最も大きな動物です。また、  
</code></pre></div></div>

<p>また、プロジェクト「llama2.c」も試してみました。<a href="https://github.com/karpathy/llama2.c">https://github.com/karpathy/llama2.c</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) lzw@lzw-MS-7E01:~/Projects/llama2.c$ ./run stories15M.bin 
ある日、大きなライオンと小さなウサギが森にいました。大きなライオンは言いました。「川まで競争して、水を飲もうよ。」小さなウサギは同意し、彼らは競争を始めました。
ライオンは速かったですが、小さなウサギを傷つけないように気をつけていました。ウサギは川に近づいていましたが、大きなライオンは彼を見ていませんでした。ライオンが川を渡ったとき、滑って川に落ちてしまいました。
小さなウサギは水の中のライオンを見て、「ライオンに飲まないように言うべきだ」と思いました。彼は言いました。「ライオン、川の水を飲んではいけない。危ないよ。」しかし、ライオンは聞かずに川の水を飲みました。
小さなウサギはライオンを助けようとしましたが、できませんでした。ライオンは飲みすぎてお腹が痛くなりました。大きなライオンはとても悲しんで言いました。「川の水を飲むなと言ったのに。」小さなウサギは泣きながら家に帰り、大きなライオンはまだ助けられませんでした。
achieved tok/s: 185.527329
</code></pre></div></div>

<h2 id="chatgptでコードを理解する">ChatGPTでコードを理解する</h2>

<p>Zhiwei: 以下のコードを説明していただけますか？</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">max_gen_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">echo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]]:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">params</span>
        <span class="n">bsz</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">bsz</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_prompt_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>
<span class="n">max_prompt_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">max_prompt_len</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span>
<span class="n">total_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_gen_len</span> <span class="o">+</span> <span class="n">max_prompt_len</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pad_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_id</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">total_len</span><span class="p">),</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">):</span>
    <span class="n">tokens</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
    <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prev_pos</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">eos_reached</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">input_text_mask</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">!=</span> <span class="n">pad_id</span>
<span class="k">for</span> <span class="n">cur_pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_prompt_len</span><span class="p">,</span> <span class="n">total_len</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span><span class="p">:</span><span class="n">cur_pos</span><span class="p">],</span> <span class="n">prev_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
        <span class="n">token_logprobs</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">target</span><span class="o">=</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">reduction</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="n">pad_id</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">sample_top_p</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># プロンプトがすでに生成されている場合のみトークンを置換
</span>            <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">next_token</span>
            <span class="p">)</span>
            <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span>
            <span class="n">eos_reached</span> <span class="o">|=</span> <span class="p">(</span><span class="o">~</span><span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span>
                <span class="n">next_token</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span>
            <span class="p">)</span>
            <span class="n">prev_pos</span> <span class="o">=</span> <span class="n">cur_pos</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">eos_reached</span><span class="p">):</span>
                <span class="k">break</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
            <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">token_logprobs</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">toks</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="c1"># 最大生成長に切り詰める
</span>            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">echo</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">toks</span> <span class="o">=</span> <span class="n">toks</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">max_gen_len</span><span class="p">]</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">token_logprobs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">start</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">max_gen_len</span><span class="p">]</span>
            <span class="c1"># もしeosトークンがあればそこで切り詰める
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span> <span class="ow">in</span> <span class="n">toks</span><span class="p">:</span>
                <span class="n">eos_idx</span> <span class="o">=</span> <span class="n">toks</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span><span class="p">)</span>
                <span class="n">toks</span> <span class="o">=</span> <span class="n">toks</span><span class="p">[:</span><span class="n">eos_idx</span><span class="p">]</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:</span><span class="n">eos_idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">logprobs</span> <span class="k">else</span> <span class="bp">None</span>
            <span class="n">out_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">toks</span><span class="p">)</span>
            <span class="n">out_logprobs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="k">if</span> <span class="n">logprobs</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>ChatGPT: もちろん、以下に提供されたコードの説明をします:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">max_gen_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">echo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]]:</span>
</code></pre></div></div>

<p>この関数は、指定されたプロンプトトークンに基づいてテキストを生成するためのものです。以下に各パラメータの説明を示します：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt_tokens</code>: 生成の基となるプロンプトのトークンリスト。リストのリストとして渡されます。</li>
  <li><code class="language-plaintext highlighter-rouge">max_gen_len</code>: 生成するテキストの最大長。</li>
  <li><code class="language-plaintext highlighter-rouge">temperature</code>: 生成時のサンプリングの温度。値が高いほどランダム性が高くなります。</li>
  <li><code class="language-plaintext highlighter-rouge">top_p</code>: トップpサンプリングのパラメータ。確率の累積がこの値に達するまでのトークンのみを考慮します。</li>
  <li><code class="language-plaintext highlighter-rouge">logprobs</code>: 生成されたトークンの対数確率を返すかどうか。</li>
  <li><code class="language-plaintext highlighter-rouge">echo</code>: プロンプトを出力に含めるかどうか。</li>
</ul>

<p>この関数は、生成されたトークンのリストと、必要に応じて対数確率のリストを返します。</p>

<p>この関数は<code class="language-plaintext highlighter-rouge">generate</code>という名前で、いくつかのパラメータを受け取ります:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt_tokens</code> は、プロンプトのトークンシーケンスを含むリストです。</li>
  <li><code class="language-plaintext highlighter-rouge">max_gen_len</code> は、生成されるテキストの最大長です。</li>
  <li><code class="language-plaintext highlighter-rouge">temperature</code> は、生成中のランダム性を制御するためのパラメータです。</li>
  <li><code class="language-plaintext highlighter-rouge">top_p</code> は、サンプリング中の多様性を制御するために使用される0から1の間の確率閾値です。</li>
  <li><code class="language-plaintext highlighter-rouge">logprobs</code> は、生成プロセスの対数確率を記録するかどうかを示すブール値です。</li>
  <li><code class="language-plaintext highlighter-rouge">echo</code> は、生成中に入力をエコーするかどうかを示すブール値です。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">params</span>
<span class="n">bsz</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">bsz</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></div>

<p>このコードスニペットでは、<code class="language-plaintext highlighter-rouge">params</code>にモデルのパラメータを代入し、<code class="language-plaintext highlighter-rouge">bsz</code>にプロンプトトークンの数を代入しています。その後、<code class="language-plaintext highlighter-rouge">assert</code>文を使用して、バッチサイズ（<code class="language-plaintext highlighter-rouge">bsz</code>）がモデルの最大バッチサイズ（<code class="language-plaintext highlighter-rouge">params.max_batch_size</code>）を超えていないことを確認しています。もし超えていた場合、エラーが発生します。</p>

<p>ここで、関数はモデルのパラメータを取得し、プロンプトのバッチサイズを計算します。この関数は、バッチサイズがモデルパラメータで指定された最大バッチサイズを超えないようにします。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_prompt_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>
<span class="n">max_prompt_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">max_prompt_len</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span>
<span class="n">total_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_gen_len</span> <span class="o">+</span> <span class="n">max_prompt_len</span><span class="p">)</span>
</code></pre></div></div>

<p>このコードスニペットでは、以下の処理を行っています：</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">min_prompt_len</code> に <code class="language-plaintext highlighter-rouge">prompt_tokens</code> 内の最小のトークン長を代入します。</li>
  <li><code class="language-plaintext highlighter-rouge">max_prompt_len</code> に <code class="language-plaintext highlighter-rouge">prompt_tokens</code> 内の最大のトークン長を代入します。</li>
  <li><code class="language-plaintext highlighter-rouge">max_prompt_len</code> が <code class="language-plaintext highlighter-rouge">params.max_seq_len</code> 以下であることを確認します（アサーション）。</li>
  <li><code class="language-plaintext highlighter-rouge">total_len</code> に <code class="language-plaintext highlighter-rouge">params.max_seq_len</code> と <code class="language-plaintext highlighter-rouge">max_gen_len + max_prompt_len</code> のうち小さい方を代入します。</li>
</ol>

<p>このコードは、プロンプトのトークン長を確認し、生成するシーケンスの長さを制限するために使用されます。</p>

<p>これらの行は、プロンプトの最小および最大長を計算し、プロンプトの最大長がモデルパラメータで指定された最大シーケンス長を超えないようにします。総長は、生成される最大長とプロンプトの最大長に基づいて計算されます。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pad_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_id</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">total_len</span><span class="p">),</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">):</span>
    <span class="n">tokens</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
</code></pre></div></div>

<p>このコードは、PyTorchを使用して、与えられたプロンプトトークンをパディングされたテンソルに変換する処理を行っています。以下に各ステップの説明を日本語で示します。</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">pad_id = self.tokenizer.pad_id</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">pad_id</code> は、トークナイザーが定義するパディングトークンのIDを取得します。パディングトークンは、シーケンスの長さを揃えるために使用されます。</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device="cuda")</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">tokens</code> は、バッチサイズ <code class="language-plaintext highlighter-rouge">bsz</code> とシーケンスの総長 <code class="language-plaintext highlighter-rouge">total_len</code> を持つテンソルを作成します。このテンソルは、すべての要素が <code class="language-plaintext highlighter-rouge">pad_id</code> で初期化され、データ型は <code class="language-plaintext highlighter-rouge">torch.long</code>、デバイスは <code class="language-plaintext highlighter-rouge">cuda</code>（GPU）に設定されます。</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">for k, t in enumerate(prompt_tokens):</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">prompt_tokens</code> は、各プロンプトのトークンリストを含むリストです。このループでは、各プロンプトトークンリスト <code class="language-plaintext highlighter-rouge">t</code> とそのインデックス <code class="language-plaintext highlighter-rouge">k</code> を取得します。</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device="cuda")</code>:
    <ul>
      <li>各プロンプトトークンリスト <code class="language-plaintext highlighter-rouge">t</code> を <code class="language-plaintext highlighter-rouge">torch.tensor</code> に変換し、<code class="language-plaintext highlighter-rouge">tokens</code> テンソルの対応する行にコピーします。このとき、<code class="language-plaintext highlighter-rouge">t</code> の長さに応じて、<code class="language-plaintext highlighter-rouge">tokens</code> テンソルの該当部分が更新されます。</li>
    </ul>
  </li>
</ol>

<p>このコードは、バッチ内の各プロンプトトークンをパディングされたテンソルに変換し、GPU上で処理できるようにするためのものです。</p>

<p>このセクションでは、<code class="language-plaintext highlighter-rouge">tokens</code>テンソルをパディングトークンで初期化し、プロンプトからのトークンシーケンスで埋めます。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
    <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">logprobs</code>がTrueの場合、対数確率を格納するためのテンソル<code class="language-plaintext highlighter-rouge">token_logprobs</code>が作成されます。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prev_pos</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">eos_reached</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="n">input_text_mask</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">!=</span> <span class="n">pad_id</span>
</code></pre></div></div>

<p>このコードスニペットでは、以下の変数が定義されています：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prev_pos</code>: 前の位置を保持するための変数で、初期値は0です。</li>
  <li><code class="language-plaintext highlighter-rouge">eos_reached</code>: バッチサイズ（<code class="language-plaintext highlighter-rouge">bsz</code>）に基づいて、各シーケンスが終端記号（EOS）に到達したかどうかを示すブール値のテンソルです。初期状態ではすべての要素が<code class="language-plaintext highlighter-rouge">False</code>に設定されています。</li>
  <li><code class="language-plaintext highlighter-rouge">input_text_mask</code>: トークンがパディングID（<code class="language-plaintext highlighter-rouge">pad_id</code>）でないかどうかを示すマスクです。このマスクは、入力テキストの有効な部分を識別するために使用されます。</li>
</ul>

<p>変数 <code class="language-plaintext highlighter-rouge">prev_pos</code>（前の位置）、<code class="language-plaintext highlighter-rouge">eos_reached</code>（シーケンス終端トークンが検出されたかどうか）、および <code class="language-plaintext highlighter-rouge">input_text_mask</code>（パディングが埋められていない位置）が初期化されます。</p>

<p>次のループは、最小のプロンプト長から始めて、トータルの長さに達するまでトークンを生成します。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">cur_pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_prompt_len</span><span class="p">,</span> <span class="n">total_len</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span><span class="p">:</span><span class="n">cur_pos</span><span class="p">],</span> <span class="n">prev_pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
        <span class="n">token_logprobs</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">target</span><span class="o">=</span><span class="n">tokens</span><span class="p">[:,</span> <span class="n">prev_pos</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cur_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">reduction</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="n">pad_id</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">sample_top_p</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>このコードは、指定された範囲内でトークンを生成するループです。<code class="language-plaintext highlighter-rouge">logits</code>はモデルの出力で、<code class="language-plaintext highlighter-rouge">logprobs</code>が有効な場合、トークンの対数確率を計算します。<code class="language-plaintext highlighter-rouge">temperature</code>が0より大きい場合、確率分布をソフトマックス関数で計算し、<code class="language-plaintext highlighter-rouge">sample_top_p</code>関数を使って次のトークンをサンプリングします。<code class="language-plaintext highlighter-rouge">temperature</code>が0以下の場合、最も確率の高いトークンを選択します。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">],</span> <span class="n">next_token</span>
    <span class="p">)</span>
    <span class="n">tokens</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span>
    <span class="n">eos_reached</span> <span class="o">|=</span> <span class="p">(</span><span class="o">~</span><span class="n">input_text_mask</span><span class="p">[:,</span> <span class="n">cur_pos</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span>
        <span class="n">next_token</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_id</span>
    <span class="p">)</span>
    <span class="n">prev_pos</span> <span class="o">=</span> <span class="n">cur_pos</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">eos_reached</span><span class="p">):</span>
        <span class="k">break</span>
</code></pre></div></div>

<p>このループ内で、モデルは次のトークンに対する確率分布 <code class="language-plaintext highlighter-rouge">logits</code> を生成します。<code class="language-plaintext highlighter-rouge">logprobs</code> が True の場合、クロスエントロピーを使用して対数確率を計算します。<code class="language-plaintext highlighter-rouge">temperature</code> に応じて、次のトークンを選択するためにソフトマックスまたはアーグマックスを使用します。その後、<code class="language-plaintext highlighter-rouge">tokens</code> テンソルを更新し、シーケンス終了トークンをチェックします。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
    <span class="n">token_logprobs</span> <span class="o">=</span> <span class="n">token_logprobs</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div></div>

<p>このコードは、<code class="language-plaintext highlighter-rouge">logprobs</code>が真（True）の場合に、<code class="language-plaintext highlighter-rouge">token_logprobs</code>をリスト形式に変換しています。<code class="language-plaintext highlighter-rouge">tolist()</code>メソッドは、NumPy配列やPyTorchテンソルなどをPythonのネイティブなリストに変換するために使用されます。</p>

<p><code class="language-plaintext highlighter-rouge">logprobs</code>がTrueの場合、対数確率テンソルはPythonのリストに変換されます。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">toks</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">.</span><span class="n">tolist</span><span class="p">()):</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>このループでは、各入力サンプルに対して、出力トークンシーケンスとそれに関連する対数確率を生成します。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="p">(</span><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_logprobs</span> <span class="k">if</span> <span class="n">logprobs</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>このコードは、<code class="language-plaintext highlighter-rouge">out_tokens</code>と、<code class="language-plaintext highlighter-rouge">logprobs</code>がTrueの場合に<code class="language-plaintext highlighter-rouge">out_logprobs</code>を返し、そうでない場合はNoneを返すことを示しています。コード自体は変更せず、そのままにしておきます。</p>

<p>最後に、この関数は生成されたトークンシーケンスとそれに対応する対数確率（要求された場合）を返します。</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-ja" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
