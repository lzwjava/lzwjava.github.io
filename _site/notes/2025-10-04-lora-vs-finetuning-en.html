<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>LoRA vs Full Fine-Tuning Performance</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LoRA vs Full Fine-Tuning Performance | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="LoRA vs Full Fine-Tuning Performance" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-10-04-lora-vs-finetuning-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-10-04-lora-vs-finetuning-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LoRA vs Full Fine-Tuning Performance" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"LoRA vs Full Fine-Tuning Performance","url":"https://lzwjava.github.io/notes/2025-10-04-lora-vs-finetuning-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=f6baf247c54f974dca0737a3862ae8612e3e7bef">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=f6baf247c54f974dca0737a3862ae8612e3e7bef" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       LoRA vs Full Fine-Tuning Performance | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-10-04-lora-vs-finetuning-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-10-04-lora-vs-finetuning-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/lora-vs-finetuning-en" selected>English</option>
        <option value="/lora-vs-finetuning-zh" >中文</option>
        <option value="/lora-vs-finetuning-ja" >日本語</option>
        <option value="/lora-vs-finetuning-es" >Español</option>
        <option value="/lora-vs-finetuning-hi" >हिंदी</option>
        <option value="/lora-vs-finetuning-fr" >Français</option>
        <option value="/lora-vs-finetuning-de" >Deutsch</option>
        <option value="/lora-vs-finetuning-ar" >العربية</option>
        <option value="/lora-vs-finetuning-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h3 id="overview-of-lora-vs-full-fine-tuning-performance">Overview of LoRA vs. Full Fine-Tuning Performance</h3>

<p>Based on empirical analysis from Thinking Machines Lab, LoRA (Low-Rank Adaptation) offers parameter-efficient fine-tuning that often matches full fine-tuning (FullFT) in capacity-constrained scenarios like small datasets or reinforcement learning (RL), but it degrades on larger datasets due to inherent training dynamics limitations.[1] This expansion delves into each finding, explaining the mechanisms, evidence, and practical implications for model developers.</p>

<h3 id="equivalence-on-small-to-medium-instruction-tuning-and-reasoning-datasets">Equivalence on Small-to-Medium Instruction-Tuning and Reasoning Datasets</h3>

<p>LoRA achieves performance parity with FullFT when fine-tuning on datasets up to a moderate size, such as those used for instruction-following (e.g., Alpaca-style datasets) or reasoning tasks (e.g., GSM8K math problems). This equivalence arises because these datasets typically contain 10,000–100,000 examples, which align well with LoRA’s low-rank parameterization capacity. LoRA approximates weight updates as a low-rank matrix decomposition (ΔW = B A, where B and A are low-rank matrices), which suffices to capture the narrow behavioral shifts needed for such tasks without needing the full expressivity of updating all parameters.</p>

<p>In practice, this means developers can use LoRA to fine-tune large models (e.g., 70B+ parameters) on consumer hardware or cloud instances with limited memory, achieving the same downstream metrics like accuracy or perplexity as FullFT. For instance, on datasets like Dolly-15k for instructions, LoRA with rank 8–16 yields indistinguishable results, saving up to 99% in trainable parameters and training time.[1] However, this holds only if the dataset doesn’t demand broad generalization beyond the training distribution—overfitting risks remain similar to FullFT.</p>

<h3 id="underperformance-on-large-datasets-exceeding-lora-capacity">Underperformance on Large Datasets Exceeding LoRA Capacity</h3>

<p>When datasets grow beyond LoRA’s effective capacity (e.g., millions of examples for domain-specific adaptation like code generation on The Stack), LoRA lags behind FullFT. The key issue isn’t a hard “capacity ceiling” where loss plateaus abruptly; instead, LoRA exhibits reduced training efficiency, with slower loss convergence tied to the mismatch between the low-rank bottleneck and dataset scale.</p>

<p>This stems from LoRA’s inductive bias: the product-of-matrices form (W’ = W + γ B A) constrains updates to a subspace, which works for sparse, low-dimensional shifts but struggles with the high-variance signals in large datasets. Empirically, loss curves show LoRA requiring 2–5x more steps to reach near-FullFT levels, and even then, final performance can be 5–10% worse on benchmarks like HumanEval for coding.[1] The relationship is parametric: efficiency drops as dataset size scales faster than LoRA rank (r), suggesting that increasing r helps marginally but doesn’t fully compensate without risking overfitting in low-data regimes.</p>

<p>Implications include preferring FullFT (or hybrids like QLoRA) for massive corpora, while LoRA shines in iterative prototyping. This also underscores the need for dataset size estimation before choosing methods—tools like token counts can guide this.</p>

<h3 id="sensitivity-to-large-batch-sizes-and-parametrization-effects">Sensitivity to Large Batch Sizes and Parametrization Effects</h3>

<p>LoRA shows greater intolerance to large batch sizes compared to FullFT, with loss penalties emerging sharply beyond optimal points (e.g., batch size &gt; 512). While FullFT’s gradient noise scales more gracefully, LoRA’s product-of-matrices setup amplifies variance in low-rank updates, leading to unstable optimization. This penalty persists even if rank is increased, as it’s rooted in the bilinear form’s different Hessian properties versus direct weight optimization.</p>

<p>For example, in experiments on reasoning datasets, LoRA loss rises 20–30% faster with batch sizes over 1k, whereas FullFT stabilizes via broader parameter averaging.[1] Mitigation strategies include gradient accumulation to simulate smaller effective batches or using techniques like AdamW with careful learning rate scheduling. This dynamic highlights LoRA’s trade-off: efficiency in memory but fragility in scaling compute parallelism, making it less ideal for high-throughput training clusters.</p>

<h3 id="benefits-of-applying-lora-to-all-layers-especially-mlps-and-moes">Benefits of Applying LoRA to All Layers, Especially MLPs and MoEs</h3>

<p>Even on small datasets, applying LoRA universally (to attention, MLP, and Mixture-of-Experts layers) outperforms attention-only variants, particularly when parameter counts are matched via higher ranks. Attention-only LoRA, common in early implementations, underperforms by 3–7% on tasks like multi-hop reasoning because it neglects the feed-forward layers (MLPs/MoEs), which handle most non-linear transformations and domain-specific knowledge integration.</p>

<p>Full-layer LoRA leverages the model’s architecture more holistically: MLPs contribute ~70% of parameters and capture task-specific computations, while MoEs (in models like Mixtral) benefit from route-specific adaptations. Matching parameters by boosting attention rank alone fails due to redundancy in attention heads, leading to inefficient subspaces. Best practices: Use rank 16–64 across all layers for small data, yielding gains in efficiency and evals without added compute.[1] This finding encourages broader adoption in libraries like PEFT, reducing the “LoRA tax” in specialized architectures.</p>

<h3 id="equivalence-in-reinforcement-learning-with-low-ranks">Equivalence in Reinforcement Learning with Low Ranks</h3>

<p>LoRA matches FullFT in RL fine-tuning (e.g., RLHF or DPO on preference datasets), even at very low ranks (r=4–8), due to RL’s inherent low-capacity requirements. Information-theoretically, RL updates focus on reward modeling and policy gradients over sparse trajectories, rarely exceeding the representational power of low-rank deltas—unlike SL’s dense token predictions.</p>

<p>Anticipated from theory, this holds because RL losses (e.g., PPO clips) emphasize directional shifts in behavior, which LoRA’s subspace efficiently encodes without full parameterization. Experiments confirm parity on benchmarks like Anthropic’s HH-RLHF, with LoRA converging faster and using 10x less memory.[1] For practitioners, this makes LoRA a default for RL pipelines, enabling scalable alignment of trillion-parameter models on modest hardware.</p>

<h3 id="broader-implications-and-recommendations">Broader Implications and Recommendations</h3>

<p>These insights reveal LoRA as a pragmatic tool for most post-training needs, especially in resource-constrained environments, but not a universal replacement for FullFT on expansive data. Future work might explore adaptive ranks or layer-specific scaling to bridge gaps. Developers should benchmark on their dataset scale and monitor batch effects early.[1] Overall, LoRA democratizes fine-tuning without regret in targeted scenarios, aligning with the shift toward efficient AI development.</p>

<p>Citations:
[1] https://tinker-docs.thinkingmachines.ai/lora-primer</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    
      <p class="model">x-ai/grok-4-fast</p>
    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
