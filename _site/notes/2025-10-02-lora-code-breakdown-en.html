<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>LoRA Code Breakdown and Explanation</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LoRA Code Breakdown and Explanation | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="LoRA Code Breakdown and Explanation" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-10-02-lora-code-breakdown-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-10-02-lora-code-breakdown-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LoRA Code Breakdown and Explanation" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"LoRA Code Breakdown and Explanation","url":"https://lzwjava.github.io/notes/2025-10-02-lora-code-breakdown-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=6cab96962eda1570e0cd005ed7ae2d22845383e2">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=6cab96962eda1570e0cd005ed7ae2d22845383e2" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       LoRA Code Breakdown and Explanation | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-10-02-lora-code-breakdown-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-10-02-lora-code-breakdown-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/lora-code-breakdown-en" selected>English</option>
        <option value="/lora-code-breakdown-zh" >中文</option>
        <option value="/lora-code-breakdown-ja" >日本語</option>
        <option value="/lora-code-breakdown-es" >Español</option>
        <option value="/lora-code-breakdown-hi" >हिंदी</option>
        <option value="/lora-code-breakdown-fr" >Français</option>
        <option value="/lora-code-breakdown-de" >Deutsch</option>
        <option value="/lora-code-breakdown-ar" >العربية</option>
        <option value="/lora-code-breakdown-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>I’ll break down the LoRA demo code into key pieces, explaining each part in detail before moving to the next. This will help illustrate how LoRA works in a clear, step-by-step manner. The code is based on the previous example, using PyTorch to implement LoRA for a simple linear layer.</p>

<h3 id="code-piece-1-lora-layer-definition">Code Piece 1: LoRA Layer Definition</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LoRALayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># Original frozen weights
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># Freeze original weights
</span>        <span class="c1"># LoRA parameters: low-rank matrices A and B
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Scaling factor for LoRA updates
</span></code></pre></div></div>

<h4 id="explanation">Explanation</h4>
<p>This piece defines the <code class="language-plaintext highlighter-rouge">LoRALayer</code> class, which implements the LoRA technique. Here’s what’s happening:</p>

<ul>
  <li><strong>Imports and Class Setup</strong>: We import PyTorch (<code class="language-plaintext highlighter-rouge">torch</code>) and its neural network module (<code class="language-plaintext highlighter-rouge">nn</code>). The <code class="language-plaintext highlighter-rouge">LoRALayer</code> class inherits from <code class="language-plaintext highlighter-rouge">nn.Module</code>, making it a PyTorch module that can be integrated into larger models.</li>
  <li><strong>Original Linear Layer</strong>: <code class="language-plaintext highlighter-rouge">self.linear = nn.Linear(in_features, out_features)</code> creates a standard linear layer (like a fully connected layer in a neural network) with <code class="language-plaintext highlighter-rouge">in_features</code> inputs and <code class="language-plaintext highlighter-rouge">out_features</code> outputs. This represents the pre-trained weights we want to adapt.</li>
  <li><strong>Freezing Weights</strong>: <code class="language-plaintext highlighter-rouge">self.linear.weight.requires_grad = False</code> freezes the original weights of the linear layer, ensuring they aren’t updated during training. This is key to LoRA’s efficiency, as it avoids modifying the large pre-trained model.</li>
  <li><strong>LoRA Parameters</strong>: <code class="language-plaintext highlighter-rouge">self.lora_A</code> and <code class="language-plaintext highlighter-rouge">self.lora_B</code> are low-rank matrices. <code class="language-plaintext highlighter-rouge">lora_A</code> has shape <code class="language-plaintext highlighter-rouge">(in_features, rank)</code>, and <code class="language-plaintext highlighter-rouge">lora_B</code> has shape <code class="language-plaintext highlighter-rouge">(rank, out_features)</code>. The <code class="language-plaintext highlighter-rouge">rank</code> parameter (default=4) controls the size of these matrices, keeping them much smaller than the original weight matrix (shape <code class="language-plaintext highlighter-rouge">in_features x out_features</code>). These matrices are trainable (<code class="language-plaintext highlighter-rouge">nn.Parameter</code>) and initialized with random values (<code class="language-plaintext highlighter-rouge">torch.randn</code>).</li>
  <li><strong>Scaling Factor</strong>: <code class="language-plaintext highlighter-rouge">self.scaling = 1.0</code> is a hyperparameter to scale the LoRA adjustment, allowing fine-tuning of the adaptation strength.</li>
</ul>

<p>This setup ensures that only the small <code class="language-plaintext highlighter-rouge">lora_A</code> and <code class="language-plaintext highlighter-rouge">lora_B</code> matrices are updated during training, drastically reducing the number of trainable parameters.</p>

<hr />

<h3 id="code-piece-2-lora-forward-pass">Code Piece 2: LoRA Forward Pass</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Original linear transformation + LoRA adjustment
</span>        <span class="n">original</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">lora_adjustment</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">lora_B</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">original</span> <span class="o">+</span> <span class="n">lora_adjustment</span>
</code></pre></div></div>

<h4 id="explanation-1">Explanation</h4>
<p>This piece defines the forward pass of the <code class="language-plaintext highlighter-rouge">LoRALayer</code>, which computes the output of the layer:</p>

<ul>
  <li><strong>Input</strong>: The input <code class="language-plaintext highlighter-rouge">x</code> is a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, in_features)</code>, representing a batch of input data.</li>
  <li><strong>Original Output</strong>: <code class="language-plaintext highlighter-rouge">original = self.linear(x)</code> computes the output of the frozen linear layer, applying the pre-trained weights to the input.</li>
  <li><strong>LoRA Adjustment</strong>: The term <code class="language-plaintext highlighter-rouge">torch.matmul(torch.matmul(x, self.lora_A), self.lora_B)</code> computes the low-rank adaptation. First, <code class="language-plaintext highlighter-rouge">x</code> is multiplied by <code class="language-plaintext highlighter-rouge">lora_A</code> (shape <code class="language-plaintext highlighter-rouge">in_features x rank</code>), producing a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, rank)</code>. Then, this is multiplied by <code class="language-plaintext highlighter-rouge">lora_B</code> (shape <code class="language-plaintext highlighter-rouge">rank x out_features</code>), yielding a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, out_features)</code>—the same shape as the original output. This adjustment represents the task-specific update.</li>
  <li><strong>Scaling and Combination</strong>: The adjustment is scaled by <code class="language-plaintext highlighter-rouge">self.scaling</code> and added to the original output, producing the final output. This ensures the model retains the pre-trained knowledge while incorporating task-specific adaptations.</li>
</ul>

<p>The low-rank structure (<code class="language-plaintext highlighter-rouge">rank</code> is small, e.g., 4) ensures the adjustment is computationally cheap and parameter-efficient compared to updating the full weight matrix.</p>

<hr />

<h3 id="code-piece-3-toy-dataset-and-training">Code Piece 3: Toy Dataset and Training</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_toy_dataset</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># Random input features
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Random target outputs
</span>    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">param</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="explanation-2">Explanation</h4>
<p>This piece creates a toy dataset and trains the LoRA-adapted model:</p>

<ul>
  <li><strong>Toy Dataset</strong>: The <code class="language-plaintext highlighter-rouge">create_toy_dataset</code> function generates synthetic data for demonstration. <code class="language-plaintext highlighter-rouge">X</code> is a tensor of shape <code class="language-plaintext highlighter-rouge">(1000, 64)</code> (1000 samples, 64 features), and <code class="language-plaintext highlighter-rouge">y</code> is a tensor of shape <code class="language-plaintext highlighter-rouge">(1000, 10)</code> (1000 samples, 10 output dimensions). These are random tensors to simulate input-output pairs.</li>
  <li><strong>Training Function</strong>: The <code class="language-plaintext highlighter-rouge">train_model</code> function sets up a simple training loop:
    <ul>
      <li><strong>Loss Function</strong>: <code class="language-plaintext highlighter-rouge">nn.MSELoss()</code> defines mean squared error as the loss, suitable for this regression-like toy task.</li>
      <li><strong>Optimizer</strong>: <code class="language-plaintext highlighter-rouge">optim.Adam</code> optimizes only the trainable parameters (<code class="language-plaintext highlighter-rouge">param.requires_grad</code> is <code class="language-plaintext highlighter-rouge">True</code>), which are <code class="language-plaintext highlighter-rouge">lora_A</code> and <code class="language-plaintext highlighter-rouge">lora_B</code>. The frozen <code class="language-plaintext highlighter-rouge">linear.weight</code> is excluded, ensuring efficiency.</li>
      <li><strong>Training Loop</strong>: For each epoch, the model computes outputs, calculates the loss, performs backpropagation (<code class="language-plaintext highlighter-rouge">loss.backward()</code>), and updates the LoRA parameters (<code class="language-plaintext highlighter-rouge">optimizer.step()</code>). The loss is printed to monitor training progress.</li>
    </ul>
  </li>
</ul>

<p>This setup demonstrates how LoRA fine-tunes only the low-rank matrices, keeping the process lightweight.</p>

<hr />

<h3 id="code-piece-4-main-execution-and-parameter-count">Code Piece 4: Main Execution and Parameter Count</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Set random seed for reproducibility
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Create toy dataset
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_toy_dataset</span><span class="p">()</span>
    
    <span class="c1"># Initialize model with LoRA
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    
    <span class="c1"># Count trainable parameters
</span>    <span class="n">trainable_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Trainable parameters: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Train the model
</span>    <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="explanation-3">Explanation</h4>
<p>This piece ties everything together and highlights LoRA’s efficiency:</p>

<ul>
  <li><strong>Random Seed</strong>: <code class="language-plaintext highlighter-rouge">torch.manual_seed(42)</code> ensures reproducibility of random initializations.</li>
  <li><strong>Dataset and Model</strong>: The toy dataset is created, and a <code class="language-plaintext highlighter-rouge">LoRALayer</code> is initialized with <code class="language-plaintext highlighter-rouge">in_features=64</code>, <code class="language-plaintext highlighter-rouge">out_features=10</code>, and <code class="language-plaintext highlighter-rouge">rank=4</code>.</li>
  <li><strong>Parameter Count</strong>: The code calculates:
    <ul>
      <li><strong>Trainable parameters</strong>: Only <code class="language-plaintext highlighter-rouge">lora_A</code> (64 × 4 = 256) and <code class="language-plaintext highlighter-rouge">lora_B</code> (4 × 10 = 40), totaling 296 parameters.</li>
      <li><strong>Total parameters</strong>: Includes the frozen <code class="language-plaintext highlighter-rouge">linear.weight</code> (64 × 10 = 640) and <code class="language-plaintext highlighter-rouge">linear.bias</code> (10), plus LoRA parameters, totaling 946.</li>
      <li>This shows LoRA’s efficiency: only ~31% (296/946) of parameters are trainable, compared to 100% in full fine-tuning.</li>
    </ul>
  </li>
  <li><strong>Training</strong>: The <code class="language-plaintext highlighter-rouge">train_model</code> function is called to fine-tune the LoRA parameters on the toy dataset.</li>
</ul>

<p>When you run this, you’ll see the number of trainable parameters is much smaller than the total, and the model trains quickly due to the low-rank updates.</p>

<hr />

<h3 id="running-the-code">Running the Code</h3>
<p>To run this, you need PyTorch installed (<code class="language-plaintext highlighter-rouge">pip install torch</code>). Save the full code as a <code class="language-plaintext highlighter-rouge">.py</code> file and execute it. The output will show the number of parameters and the training loss over 10 epochs, demonstrating LoRA’s efficiency in fine-tuning.</p>

<p><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></p>

<p>Let me know if you’d like to dive deeper into any part or modify the example!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    
      <p class="model">x-ai/grok-4-fast:free</p>
    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
