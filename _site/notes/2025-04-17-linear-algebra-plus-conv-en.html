<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Linear Algebra Plus - Conversation</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Linear Algebra Plus - Conversation | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Linear Algebra Plus - Conversation" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-04-17-linear-algebra-plus-conv-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-04-17-linear-algebra-plus-conv-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Linear Algebra Plus - Conversation" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Linear Algebra Plus - Conversation","url":"https://lzwjava.github.io/notes/2025-04-17-linear-algebra-plus-conv-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=e8544ebb8cd270e46ba5d6942d874dd40bd87619">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=e8544ebb8cd270e46ba5d6942d874dd40bd87619" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Linear Algebra Plus - Conversation | Original
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-04-17-linear-algebra-plus-conv-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!-- 
    <a href="#" id="playAudioButton" class="button audio-button" data-file-path="notes/2025-04-17-linear-algebra-plus-conv-en.md">Audio</a>
     -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-04-17-linear-algebra-plus-conv-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/linear-algebra-plus-conv-en" selected>English</option>
        <option value="/linear-algebra-plus-conv-zh" >中文</option>
        <option value="/linear-algebra-plus-conv-ja" >日本語</option>
        <option value="/linear-algebra-plus-conv-es" >Español</option>
        <option value="/linear-algebra-plus-conv-hi" >हिंदी</option>
        <option value="/linear-algebra-plus-conv-fr" >Français</option>
        <option value="/linear-algebra-plus-conv-de" >Deutsch</option>
        <option value="/linear-algebra-plus-conv-ar" >العربية</option>
        <option value="/linear-algebra-plus-conv-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>A: Hey, I’ve been revisiting quadratic forms lately, especially the process of transforming them into canonical form. Can you break down how you approach it with that example, Q(x, y) = 2x² + 4xy + 3y²?</p>

<p>B: Sure! Let’s start with the basics. That quadratic form can be written as a matrix equation, right? You take the coefficients and build a symmetric matrix A. For this one, it’s [2, 2; 2, 3], since the 4xy term splits evenly as 2xy + 2yx. Does that align with how you see it?</p>

<p>A: Exactly, I agree on the matrix setup. The off-diagonal 2 comes from halving the 4, which makes sense for symmetry. So, next step is eigenvalues, right? How do you tackle that here?</p>

<p>B: Yup, eigenvalues are key. We solve det(A - λI) = 0. So, for [2-λ, 2; 2, 3-λ], the determinant is (2-λ)(3-λ) - 4. Expanding that, you get λ² - 5λ + 2 = 0. Solving that quadratic gives λ = (5 ± √17)/2. What do you think about those values?</p>

<p>A: Let me check that… Yeah, the discriminant is 25 - 8 = 17, so (5 ± √17)/2 looks spot on. Both are positive, which suggests this form might be positive definite. But let’s not jump ahead—how do you handle the eigenvectors next?</p>

<p>B: Good catch on the positivity! For eigenvectors, take λ₁ = (5 + √17)/2 first. Plug it into A - λI, so [2 - λ₁, 2; 2, 3 - λ₁]. Row reducing that system, you get an eigenvector like [2, λ₁ - 2]. Then repeat for λ₂ = (5 - √17)/2. It’s a bit tedious—do you normalize them right away or wait?</p>

<p>A: I usually wait until I’m building the P matrix to normalize, just to keep the algebra cleaner early on. So, P’s columns would be those eigenvectors, and then D is diagonal with λ₁ and λ₂. How does that transform Q into canonical form?</p>

<p>B: Exactly, P diagonalizes A, so P^T A P = D. You define new variables, say [x; y] = P [u; v], and substitute back. The quadratic form becomes Q(u, v) = λ₁u² + λ₂v². Since both eigenvalues are positive here, it’s a sum of squares—no cross terms. Does that simplicity ever surprise you?</p>

<p>A: Sometimes, yeah! It’s elegant how the cross terms vanish. But I’m curious—what if one eigenvalue were negative? How would that change the interpretation in, say, optimization contexts?</p>

<table>
  <tbody>
    <tr>
      <td>B: Great question! If λ₂ were negative, you’d get Q = λ₁u² -</td>
      <td>λ₂</td>
      <td>v², making it indefinite. In optimization, that’s a saddle point—maximizing in one direction, minimizing in another. Think of a function like f(x, y) = 2x² + 4xy - 3y². It’s trickier to classify extrema. Ever run into that in real applications?</td>
    </tr>
  </tbody>
</table>

<p>A: Oh, definitely. In machine learning, indefinite forms pop up with Hessian matrices when you’re checking second-order conditions. Positive definite means a local min, but indefinite signals a saddle. Do you think this diagonalization approach scales well for higher dimensions?</p>

<p>B: It does, but the computation gets hairy. For n variables, you’re solving an n-th degree polynomial for eigenvalues, and numerical stability becomes an issue. Libraries like NumPy or LAPACK handle it, but analytically? Brutal. What’s your go-to for big systems?</p>

<p>A: I lean on numerical tools too—eigenvalue decomposition’s a lifesaver there. But I wonder, are there alternatives to diagonalization? Like, completing the square instead?</p>

<p>B: Oh, absolutely! For 2x² + 4xy + 3y², you could try completing the square: 2(x² + 2xy) + 3y² = 2(x + y)² - 2y² + 3y² = 2(x + y)² + y². It’s not quite canonical yet, but a substitution like u = x + y, v = y could clean it up. It’s less systematic than diagonalization, though—thoughts on trade-offs?</p>

<p>A: I like that—it’s more intuitive for small cases, but I see the lack of generality. Diagonalization’s rigorous and extends to n dimensions, while completing the square feels ad hoc past three variables. Ever tried hybrid approaches?</p>

<p>B: Not really, but that’s an idea! Maybe start with completing the square to get a feel, then formalize with diagonalization. Emerging trends lean toward computational efficiency anyway—think iterative methods for sparse matrices. Where do you see this heading?</p>

<p>A: I’d bet on hybrid numerical-symbolic methods, especially with AI optimizing matrix ops. Canonical forms are timeless, but the tools to get there? They’re evolving fast. This was fun—want to tackle a 3D example next time?</p>

<p>B: Totally! Let’s do Q(x, y, z) = x² + 2xy + 2yz + z² or something wild. See you then!</p>

<p>A: Hey, I’ve been brushing up on matrices lately—notation, operations, all that jazz. Can you walk me through how you’d explain the basics to someone, maybe starting with that 2x² + 4xy + 3y² quadratic form matrix from earlier?</p>

<p>B: Sure, let’s dive in! A matrix is just a rectangular array, right? For that quadratic form, we turned it into a symmetric matrix: [2, 2; 2, 3]. The 2’s on the off-diagonal come from splitting the 4xy term. How do you usually introduce matrix notation?</p>

<p>A: I’d go with the general form: A = [a_ij], where i’s the row, j’s the column. So, for that example, a_11 = 2, a_12 = 2, and so on. It’s a 2×2 square matrix. What’s your next step—types of matrices or operations?</p>

<p>B: Let’s hit types first. That [2, 2; 2, 3] is square, m = n = 2. Then there’s the identity matrix, like [1, 0; 0, 1], which acts like a ‘1’ in multiplication. Ever find it weird how simple yet powerful that is?</p>

<p>A: Yeah, it’s almost too neat—AI = IA = A just clicks. What about the zero matrix? I’d toss in [0, 0; 0, 0]—multiplying by it kills everything. Does that tie into operations for you?</p>

<p>B: Totally! Operations are where it gets fun. Addition’s straightforward—same sizes, add elements. Say [1, 2; 3, 4] + [2, 0; 1, 3] = [3, 2; 4, 7]. Subtraction’s the same deal. What about scalar multiplication—how do you demo that?</p>

<p>A: Easy—multiply every entry by a number. Like 3 × [1, -2; 4, 0] = [3, -6; 12, 0]. It’s intuitive, but matrix multiplication? That’s where I trip up explaining the row-column dance. How do you break it down?</p>

<p>B: I go with an example. Take [1, 2; 3, 4] times [2, 0; 1, 3]. The (1,1) entry is 1×2 + 2×1 = 4, (1,2) is 1×0 + 2×3 = 6, and so on. You end up with [4, 6; 10, 12]. It’s all dot products. Does that click, or is the condition part trickier?</p>

<p>A: The dot product part’s clear, but I always stress the condition: columns of the first must match rows of the second. Here, 2×2 times 2×2 works. What if they don’t match—any real-world cases where that messes things up?</p>

<p>B: Oh, tons! In data science, mismatched dimensions crash your code—like multiplying a feature matrix by a weight vector with wrong sizes. Next up, transpose—swap rows and columns. For [1, 2; 3, 4], it’s [1, 3; 2, 4]. Any favorite transpose properties?</p>

<p>A: I love (AB)^T = B^T A^T—it’s so counterintuitive at first! Rows become columns, and order flips. How does that play into our quadratic form matrix?</p>

<p>B: Good one! For [2, 2; 2, 3], it’s symmetric, so A^T = A. That’s why Q(x, y) = x^T A x works—symmetry keeps it clean. Now, inverses—only square matrices with nonzero determinants. Want to try finding A^-1 for [4, 7; 2, 6]?</p>

<p>A: Sure! Det = 4×6 - 7×2 = 24 - 14 = 10. Then A^-1 = (1/10) × [6, -7; -2, 4] = [0.6, -0.7; -0.2, 0.4]. Did I nail it?</p>

<p>B: Spot on! Multiply A A^-1, you get the identity. Inverses are clutch for solving systems or optimization. Ever use them in bigger contexts, like 3×3 or beyond?</p>

<p>A: Yeah, in graphics—rotation matrices need inverses to undo transformations. But past 2×2, I lean on software. Hand-computing a 3×3 inverse is a slog. You?</p>

<p>B: Same—numerical libraries all the way. Though, for teaching, I’ll grind a 2×2 to show the pattern. What’s your take on emerging tools—like AI speeding up matrix ops?</p>

<p>A: I’m all in for it. AI could optimize sparse matrix multiplications or inverses in real time. Classics like these operations don’t change, but the tech? It’s a game-changer. Want to try a 3×3 next?</p>

<p>B: Let’s do it! How about [1, 2, 0; 0, 3, 1; 2, -1, 4]? We’ll tackle the inverse or multiplication—your pick!</p>

<p>A: Hey, I’m prepping for a linear algebra exam and trying to nail down the key points. Want to run through some together? Maybe start with what linear algebra even is?</p>

<p>B: Sure, let’s do it! Linear algebra’s all about vector spaces and linear mappings—like solving systems of equations. It’s the backbone of so much math. What’s your first big concept to tackle?</p>

<p>A: Vectors, I think. They’ve got magnitude and direction, right? And you can stick them in n-dimensional space. How do you picture them—rows or columns?</p>

<p>B: Depends on context! I see them as columns usually, like [x; y], but row vectors pop up too. Next up—matrices? They’re just arrays of numbers, but they’re everywhere in this stuff.</p>

<p>A: Yeah, rectangular arrays with rows and columns. Square ones have m = n, like [2, -1; 4, 3]. What’s special about the identity matrix?</p>

<p>B: Oh, the identity’s cool—it’s got 1s on the diagonal, 0s elsewhere, like [1, 0; 0, 1]. Multiply it by any matrix, and nothing changes. Ever mess with the zero matrix?</p>

<p>A: The all-zero one? Like [0, 0; 0, 0]? It wipes out anything you multiply it with. Speaking of operations, how’s matrix addition work?</p>

<p>B: Simple—same sizes, add element-wise. [1, 2] + [3, 4] = [4, 6]. But multiplication’s trickier—columns of the first have to match rows of the second. Ever notice it’s not commutative?</p>

<p>A: Yeah, AB ≠ BA throws me off! What about determinants? I know they’re tied to invertibility.</p>

<p>B: Exactly! A matrix is invertible only if its determinant isn’t zero. For a 2×2, it’s ad - bc. What’s the deal with inverses for you?</p>

<p>A: A^-1 times A gives the identity, but only for square, non-singular matrices. How do eigenvalues fit in?</p>

<p>B: Eigenvalues are scalars where Av = λv holds for some vector v. You solve det(A - λI) = 0. Eigenvectors don’t change direction, just scale. Big in diagonalization—want to dig into that?</p>

<p>A: Yeah, diagonalization’s huge. A matrix is diagonalizable if it’s got enough independent eigenvectors, right? Turns it into a diagonal matrix. What’s that do for us?</p>

<p>B: Simplifies everything—systems of equations, powers of matrices. Ties into quadratic forms too, like xᵀAx. Ever play with symmetric matrices?</p>

<p>A: Symmetric ones where A = Aᵀ? They’re big for quadratic forms. How do you handle systems of equations—Gaussian elimination?</p>

<p>B: Yup, Gaussian elimination gets you to row echelon form, or reduced row echelon for solutions. Homogeneous systems always have the zero solution. What’s your take on consistent vs. inconsistent systems?</p>

<p>A: Consistent means at least one solution, inconsistent means none. Dependent systems have infinite solutions, independent just one. How’s that tie to rank?</p>

<p>B: Rank’s the number of independent rows or columns. Full rank means max independence. Null space is all vectors where Ax = 0—rank-nullity theorem connects them. Ever use that?</p>

<p>A: Not yet, but I get rank + nullity = number of columns. What about vector spaces and bases?</p>

<p>B: Vector space is vectors you can add and scale. A basis is linearly independent and spans it—dimension’s the basis size. Subspaces are smaller vector spaces inside. Cool, right?</p>

<p>A: Super cool! Linear independence means no vector’s a combo of others. Span’s all their combinations. How do transformations fit in?</p>

<p>B: Linear transformations preserve addition and scaling. Kernel’s what maps to zero, image is the output range. Think rotations or projections. Orthogonality next?</p>

<p>A: Yeah, orthogonal vectors—dot product zero. Orthonormal’s that plus unit length. Orthogonal matrices are wild—their inverse is their transpose. How’s that useful?</p>

<p>B: Preserves lengths and angles—huge in graphics. Gram-Schmidt makes vectors orthogonal. What about determinants in bigger matrices?</p>

<p>A: For 3×3, cofactor expansion, right? Triangular ones are just diagonal products. Singular if det = 0. How’s that help systems?</p>

<p>B: Tells you if there’s a unique solution—det ≠ 0 means invertible. Row ops simplify it. Ever try SVD or LU decomposition?</p>

<p>A: Heard of them—SVD breaks a matrix into three, LU’s for solving systems. Real-world stuff like graphics or data science uses all this, huh?</p>

<p>B: Oh yeah—optimization, engineering, machine learning. Least-squares for overdetermined systems, too. What’s your favorite application?</p>

<p>A: Computer graphics—rotations and projections are all matrices. This is a lot—want to hit a tricky one, like a 3×3 inverse?</p>

<p>B: Let’s do it! Pick one—maybe [1, 2, 0; 0, 3, 1; 2, -1, 4]? We’ll grind it out together!</p>

<p>A: Alright, let’s tackle that 3×3 inverse for [1, 2, 0; 0, 3, 1; 2, -1, 4]. First step’s the determinant, right? How do you usually start that?</p>

<p>B: Yup, determinant first! For a 3×3, I go with cofactor expansion along the first row. So, it’s 1 times det([3, 1; -1, 4]) minus 2 times det([0, 1; 2, 4]) plus 0 times something. Want to compute those 2×2s with me?</p>

<p>A: Sure! First one’s [3, 1; -1, 4], so 3×4 - 1×(-1) = 12 + 1 = 13. Second is [0, 1; 2, 4], so 0×4 - 1×2 = -2. The last term’s 0, so det = 1×13 - 2×(-2) = 13 + 4 = 17. Sound good?</p>

<p>B: Spot on! Det = 17, so it’s invertible. Next, we need the adjugate—cofactors transposed. Start with the cofactor matrix—pick an element, like (1,1). What’s its minor and cofactor?</p>

<p>A: For (1,1), cover row 1, column 1, so minor is [3, 1; -1, 4], det = 13. Cofactor’s (-1)^(1+1) × 13 = 13. Next, (1,2)—minor’s [0, 1; 2, 4], det = -2, cofactor’s (-1)^(1+2) × (-2) = 2. Keep going?</p>

<p>B: Yeah, let’s do one more—(1,3). Minor’s [0, 3; 2, -1], det = 0×(-1) - 3×2 = -6, cofactor’s (-1)^(1+3) × (-6) = -6. You’re killing it! Want to finish the cofactor matrix or jump to adjugate?</p>

<p>A: Let’s finish it. Row 2: (2,1) minor [2, 0; -1, 4], det = 8, cofactor = -8; (2,2) minor [1, 0; 2, 4], det = 4, cofactor = 4; (2,3) minor [1, 2; 2, -1], det = -5, cofactor = 5. Row 3?</p>

<p>B: Row 3: (3,1) minor [2, 0; 3, 1], det = 2, cofactor = -2; (3,2) minor [1, 0; 0, 1], det = 1, cofactor = -1; (3,3) minor [1, 2; 0, 3], det = 3, cofactor = 3. So cofactor matrix is [13, 2, -6; -8, 4, 5; -2, -1, 3]. Transpose it!</p>

<p>A: Adjugate’s [13, -8, -2; 2, 4, -1; -6, 5, 3]. Inverse is (1/17) times that, so [13/17, -8/17, -2/17; 2/17, 4/17, -1/17; -6/17, 5/17, 3/17]. Should we check it?</p>

<p>B: Let’s do a quick check—multiply original by inverse, should get identity. First row, first column: 1×(13/17) + 2×(2/17) + 0×(-6/17) = 13/17 + 4/17 = 1. Looks promising! Want to try another spot?</p>

<p>A: Yeah, (2,2): 0×(-8/17) + 3×(4/17) + 1×(5/17) = 12/17 + 5/17 = 1. Off-diagonal, like (1,2): 1×(-8/17) + 2×(4/17) + 0×(5/17) = -8/17 + 8/17 = 0. It works! Gaussian elimination faster?</p>

<table>
  <tbody>
    <tr>
      <td>B: Oh, way faster for big matrices! Augment with identity, row reduce to [I</td>
      <td>A^-1]. But this adjoint method’s great for understanding. What’s next—eigenvalues for this guy?</td>
    </tr>
  </tbody>
</table>

<p>A: Let’s try it! Characteristic equation’s det(A - λI) = 0. So [1-λ, 2, 0; 0, 3-λ, 1; 2, -1, 4-λ]. Determinant’s a cubic—how do you expand that?</p>

<p>B: First row again: (1-λ) times det([3-λ, 1; -1, 4-λ]) - 2 times det([0, 1; 2, 4-λ]) + 0. First minor: (3-λ)(4-λ) - (-1)×1 = 12 - 7λ + λ² + 1 = λ² - 7λ + 13. Second: 0×(4-λ) - 1×2 = -2. So (1-λ)(λ² - 7λ + 13) - 2×(-2). Simplify it?</p>

<p>A: Sure! Expand: (1-λ)(λ² - 7λ + 13) = λ³ - 7λ² + 13λ - λ² + 7λ - 13 = λ³ - 8λ² + 20λ - 13, then + 4 = λ³ - 8λ² + 20λ - 9. Roots are the eigenvalues—tough to factor by hand. Numerical solver?</p>

<p>B: Yeah, cubic’s brutal analytically. Software says roots are around 1, 3, 4—makes sense, det = 17’s their product. Eigenvectors next, or switch gears to something like SVD?</p>

<p>A: Let’s peek at SVD—it’s A = UΣVᵀ, right? Breaks a matrix into orthogonal U and V, diagonal Σ. How’s that differ from diagonalization?</p>

<p>B: Big difference! Diagonalization needs a square matrix with enough eigenvectors. SVD works for any matrix, even rectangular. Σ has singular values, not eigenvalues. Super useful in data compression—like PCA. Ever tried it?</p>

<p>A: Not hands-on, but I’ve seen it in machine learning—reducing dimensions. So U and V are orthogonal, Σ’s got the ‘stretch’ factors. How do you compute it practically?</p>

<p>B: Start with AᵀA or AAᵀ to get eigenvalues, eigenvectors for V and U. Singular values are square roots of those eigenvalues. Libraries like NumPy do the heavy lifting. Want to wrap with a real-world use?</p>

<p>A: Yeah, how’s all this hit graphics? Rotations, projections—matrices everywhere, right?</p>

<p>B: Totally! Rotation matrices are orthogonal, preserve length. Projections use stuff like A(AᵀA)^-1Aᵀ. Linear algebra’s the engine behind 3D rendering, animations—all that cool stuff. What’s your next exam topic?</p>

<p>A: Maybe vector spaces deeper—bases, spans. This was awesome—3×3’s no joke! Again soon?</p>

<p>B: Anytime! Let’s hit spans and bases—maybe tie it to rank and nullity. You’re crushing this!</p>



  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
