<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>DeepSeek V3</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>DeepSeek V3 | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="DeepSeek V3" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-02-27-deepseek-v3-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-02-27-deepseek-v3-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="DeepSeek V3" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"DeepSeek V3","url":"https://lzwjava.github.io/notes/2025-02-27-deepseek-v3-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=2dd6920d59ca16b89466941ea3225cdad2ba95fc">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=2dd6920d59ca16b89466941ea3225cdad2ba95fc" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       DeepSeek V3 | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-02-27-deepseek-v3-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!-- 
    <a href="#" id="playAudioButton" class="button audio-button" data-file-path="notes/2025-02-27-deepseek-v3-en.md">Audio</a>
     -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-02-27-deepseek-v3-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/deepseek-v3-en" selected>English</option>
        <option value="/deepseek-v3-zh" >中文</option>
        <option value="/deepseek-v3-ja" >日本語</option>
        <option value="/deepseek-v3-es" >Español</option>
        <option value="/deepseek-v3-hi" >हिंदी</option>
        <option value="/deepseek-v3-fr" >Français</option>
        <option value="/deepseek-v3-de" >Deutsch</option>
        <option value="/deepseek-v3-ar" >العربية</option>
        <option value="/deepseek-v3-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h3 id="overview-and-key-highlights">Overview and Key Highlights</h3>
<ol>
  <li>Model Name: DeepSeek-V3, a Mixture-of-Experts (MoE) language model with 671 billion parameters, of which 37 billion are activated per token.</li>
  <li>Training Dataset: Pre-trained on 14.8 trillion diverse, high-quality tokens.</li>
  <li>Core Innovations: Incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE architectures with auxiliary-loss-free load balancing for efficiency.</li>
  <li>Training Efficiency: Achieves full training with only 2.788 million H800 GPU hours.</li>
  <li>Cost Efficiency: Training cost is estimated at 5.576M USD, assuming 2 USD per GPU hour.</li>
</ol>

<hr />

<h3 id="architectural-innovations">Architectural Innovations</h3>
<ol>
  <li>Transformer-Based Framework: Retains the Transformer architecture for scalability and flexibility.</li>
  <li>Multi-Head Latent Attention (MLA): Reduces inference memory by compressing key-value caches without performance loss.</li>
  <li>DeepSeekMoE: Utilizes a combination of shared and routed experts for cost-effective training and high computational efficiency.</li>
  <li>Auxiliary-Loss-Free Load Balancing: Introduces bias terms to maintain balanced expert loads without compromising performance.</li>
  <li>Multi-Token Prediction (MTP): Sequentially predicts multiple tokens per position, improving data efficiency and representation pre-planning.</li>
</ol>

<hr />

<h3 id="training-framework">Training Framework</h3>
<ol>
  <li>FP8 Mixed Precision Training: Leverages fine-grained quantization and low-precision storage to optimize memory and computation.</li>
  <li>DualPipe Algorithm: Overlaps computation and communication phases, reducing pipeline bubbles and improving parallelism.</li>
  <li>Efficient Cross-Node Communication: Employs optimized kernels for all-to-all operations, utilizing NVLink and InfiniBand bandwidths.</li>
  <li>Low-Precision Optimizer States: Stores optimizer states in BF16, reducing memory consumption without performance loss.</li>
  <li>Memory Optimization Techniques: Recomputes certain operations (e.g., RMSNorm) during back-propagation to save memory.</li>
</ol>

<hr />

<h3 id="pre-training-details">Pre-Training Details</h3>
<ol>
  <li>Stable Training Process: No irrecoverable loss spikes or rollbacks occurred during pre-training.</li>
  <li>Context Length Extension: Extended context length to 32K and subsequently to 128K in two stages.</li>
  <li>Training Costs: Pre-training required 2.664M GPU hours, context extension 119K GPU hours, and post-training 5K GPU hours.</li>
  <li>Token Efficiency: Training efficiency ensured by minimizing GPU hours per trillion tokens.</li>
  <li>High-Quality Data: Pre-training dataset curated for diversity and relevance.</li>
</ol>

<hr />

<h3 id="post-training-enhancements">Post-Training Enhancements</h3>
<ol>
  <li>Supervised Fine-Tuning (SFT): Aligns model outputs with human preferences.</li>
  <li>Reinforcement Learning (RL): Employs Group Relative Policy Optimization for fine-tuning.</li>
  <li>Knowledge Distillation: Integrates reasoning capabilities from DeepSeek-R1 models.</li>
  <li>Output Style Control: Balances accuracy with generation length and style.</li>
  <li>Performance Refinement: Post-training further improves benchmark results.</li>
</ol>

<hr />

<h3 id="benchmark-performance">Benchmark Performance</h3>
<ol>
  <li>MMLU (Educational Benchmarks): Achieves 88.5, surpassing other open-source models.</li>
  <li>GPQA (General Knowledge): Scores 59.1, comparable to GPT-4o and Claude-3.5-Sonnet.</li>
  <li>Math Benchmarks: State-of-the-art performance in mathematical reasoning tasks.</li>
  <li>Code Competitions: Excels in coding benchmarks such as LiveCodeBench.</li>
  <li>Factual Knowledge: Demonstrates superior results in English and Chinese factuality benchmarks.</li>
</ol>

<hr />

<h3 id="inference-and-deployment">Inference and Deployment</h3>
<ol>
  <li>Prefilling Stage: Combines tensor parallelism (TP4), sequence parallelism (SP), and expert parallelism (EP32) for efficiency.</li>
  <li>Decoding Stage: Utilizes EP320 with IBGDA for low-latency communication.</li>
  <li>Dynamic Redundancy: Adjusts expert loads dynamically to optimize resource utilization.</li>
  <li>Separation of Stages: Prefilling and decoding stages are separated to enhance throughput.</li>
  <li>Hardware Utilization: Optimized for H800 GPUs with NVLink and InfiniBand interconnects.</li>
</ol>

<hr />

<h3 id="innovations-in-load-balancing-and-decoding">Innovations in Load Balancing and Decoding</h3>
<ol>
  <li>Bias-Based Routing: Introduces bias terms to ensure balanced expert loads dynamically.</li>
  <li>Speculative Decoding: Enhances generation latency using MTP modules.</li>
  <li>Redundant Experts: Duplicates high-load experts to balance GPU workloads.</li>
  <li>Node-Limited Routing: Restricts token routing to a maximum of 4 nodes to reduce communication overhead.</li>
  <li>No Token Dropping: Ensures all tokens are retained during training and inference.</li>
</ol>

<hr />

<h3 id="technical-details">Technical Details</h3>
<ol>
  <li>Cluster Configuration: Trained on a cluster with 2048 NVIDIA H800 GPUs.</li>
  <li>Pipeline Parallelism: Employs a 16-way parallelism scheme for scalability.</li>
  <li>Memory Footprint: Avoids costly tensor parallelism by optimizing memory usage.</li>
  <li>Custom Kernels: Develops specialized communication kernels to handle cross-node operations efficiently.</li>
  <li>Mixed Precision Optimization: Combines FP8 and BF16 formats for optimal training dynamics.</li>
</ol>

<hr />

<h3 id="evaluation-and-results">Evaluation and Results</h3>
<ol>
  <li>Comprehensive Benchmarks: Evaluated across diverse domains including education, coding, and reasoning.</li>
  <li>Open-Source Leadership: Emerges as the strongest open-source base model in its category.</li>
  <li>Comparison with Closed-Source Models: Performance comparable to GPT-4o and Claude-3.5-Sonnet.</li>
  <li>Strength in Chinese Knowledge: Outperforms leading models in Chinese factuality benchmarks.</li>
  <li>Long-Context Handling: Excels in tasks requiring extended context processing.</li>
</ol>

<hr />

<h3 id="future-directions">Future Directions</h3>
<ol>
  <li>Dynamic Redundancy Exploration: Investigating more adaptive redundancy strategies.</li>
  <li>Speculative Decoding Expansion: Exploring further uses of MTP for inference acceleration.</li>
  <li>Hardware Co-Design: Adapting to next-generation GPUs for enhanced performance.</li>
  <li>Broader Benchmark Coverage: Expanding evaluations to more diverse tasks.</li>
  <li>Sustainability: Reducing training costs further through algorithmic and hardware optimizations.</li>
</ol>

<hr />

<p>This document provides a comprehensive summary of DeepSeek-V3, encapsulating its architecture, training methodologies, benchmark performance, and future prospects. Let me know if you need further elaboration on specific sections or additional points!</p>



  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
