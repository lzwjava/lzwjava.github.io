<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Deep Learning Pioneers Before Hinton</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Learning Pioneers Before Hinton | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Deep Learning Pioneers Before Hinton" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-10-14-pre-hinton-pioneers-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-10-14-pre-hinton-pioneers-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Learning Pioneers Before Hinton" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Deep Learning Pioneers Before Hinton","url":"https://lzwjava.github.io/notes/2025-10-14-pre-hinton-pioneers-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=9a180898a5ab588f59681e106e72f8e76295c079">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=9a180898a5ab588f59681e106e72f8e76295c079" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Deep Learning Pioneers Before Hinton | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-10-14-pre-hinton-pioneers-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-10-14-pre-hinton-pioneers-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/pre-hinton-pioneers-en" selected>English</option>
        <option value="/pre-hinton-pioneers-zh" >中文</option>
        <option value="/pre-hinton-pioneers-ja" >日本語</option>
        <option value="/pre-hinton-pioneers-es" >Español</option>
        <option value="/pre-hinton-pioneers-hi" >हिंदी</option>
        <option value="/pre-hinton-pioneers-fr" >Français</option>
        <option value="/pre-hinton-pioneers-de" >Deutsch</option>
        <option value="/pre-hinton-pioneers-ar" >العربية</option>
        <option value="/pre-hinton-pioneers-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h3 id="alexey-ivakhnenkos-work">Alexey Ivakhnenko’s Work</h3>

<p>Alexey Grigoryevich Ivakhnenko, a Ukrainian-Soviet cyberneticist, is often called the “godfather of deep learning” for his pioneering efforts in the 1960s and 1970s. Working under resource constraints during the Cold War era—when computing power was millions of times more limited than today—he focused on multilayer neural networks that could automatically learn hierarchical representations of data.</p>

<ul>
  <li>
    <p><strong>1965: Group Method of Data Handling (GMDH)</strong>: Alongside Valentin Lapa, Ivakhnenko published the first general, working learning algorithm for supervised deep feedforward multilayer perceptrons (MLPs). This method trained networks layer by layer using regression analysis on input-output data pairs. It incrementally grew layers, trained them sequentially, and included pruning of unnecessary hidden units via validation sets. Crucially, it enabled the networks to learn distributed, internal representations of input data— a core idea in modern deep learning—without manual feature engineering. This predated similar concepts in Western AI by decades and was applied to real-world problems like pattern recognition and forecasting.</p>
  </li>
  <li>
    <p><strong>1971: Deep Network Implementation</strong>: Ivakhnenko demonstrated an 8-layer deep neural network using GMDH principles, showcasing scalable depth for complex tasks. His approach treated deep networks as a form of polynomial approximation, allowing automatic model selection and avoiding the “curse of dimensionality” in high-layer architectures.</p>
  </li>
</ul>

<p>Ivakhnenko’s GMDH evolved into a broader inductive modeling framework, influencing fields like control systems and economics. Despite its impact, much of his work was published in Russian and overlooked in English-language AI circles.</p>

<h3 id="shun-ichi-amaris-work">Shun-ichi Amari’s Work</h3>

<p>Shun-ichi Amari, a Japanese mathematician and neuroscientist, made foundational contributions to neural network theory in the 1960s and 1970s, emphasizing adaptive learning and geometric perspectives on information processing. His research bridged neuroscience and computation, laying groundwork for self-organizing systems.</p>

<ul>
  <li>
    <p><strong>1967-1968: Adaptive Pattern Classification and Stochastic Gradient Descent (SGD)</strong>: Amari proposed the first method for end-to-end training of deep MLPs using SGD, a optimization technique dating back to 1951 but newly applied to multilayer networks. In simulations with a five-layer network (two modifiable layers), his system learned to classify nonlinearly separable patterns by adjusting weights directly across layers. This enabled internal representations to emerge through gradient-based updates, a direct precursor to backpropagation-like methods, all under compute constraints billions of times harsher than modern standards.</p>
  </li>
  <li>
    <p><strong>1972: Adaptive Associative Memory Networks</strong>: Building on the 1925 Lenz-Ising model (a physics-based recurrent architecture), Amari introduced an adaptive version that learned to store and recall patterns by tuning connection weights based on correlations. It handled sequence processing and retrieved stored patterns from noisy or partial inputs via neural dynamics. Published first in Japanese in 1969, this work is seen as the origin of the “Hopfield network” for associative memory.</p>
  </li>
</ul>

<p>Amari also founded information geometry, a field using differential geometry to analyze statistical models and neural dynamics, which underpins modern probabilistic neural networks.</p>

<h3 id="context-in-the-2024-nobel-backlash">Context in the 2024 Nobel Backlash</h3>

<p>In his 2024 report “A Nobel Prize for Plagiarism,” Jürgen Schmidhuber argues that Hinton and Hopfield’s Nobel-winning ideas—such as the Boltzmann machine (1985) for learning representations and the Hopfield network (1982) for associative memory—repackaged Ivakhnenko’s layer-wise deep learning and Amari’s SGD/adaptive recurrent models without attribution. For instance, the Boltzmann machine omitted citations to Ivakhnenko’s 1965 internal representation learning and Amari’s 1967 SGD, while Hopfield’s network ignored Amari’s 1972 adaptive Ising model. Schmidhuber claims this omission persisted in later surveys and the Nobel’s scientific background document, highlighting a Western-centric bias in AI history.</p>

<p><a href="https://en.wikipedia.org/wiki/Alexey_Ivakhnenko">Alexey Ivakhnenko - Wikipedia</a><br />
<a href="https://developer.nvidia.com/blog/deep-learning-nutshell-history-training/">Deep Learning in a Nutshell: History and Training - NVIDIA Developer</a><br />
<a href="https://www.kyotoprize.org/en/laureates/shun-ichi_amari/">Shun-ichi Amari | Kyoto Prize</a><br />
<a href="https://modernsciences.org/japanese-ai-pioneers-history-shuninchi-amari-kunihiko-fukushima-december-2024/">Japanese scientists were pioneers of AI, yet they’re being written out of history</a><br />
<a href="https://people.idsia.ch/~juergen/physics-nobel-2024-plagiarism.pdf">A Nobel Prize for Plagiarism - Technical Report IDSIA-24-24</a><br />
<a href="https://harvardtechnologyreview.com/2024/11/18/machine-learning-stirs-controversy-in-nobel-prize-in-physics/">Machine Learning Stirs Controversy in Nobel Prize in Physics - Harvard Technology Review</a></p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    
      <p class="model">x-ai/grok-4-fast</p>
    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
