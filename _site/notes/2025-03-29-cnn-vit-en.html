<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Image Processing with CNNs and Vision Transformers (ViT)</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Image Processing with CNNs and Vision Transformers (ViT) | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Image Processing with CNNs and Vision Transformers (ViT)" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-03-29-cnn-vit-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-03-29-cnn-vit-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Image Processing with CNNs and Vision Transformers (ViT)" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Image Processing with CNNs and Vision Transformers (ViT)","url":"https://lzwjava.github.io/notes/2025-03-29-cnn-vit-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=ac3bd15ece605a1011ad180a8adcb805f218403e">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=ac3bd15ece605a1011ad180a8adcb805f218403e" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Image Processing with CNNs and Vision Transformers (ViT) | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-03-29-cnn-vit-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-03-29-cnn-vit-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.09</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/cnn-vit-en" selected>English</option>
        <option value="/cnn-vit-zh" >中文</option>
        <option value="/cnn-vit-ja" >日本語</option>
        <option value="/cnn-vit-es" >Español</option>
        <option value="/cnn-vit-hi" >हिंदी</option>
        <option value="/cnn-vit-fr" >Français</option>
        <option value="/cnn-vit-de" >Deutsch</option>
        <option value="/cnn-vit-ar" >العربية</option>
        <option value="/cnn-vit-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Below are code examples demonstrating how images are processed using <strong>Convolutional Neural Networks (CNNs)</strong> and <strong>Vision Transformers (ViT)</strong> in Python with <strong>PyTorch</strong>.</p>

<hr />

<h2 id="1-image-processing-with-a-cnn-convolutional-neural-network"><strong>1. Image Processing with a CNN (Convolutional Neural Network)</strong></h2>
<p>CNNs are widely used for image classification, object detection, and feature extraction.</p>

<h3 id="example-using-a-pre-trained-cnn-resnet"><strong>Example: Using a Pre-trained CNN (ResNet)</strong></h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Load a pre-trained ResNet model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>  <span class="c1"># Set to evaluation mode
</span>
<span class="c1"># Define image preprocessing
</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
<span class="p">])</span>

<span class="c1"># Load and preprocess an image
</span><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"example.jpg"</span><span class="p">)</span>  <span class="c1"># Replace with your image path
</span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension
</span>
<span class="c1"># Move to GPU if available
</span><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>

<span class="c1"># Extract features (before the final classification layer)
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Feature vector shape:"</span><span class="p">,</span> <span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># e.g., torch.Size([1, 1000])
</span></code></pre></div></div>
<p><strong>Explanation</strong>:</p>
<ol>
  <li><strong>ResNet18</strong> is a CNN architecture pre-trained on ImageNet.</li>
  <li>The image is preprocessed (resized, normalized).</li>
  <li>The model converts the image into a <strong>feature vector</strong> (e.g., 1000-dimensional for ResNet18).</li>
</ol>

<hr />

<h2 id="2-image-processing-with-a-vision-transformer-vit"><strong>2. Image Processing with a Vision Transformer (ViT)</strong></h2>
<p>ViTs treat images as sequences of patches and use self-attention mechanisms (like in NLP).</p>

<h3 id="example-using-a-pre-trained-vit-hugging-face"><strong>Example: Using a Pre-trained ViT (Hugging Face)</strong></h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ViTFeatureExtractor</span><span class="p">,</span> <span class="n">ViTModel</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Load a pre-trained Vision Transformer (ViT)
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"google/vit-base-patch16-224-in21k"</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">ViTFeatureExtractor</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ViTModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Load an image
</span><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"example.jpg"</span><span class="p">)</span>  <span class="c1"># Replace with your image path
</span>
<span class="c1"># Preprocess the image (convert to patches)
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>

<span class="c1"># Extract features (CLS token or patch embeddings)
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Get the feature vector (CLS token)
</span><span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Shape: [1, 768]
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Feature vector shape:"</span><span class="p">,</span> <span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># e.g., torch.Size([1, 768])
</span></code></pre></div></div>
<p><strong>Explanation</strong>:</p>
<ol>
  <li><strong>ViT</strong> splits the image into <strong>16x16 patches</strong> and processes them like tokens in NLP.</li>
  <li>The <code class="language-plaintext highlighter-rouge">CLS token</code> (first token) represents the entire image’s feature vector.</li>
  <li>The output is a <strong>768-dimensional vector</strong> (for <code class="language-plaintext highlighter-rouge">vit-base</code>).</li>
</ol>

<hr />

<h2 id="3-comparing-cnn-vs-vit-feature-extraction"><strong>3. Comparing CNN vs. ViT Feature Extraction</strong></h2>
<p>| Model | Approach | Feature Vector Size | Libraries |
|——-|———-|———————|———–|
| <strong>CNN (ResNet18)</strong> | Convolutional layers + pooling | 1000 (ImageNet classes) | <code class="language-plaintext highlighter-rouge">torchvision</code> |
| <strong>ViT (Google ViT-Base)</strong> | Patch embeddings + Transformer | 768 (hidden dimension) | <code class="language-plaintext highlighter-rouge">transformers</code> |</p>

<hr />

<h2 id="4-applications-of-image-feature-vectors"><strong>4. Applications of Image Feature Vectors</strong></h2>
<ul>
  <li><strong>Image Search</strong>: Compare feature vectors (e.g., cosine similarity).</li>
  <li><strong>Transfer Learning</strong>: Use pre-trained features for custom tasks.</li>
  <li><strong>Object Detection</strong>: Extract regions of interest (ROI) from features.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Compute similarity between two images
</span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="c1"># Assume features1 and features2 are extracted vectors
</span><span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">features1</span><span class="p">,</span> <span class="n">features2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Similarity score:"</span><span class="p">,</span> <span class="n">similarity</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<hr />

<h2 id="key-takeaways"><strong>Key Takeaways</strong></h2>
<ol>
  <li><strong>CNNs</strong> (e.g., ResNet) use <strong>convolutional layers</strong> to extract hierarchical features.</li>
  <li><strong>ViTs</strong> (e.g., Google’s ViT) treat images as <strong>patch sequences</strong> and use <strong>self-attention</strong>.</li>
  <li>Both convert images into <strong>feature vectors</strong> for downstream tasks.</li>
</ol>

<p>Would you like an example on <strong>training a custom CNN/ViT</strong> from scratch? Let me know! 🚀</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
