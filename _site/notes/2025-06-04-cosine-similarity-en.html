<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Cosine Similarity</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Cosine Similarity | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Cosine Similarity" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-06-04-cosine-similarity-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-06-04-cosine-similarity-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Cosine Similarity" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Cosine Similarity","url":"https://lzwjava.github.io/notes/2025-06-04-cosine-similarity-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=f129006fa2eced53824cd281a617ce668abdbb21">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=f129006fa2eced53824cd281a617ce668abdbb21" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Cosine Similarity | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-06-04-cosine-similarity-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-06-04-cosine-similarity-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/cosine-similarity-en" selected>English</option>
        <option value="/cosine-similarity-zh" >中文</option>
        <option value="/cosine-similarity-ja" >日本語</option>
        <option value="/cosine-similarity-es" >Español</option>
        <option value="/cosine-similarity-hi" >हिंदी</option>
        <option value="/cosine-similarity-fr" >Français</option>
        <option value="/cosine-similarity-de" >Deutsch</option>
        <option value="/cosine-similarity-ar" >العربية</option>
        <option value="/cosine-similarity-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Cosine similarity is a widely used metric in machine learning to measure the similarity between two vectors in a high-dimensional space. It’s particularly popular in fields like natural language processing (NLP), information retrieval, and recommendation systems due to its ability to capture the orientation (or angle) between vectors, rather than their magnitude. This makes it robust for comparing objects like text documents, user preferences, or embeddings, where the direction of the vector matters more than its length.</p>

<h3 id="what-is-cosine-similarity">What is Cosine Similarity?</h3>

<p>Cosine similarity quantifies how similar two vectors are by calculating the cosine of the angle between them. Mathematically, it’s defined as:</p>

<p>\[
\text{Cosine Similarity} = \cos(\theta) = \frac{A \cdot B}{|A| |B|}
\]</p>

<p>Where:</p>
<ul>
  <li>\( A \) and \( B \) are two vectors (e.g., representing documents, embeddings, or feature sets).</li>
  <li>\( A \cdot B \) is the dot product of the vectors, computed as \( \sum_{i=1}^n A_i B_i \).</li>
  <li>\( |A| \) and \( |B| \) are the Euclidean norms (magnitudes) of vectors \( A \) and \( B \), calculated as \( \sqrt{\sum_{i=1}^n A_i^2} \) and \( \sqrt{\sum_{i=1}^n B_i^2} \), respectively.</li>
  <li>\( \theta \) is the angle between the vectors.</li>
</ul>

<p>The result ranges from:</p>
<ul>
  <li><strong>1</strong>: Vectors are identical in direction (angle = 0°).</li>
  <li><strong>0</strong>: Vectors are orthogonal (angle = 90°), indicating no similarity.</li>
  <li><strong>-1</strong>: Vectors are opposite (angle = 180°), indicating maximum dissimilarity.</li>
</ul>

<h3 id="key-properties">Key Properties</h3>

<ol>
  <li><strong>Range</strong>: Cosine similarity values lie between -1 and 1, making it easy to interpret.</li>
  <li><strong>Magnitude Independence</strong>: Since the vectors are normalized by their magnitudes, cosine similarity focuses on the direction, not the length. This is useful when comparing documents of different lengths or embeddings with varying scales.</li>
  <li><strong>Non-Negative Features</strong>: In many applications (e.g., text data with term frequencies), vectors have non-negative components, so the similarity typically ranges from 0 to 1.</li>
  <li><strong>Computational Efficiency</strong>: The dot product and norm calculations are straightforward, making cosine similarity computationally efficient for high-dimensional data.</li>
</ol>

<h3 id="how-its-used-in-machine-learning">How It’s Used in Machine Learning</h3>

<p>Cosine similarity is applied across various machine learning tasks due to its versatility:</p>

<ol>
  <li><strong>Text Analysis and NLP</strong>:
    <ul>
      <li><strong>Document Similarity</strong>: In tasks like clustering or search engines, documents are represented as vectors (e.g., TF-IDF or word embeddings like Word2Vec, GloVe, or BERT). Cosine similarity measures how similar two documents are based on their content.</li>
      <li><strong>Sentiment Analysis</strong>: Comparing sentiment vectors of text snippets.</li>
      <li><strong>Plagiarism Detection</strong>: Identifying similarities between texts by comparing their vector representations.</li>
    </ul>
  </li>
  <li><strong>Recommendation Systems</strong>:
    <ul>
      <li>Cosine similarity is used to compare user or item profiles (e.g., in collaborative filtering). For example, it can measure how similar two users’ preferences are based on their ratings or behavior.</li>
      <li>It’s effective in content-based filtering, where items (e.g., movies, products) are represented as feature vectors.</li>
    </ul>
  </li>
  <li><strong>Image and Audio Processing</strong>:
    <ul>
      <li>In computer vision, cosine similarity compares feature vectors extracted from images (e.g., from CNNs) to measure visual similarity.</li>
      <li>In audio processing, it’s used to compare spectrograms or embeddings of sound clips.</li>
    </ul>
  </li>
  <li><strong>Clustering and Classification</strong>:
    <ul>
      <li>In clustering algorithms (e.g., K-means with text data), cosine similarity serves as a distance metric to group similar items.</li>
      <li>In classification tasks, it’s used to compare input vectors to class prototypes.</li>
    </ul>
  </li>
  <li><strong>Anomaly Detection</strong>:
    <ul>
      <li>Cosine similarity can identify outliers by comparing data points to a centroid or expected pattern. Low similarity indicates potential anomalies.</li>
    </ul>
  </li>
</ol>

<h3 id="example-cosine-similarity-in-text-analysis">Example: Cosine Similarity in Text Analysis</h3>

<p>Suppose we have two documents represented as TF-IDF vectors:</p>
<ul>
  <li>Document 1: \( A = [2, 1, 0, 3] \) (e.g., word frequencies for four terms).</li>
  <li>Document 2: \( B = [1, 1, 1, 0] \).</li>
</ul>

<p><strong>Step 1: Compute the Dot Product</strong>:
\[
A \cdot B = (2 \cdot 1) + (1 \cdot 1) + (0 \cdot 1) + (3 \cdot 0) = 2 + 1 + 0 + 0 = 3
\]</p>

<p><strong>Step 2: Compute the Norms</strong>:
\[
|A| = \sqrt{2^2 + 1^2 + 0^2 + 3^2} = \sqrt{4 + 1 + 0 + 9} = \sqrt{14} \approx 3.742
\]
\[
|B| = \sqrt{1^2 + 1^2 + 1^2 + 0^2} = \sqrt{1 + 1 + 1 + 0} = \sqrt{3} \approx 1.732
\]</p>

<p><strong>Step 3: Compute Cosine Similarity</strong>:
\[
\cos(\theta) = \frac{A \cdot B}{|A| |B|} = \frac{3}{3.742 \cdot 1.732} \approx \frac{3}{6.483} \approx 0.462
\]</p>

<p>The cosine similarity is approximately 0.462, indicating moderate similarity between the documents.</p>

<h3 id="advantages-of-cosine-similarity">Advantages of Cosine Similarity</h3>

<ul>
  <li><strong>Scale Invariance</strong>: It’s unaffected by the magnitude of vectors, making it ideal for text data where document length varies.</li>
  <li><strong>Handles High-Dimensional Data</strong>: Effective in sparse, high-dimensional spaces (e.g., text data with thousands of features).</li>
  <li><strong>Intuitive Interpretation</strong>: The cosine value directly relates to the angle, providing a clear measure of similarity.</li>
</ul>

<h3 id="limitations">Limitations</h3>

<ul>
  <li><strong>Ignores Magnitude</strong>: In some cases, magnitude differences are important (e.g., when comparing absolute quantities).</li>
  <li><strong>Assumes Linear Relationships</strong>: Cosine similarity assumes that similarity is best captured by angular distance, which may not always hold.</li>
  <li><strong>Sparse Data Sensitivity</strong>: In very sparse vectors, cosine similarity may be less discriminative, as many dimensions contribute little to the dot product.</li>
</ul>

<h3 id="cosine-similarity-vs-other-metrics">Cosine Similarity vs. Other Metrics</h3>

<ul>
  <li><strong>Euclidean Distance</strong>: Measures straight-line distance and is sensitive to magnitude, unlike cosine similarity. Cosine is preferred when direction matters more than absolute differences.</li>
  <li><strong>Jaccard Similarity</strong>: Used for sets (e.g., binary data), focusing on shared elements rather than vector orientation.</li>
  <li><strong>Pearson Correlation</strong>: Measures linear correlation, accounting for mean-centered data, while cosine similarity works on raw vectors.</li>
</ul>

<h3 id="practical-implementation">Practical Implementation</h3>

<p>Cosine similarity is implemented in many machine learning libraries:</p>
<ul>
  <li><strong>Python</strong>: <code class="language-plaintext highlighter-rouge">scikit-learn</code> provides <code class="language-plaintext highlighter-rouge">cosine_similarity</code> in <code class="language-plaintext highlighter-rouge">sklearn.metrics.pairwise</code>.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span>  <span class="c1"># Output: [[0.46225063]]
</span></code></pre></div>    </div>
  </li>
  <li><strong>TensorFlow/PyTorch</strong>: Used for comparing embeddings in deep learning models.</li>
  <li><strong>Spark</strong>: For large-scale distributed similarity computations.</li>
</ul>

<h3 id="advanced-applications">Advanced Applications</h3>

<ol>
  <li><strong>Word Embeddings</strong>: Cosine similarity is the standard for comparing word or sentence embeddings (e.g., BERT, Sentence-BERT) to measure semantic similarity.</li>
  <li><strong>Graph-Based Models</strong>: Used in node similarity for graph neural networks.</li>
  <li><strong>Personalized Search</strong>: Search engines like Elasticsearch use cosine similarity to rank documents by relevance to a query.</li>
</ol>

<h3 id="conclusion">Conclusion</h3>

<p>Cosine similarity is a powerful, intuitive, and computationally efficient metric for measuring similarity in machine learning. Its focus on vector orientation makes it ideal for text, embeddings, and recommendation systems, though it’s less suited for tasks where magnitude matters. By understanding its strengths and limitations, practitioners can apply it effectively across diverse applications, from NLP to computer vision. If you’d like a deeper dive into a specific use case or implementation, let me know!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
