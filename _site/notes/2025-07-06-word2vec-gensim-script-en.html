<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Word2Vec Script with Gensim</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Word2Vec Script with Gensim | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Word2Vec Script with Gensim" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-07-06-word2vec-gensim-script-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-07-06-word2vec-gensim-script-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Word2Vec Script with Gensim" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Word2Vec Script with Gensim","url":"https://lzwjava.github.io/notes/2025-07-06-word2vec-gensim-script-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=5014237bccf8eb405d07d2295d288aaeb5d7438b">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=5014237bccf8eb405d07d2295d288aaeb5d7438b" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Word2Vec Script with Gensim | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-07-06-word2vec-gensim-script-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-07-06-word2vec-gensim-script-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/word2vec-gensim-script-en" selected>English</option>
        <option value="/word2vec-gensim-script-zh" >中文</option>
        <option value="/word2vec-gensim-script-ja" >日本語</option>
        <option value="/word2vec-gensim-script-es" >Español</option>
        <option value="/word2vec-gensim-script-hi" >हिंदी</option>
        <option value="/word2vec-gensim-script-fr" >Français</option>
        <option value="/word2vec-gensim-script-de" >Deutsch</option>
        <option value="/word2vec-gensim-script-ar" >العربية</option>
        <option value="/word2vec-gensim-script-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>To provide you with a Python script for Word2Vec, I need to make a couple of assumptions about what you’re looking for, as “a script about Word2Vec” can be quite broad. I’ll provide two common scenarios:</p>

<ol>
  <li><strong>A simple script using <code class="language-plaintext highlighter-rouge">gensim</code> for training and basic usage:</strong> This is the most common and practical way to use Word2Vec in Python.</li>
  <li><strong>A very basic, conceptual implementation from scratch:</strong> This is more for understanding the core algorithms, but less practical for real-world use due to complexity and optimization.</li>
</ol>

<p>Given your request, I’ll prioritize the <code class="language-plaintext highlighter-rouge">gensim</code> approach as it’s what most people would use.</p>

<hr />

<h3 id="python-script-for-word2vec-using-gensim">Python Script for Word2Vec using <code class="language-plaintext highlighter-rouge">gensim</code></h3>

<p>This script will demonstrate:</p>

<ul>
  <li>Preparing sample text data.</li>
  <li>Training a Word2Vec model.</li>
  <li>Finding most similar words.</li>
  <li>Performing word analogies.</li>
  <li>Saving and loading the model.</li>
</ul>

<p><strong>Prerequisites:</strong></p>

<p>You’ll need to install the <code class="language-plaintext highlighter-rouge">gensim</code> library:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gensim
</code></pre></div></div>

<p><strong>Python Script (<code class="language-plaintext highlighter-rouge">word2vec_gensim_example.py</code>):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="kn">from</span> <span class="nn">gensim.utils</span> <span class="kn">import</span> <span class="n">simple_preprocess</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="c1"># Configure logging to see training progress
</span><span class="n">logging</span><span class="p">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="p">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># --- 1. Prepare Sample Text Data ---
# In a real-world scenario, you would load your text data from files.
# Here, we use a simple list of sentences.
</span><span class="n">raw_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"I love natural language processing."</span><span class="p">,</span>
    <span class="s">"Word embeddings are a fascinating topic."</span><span class="p">,</span>
    <span class="s">"Machine learning revolutionizes many industries."</span><span class="p">,</span>
    <span class="s">"Deep learning is a subset of machine learning."</span><span class="p">,</span>
    <span class="s">"Natural language processing uses various techniques."</span><span class="p">,</span>
    <span class="s">"Computers understand human language better with word vectors."</span><span class="p">,</span>
    <span class="s">"The quick brown fox jumps over the lazy dog."</span><span class="p">,</span>
    <span class="s">"Dog and cat are common pets."</span><span class="p">,</span>
    <span class="s">"King is to Queen as Man is to Woman."</span>
<span class="p">]</span>

<span class="c1"># Preprocess the sentences: tokenize and convert to lowercase
# gensim's simple_preprocess is convenient for this
</span><span class="n">tokenized_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">raw_sentences</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Tokenized Sentences ---"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tokenized_sentences</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="c1"># --- 2. Train the Word2Vec Model ---
# Parameters explained:
#   vector_size: Dimensionality of the word vectors.
#   window: Maximum distance between the current and predicted word within a sentence.
#   min_count: Ignores all words with total frequency lower than this.
#   sg: Training algorithm: 0 for CBOW, 1 for Skip-gram. Skip-gram is generally better for rare words.
#   workers: Use these many worker threads to train the model (=faster training).
#   epochs: Number of iterations (epochs) over the corpus.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span>
    <span class="n">sentences</span><span class="o">=</span><span class="n">tokenized_sentences</span><span class="p">,</span>
    <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>      <span class="c1"># Example: 100-dimensional vectors
</span>    <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>             <span class="c1"># Context window of 5 words on either side
</span>    <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>          <span class="c1"># Include all words (for this small example)
</span>    <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>                 <span class="c1"># Use Skip-gram (sg=1)
</span>    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># Use 4 CPU cores for training
</span>    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>             <span class="c1"># Train for 10 epochs
</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Model Training Complete ---"</span><span class="p">)</span>

<span class="c1"># --- 3. Explore the Model ---
</span>
<span class="c1"># Get the vector for a specific word
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">word_vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="s">'machine'</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Vector for 'machine' (first 10 dimensions): </span><span class="si">{</span><span class="n">word_vector</span><span class="p">[</span><span class="si">:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Vector for 'machine' shape: </span><span class="si">{</span><span class="n">word_vector</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">'machine' not found in vocabulary."</span><span class="p">)</span>

<span class="c1"># Find the most similar words
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Most Similar Words to 'machine' ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">similar_words</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'machine'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="n">similar_words</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">'machine' not found in vocabulary."</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Most Similar Words to 'dog' ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">similar_words_dog</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'dog'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="n">similar_words_dog</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">'dog' not found in vocabulary."</span><span class="p">)</span>


<span class="c1"># Perform word analogies (e.g., King - Man + Woman = Queen)
# This may not work perfectly with very small datasets, but demonstrates the concept.
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Word Analogies (King - Man + Woman = ?) ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">'king'</span><span class="p">,</span> <span class="s">'woman'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">'man'</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"King - Man + Woman is similar to: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s"> (similarity: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"No result for analogy. Vocabulary might be too small."</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"One or more words for analogy not found in vocabulary: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An error occurred during analogy: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Check similarity between two words
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Similarity between 'machine' and 'learning' ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'machine'</span><span class="p">,</span> <span class="s">'learning'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similarity: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"One or both words not found in vocabulary."</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Similarity between 'dog' and 'cat' ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">similarity_pets</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'dog'</span><span class="p">,</span> <span class="s">'cat'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similarity: </span><span class="si">{</span><span class="n">similarity_pets</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"One or both words not found in vocabulary."</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Similarity between 'dog' and 'computer' ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">similarity_diff</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'dog'</span><span class="p">,</span> <span class="s">'computer'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similarity: </span><span class="si">{</span><span class="n">similarity_diff</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"One or both words not found in vocabulary."</span><span class="p">)</span>

<span class="c1"># --- 4. Save and Load the Model ---
</span><span class="n">model_path</span> <span class="o">=</span> <span class="s">"word2vec_model.bin"</span>
<span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Model saved to </span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Load the model back
</span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Model loaded from </span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Verify loaded model
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">--- Verify Loaded Model (Most Similar Words to 'machine' using loaded model) ---"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">similar_words_loaded</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'machine'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="n">similar_words_loaded</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">'machine' not found in vocabulary in loaded model."</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Script execution complete."</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>How to Run the Script:</strong></p>

<ol>
  <li>Save the code above as a Python file (e.g., <code class="language-plaintext highlighter-rouge">word2vec_gensim_example.py</code>).</li>
  <li>Open your terminal or command prompt.</li>
  <li>Navigate to the directory where you saved the file.</li>
  <li>Run the script using: <code class="language-plaintext highlighter-rouge">python word2vec_gensim_example.py</code></li>
</ol>

<hr />

<h3 id="key-concepts-in-word2vec-gensim-implementation">Key Concepts in Word2Vec (<code class="language-plaintext highlighter-rouge">gensim</code> implementation)</h3>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">gensim.models.Word2Vec</code></strong>: The main class for training.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">sentences</code></strong>: An iterable of iterables, where the outer iterable is the corpus and the inner iterables are lists of words (tokenized sentences).</li>
  <li><strong><code class="language-plaintext highlighter-rouge">vector_size</code></strong>: The number of dimensions for the word vectors. Higher dimensions can capture more nuances but require more data and computation.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">window</code></strong>: The maximum distance between the current word and the words around it in a sentence.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">min_count</code></strong>: Words that appear fewer times than <code class="language-plaintext highlighter-rouge">min_count</code> are ignored. This helps filter out very rare words that might not have meaningful embeddings.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">sg</code> (Skip-gram vs. CBOW)</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">sg=0</code> (CBOW - Continuous Bag of Words): Predicts the current word based on its context. Faster to train.</li>
      <li><code class="language-plaintext highlighter-rouge">sg=1</code> (Skip-gram): Predicts surrounding words given the current word. Generally performs better for rare words.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">workers</code></strong>: Number of CPU cores to use for training, speeding up the process.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">epochs</code></strong>: Number of times the algorithm iterates over the entire dataset. More epochs can lead to better quality embeddings but also take longer.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">model.wv</code></strong>: The “word vectors” object (<code class="language-plaintext highlighter-rouge">KeyedVectors</code>) which contains the trained word embeddings and methods like <code class="language-plaintext highlighter-rouge">most_similar</code>, <code class="language-plaintext highlighter-rouge">similarity</code>, etc. You can also access vectors directly like <code class="language-plaintext highlighter-rouge">model.wv['word']</code>.</li>
</ul>

<hr />

<p>This <code class="language-plaintext highlighter-rouge">gensim</code> script provides a solid foundation for working with Word2Vec. For real-world applications, you would typically use a much larger corpus (e.g., Wikipedia, Common Crawl) and tune the hyperparameters for optimal performance.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
