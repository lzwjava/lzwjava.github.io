<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Long Short-Term Memory Networks</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Long Short-Term Memory Networks | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Long Short-Term Memory Networks" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-04-01-long-short-term-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-04-01-long-short-term-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Long Short-Term Memory Networks" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Long Short-Term Memory Networks","url":"https://lzwjava.github.io/notes/2025-04-01-long-short-term-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=b09586fd9b58750f3ef2db8c63a11ecd352c0a95">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=b09586fd9b58750f3ef2db8c63a11ecd352c0a95" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Long Short-Term Memory Networks | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-04-01-long-short-term-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-04-01-long-short-term-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/long-short-term-en" selected>English</option>
        <option value="/long-short-term-zh" >中文</option>
        <option value="/long-short-term-ja" >日本語</option>
        <option value="/long-short-term-es" >Español</option>
        <option value="/long-short-term-hi" >हिंदी</option>
        <option value="/long-short-term-fr" >Français</option>
        <option value="/long-short-term-de" >Deutsch</option>
        <option value="/long-short-term-ar" >العربية</option>
        <option value="/long-short-term-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Long Short-Term Memory (LSTM) networks are a type of recurrent neural network architecture specifically designed to address the vanishing gradient problem and enable the learning of long-term dependencies in sequential data. They achieve this through a more sophisticated internal structure called an <strong>LSTM cell</strong>.</p>

<p>Here’s a breakdown of how an LSTM cell works:</p>

<p><strong>Core Idea: The Cell State</strong></p>

<p>The central concept in an LSTM is the <strong>cell state</strong> (often denoted as ‘C<sub>t</sub>’). Think of the cell state as a conveyor belt running through the entire sequence. It carries information relevant to the long-term history of the sequence. Information can be added to or removed from the cell state as it flows through the network via structures called <strong>gates</strong>.</p>

<p><strong>The Gates</strong></p>

<p>LSTM cells have three main gates that regulate the flow of information:</p>

<ol>
  <li><strong>Forget Gate:</strong> This gate decides what information from the previous cell state should be discarded.
    <ul>
      <li>It receives the previous hidden state (h<sub>t-1</sub>) and the current input (x<sub>t</sub>).</li>
      <li>These are passed through a neural network layer followed by a <strong>sigmoid activation function</strong>.</li>
      <li>The sigmoid function outputs values between 0 and 1. A value close to 0 means “completely forget this information,” while a value close to 1 means “completely keep this information.”</li>
      <li>Mathematically, the forget gate’s output (f<sub>t</sub>) is calculated as:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
</code></pre></div>        </div>
        <p>where:</p>
        <ul>
          <li>σ is the sigmoid function.</li>
          <li>W<sub>f</sub> is the weight matrix for the forget gate.</li>
          <li>[h<sub>t-1</sub>, x_t] is the concatenation of the previous hidden state and the current input.</li>
          <li>b<sub>f</sub> is the bias vector for the forget gate.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Input Gate:</strong> This gate decides what new information from the current input should be added to the cell state. This process involves two steps:
    <ul>
      <li><strong>Input Gate Layer:</strong> A sigmoid layer decides which values we’ll update.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
</code></pre></div>        </div>
        <p>where:</p>
        <ul>
          <li>σ is the sigmoid function.</li>
          <li>W<sub>i</sub> is the weight matrix for the input gate.</li>
          <li>[h<sub>t-1</sub>, x_t] is the concatenation of the previous hidden state and the current input.</li>
          <li>b<sub>i</sub> is the bias vector for the input gate.</li>
        </ul>
      </li>
      <li><strong>Candidate Values Layer:</strong> A tanh layer creates a vector of new candidate values (candidate cell state, denoted as ‘C̃<sub>t</sub>’) that could be added to the cell state. The tanh function outputs values between -1 and 1, which helps in regulating the network.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
</code></pre></div>        </div>
        <p>where:</p>
        <ul>
          <li>tanh is the hyperbolic tangent function.</li>
          <li>W<sub>C</sub> is the weight matrix for the candidate cell state.</li>
          <li>[h<sub>t-1</sub>, x_t] is the concatenation of the previous hidden state and the current input.</li>
          <li>b<sub>C</sub> is the bias vector for the candidate cell state.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Output Gate:</strong> This gate decides what information from the current cell state should be output as the hidden state for the current time step.
    <ul>
      <li>It receives the previous hidden state (h<sub>t-1</sub>) and the current input (x<sub>t</sub>).</li>
      <li>These are passed through a neural network layer followed by a <strong>sigmoid activation function</strong> to determine which parts of the cell state to output.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
</code></pre></div>        </div>
        <p>where:</p>
        <ul>
          <li>σ is the sigmoid function.</li>
          <li>W<sub>o</sub> is the weight matrix for the output gate.</li>
          <li>[h<sub>t-1</sub>, x_t] is the concatenation of the previous hidden state and the current input.</li>
          <li>b<sub>o</sub> is the bias vector for the output gate.</li>
        </ul>
      </li>
      <li>The cell state is then passed through a <strong>tanh function</strong> to squash the values between -1 and 1.</li>
      <li>Finally, the output of the sigmoid gate is multiplied element-wise with the output of the tanh function applied to the cell state. This becomes the new hidden state (h<sub>t</sub>) which is passed to the next time step and can also be used for making predictions.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  h_t = o_t * tanh(C_t)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Updating the Cell State</strong></p>

<p>The cell state is updated based on the decisions made by the forget and input gates:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>C_t = f_t * C_{t-1} + i_t * C̃_t
</code></pre></div></div>

<ul>
  <li>The forget gate (f<sub>t</sub>) determines how much of the previous cell state (C<sub>t-1</sub>) to keep. If f<sub>t</sub> is close to 0, the information is mostly forgotten. If it’s close to 1, the information is mostly kept.</li>
  <li>The input gate (i<sub>t</sub>) determines how much of the new candidate values (C̃<sub>t</sub>) should be added to the cell state. If i<sub>t</sub> is close to 0, the new information is mostly ignored. If it’s close to 1, the new information is mostly added.</li>
</ul>

<p><strong>How LSTMs Solve the Vanishing Gradient Problem:</strong></p>

<p>The key to LSTM’s ability to handle long-term dependencies lies in the cell state and the gating mechanisms. The cell state acts as a direct pathway for information to flow across many time steps without being repeatedly multiplied by weights, which is the primary cause of vanishing gradients in traditional RNNs.</p>

<p>The gates allow the LSTM to learn when to store, access, and forget information in the cell state. This selective memory mechanism enables the network to retain relevant information over long sequences and use it when needed, effectively mitigating the vanishing gradient problem and allowing for the learning of long-term dependencies.</p>

<p>In essence, LSTMs provide a way for the network to maintain a “memory” over extended periods, making them highly effective for tasks involving sequential data with long-range dependencies.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
