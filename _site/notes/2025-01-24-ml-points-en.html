<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>ML, DL, and GPT</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ML, DL, and GPT | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="ML, DL, and GPT" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-01-24-ml-points-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-01-24-ml-points-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ML, DL, and GPT" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"ML, DL, and GPT","url":"https://lzwjava.github.io/notes/2025-01-24-ml-points-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=e6d2d2cceee29cf846f2c21bcf3046831c26c0c5">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=e6d2d2cceee29cf846f2c21bcf3046831c26c0c5" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       ML, DL, and GPT | Original
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-01-24-ml-points-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!-- 
    <a href="#" id="playAudioButton" class="button audio-button" data-file-path="notes/2025-01-24-ml-points-en.md">Audio</a>
     -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-01-24-ml-points-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/ml-points-en" selected>English</option>
        <option value="/ml-points-zh" >中文</option>
        <option value="/ml-points-ja" >日本語</option>
        <option value="/ml-points-es" >Español</option>
        <option value="/ml-points-hi" >हिंदी</option>
        <option value="/ml-points-fr" >Français</option>
        <option value="/ml-points-de" >Deutsch</option>
        <option value="/ml-points-ar" >العربية</option>
        <option value="/ml-points-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <ol>
  <li>
    <p>Machine Learning (ML) is a field of computer science that enables systems to learn from data and improve their performance without explicit programming.</p>
  </li>
  <li>
    <p>Deep Learning (DL) is a subfield of ML that utilizes multi-layered neural networks to model complex patterns in data.</p>
  </li>
  <li>
    <p>Neural Networks are computational models inspired by the human brain, composed of interconnected nodes (neurons) that process information in layers.</p>
  </li>
  <li>
    <p>Training Data is the labeled or unlabeled dataset used to teach a machine learning model how to perform a task.</p>
  </li>
  <li>
    <p>Supervised Learning involves training a model on labeled data, where each example has an input and an associated correct output.</p>
  </li>
  <li>
    <p>Unsupervised Learning uses unlabeled data, allowing the model to discover hidden patterns or groupings without explicit instruction.</p>
  </li>
  <li>
    <p>Reinforcement Learning (RL) trains agents to make decisions by rewarding desired behaviors and penalizing undesirable ones.</p>
  </li>
  <li>
    <p>Generative Models learn to produce new data similar to their training examples (e.g., text, images).</p>
  </li>
  <li>
    <p>Discriminative Models focus on classifying inputs into categories or predicting specific outcomes.</p>
  </li>
  <li>
    <p>Transfer Learning allows a model trained on one task to be reused or fine-tuned on a related task.</p>
  </li>
  <li>
    <p>GPT (Generative Pre-trained Transformer) is a family of large language models developed by OpenAI that can generate human-like text.</p>
  </li>
  <li>
    <p>ChatGPT is an interactive variant of GPT, fine-tuned for conversation and instruction-following tasks.</p>
  </li>
  <li>
    <p>Transformer Architecture was introduced in the paper “Attention Is All You Need,” revolutionizing natural language processing by relying on attention mechanisms.</p>
  </li>
  <li>
    <p>Self-Attention mechanisms let the model weigh different parts of the input sequence when constructing an output representation.</p>
  </li>
  <li>
    <p>Positional Encoding in Transformers helps the model identify the order of tokens in a sequence.</p>
  </li>
  <li>
    <p>Pre-training is the initial phase where a model learns general features from large-scale data before being fine-tuned on specific tasks.</p>
  </li>
  <li>
    <p>Fine-tuning is the process of taking a pre-trained model and adapting it to a narrower task using a smaller, task-specific dataset.</p>
  </li>
  <li>
    <p>Language Modeling is the task of predicting the next token (word or subword) in a sequence, foundational to GPT-like models.</p>
  </li>
  <li>
    <p>Zero-shot Learning allows a model to handle tasks without explicit training examples, relying on learned general knowledge.</p>
  </li>
  <li>
    <p>Few-shot Learning leverages a limited number of task-specific examples to guide model predictions or behaviors.</p>
  </li>
  <li>
    <p>RLHF (Reinforcement Learning from Human Feedback) is used to align model outputs with human preferences and values.</p>
  </li>
  <li>
    <p>Human Feedback can include rankings or labels that guide the model’s generation toward more desired responses.</p>
  </li>
  <li>
    <p>Prompt Engineering is the art of crafting input queries or instructions to guide large language models effectively.</p>
  </li>
  <li>
    <p>Context Window refers to the maximum amount of text the model can process at once; GPT models have a limited context length.</p>
  </li>
  <li>
    <p>Inference is the stage where a trained model makes predictions or generates outputs given new inputs.</p>
  </li>
  <li>
    <p>Parameter Count is a key factor in model capacity; larger models can capture more complex patterns but require more computation.</p>
  </li>
  <li>
    <p>Model Compression techniques (e.g., pruning, quantization) reduce a model’s size and speed up inference with minimal accuracy loss.</p>
  </li>
  <li>
    <p>Attention Heads in Transformers process different aspects of the input in parallel, improving representational power.</p>
  </li>
  <li>
    <p>Masked Language Modeling (e.g., in BERT) involves predicting missing tokens in a sentence, helping the model learn context.</p>
  </li>
  <li>
    <p>Causal Language Modeling (e.g., in GPT) involves predicting the next token based on all previous tokens.</p>
  </li>
  <li>
    <p>Encoder-Decoder Architecture (e.g., T5) uses one network to encode the input and another to decode it into a target sequence.</p>
  </li>
  <li>
    <p>Convolutional Neural Networks (CNNs) excel at processing grid-like data (e.g., images) via convolutional layers.</p>
  </li>
  <li>
    <p>Recurrent Neural Networks (RNNs) process sequential data by passing hidden states along time steps, though they can struggle with long-term dependencies.</p>
  </li>
  <li>
    <p>Long Short-Term Memory (LSTM) and GRU are RNN variants designed to better capture long-range dependencies.</p>
  </li>
  <li>
    <p>Batch Normalization helps stabilize training by normalizing intermediate layer outputs.</p>
  </li>
  <li>
    <p>Dropout is a regularization technique that randomly “drops” neurons during training to prevent overfitting.</p>
  </li>
  <li>
    <p>Optimizer Algorithms like Stochastic Gradient Descent (SGD), Adam, and RMSProp update model parameters based on gradients.</p>
  </li>
  <li>
    <p>Learning Rate is a hyperparameter that determines how drastically weights are updated during training.</p>
  </li>
  <li>
    <p>Hyperparameters (e.g., batch size, number of layers) are configuration settings chosen before training to control how learning unfolds.</p>
  </li>
  <li>
    <p>Model Overfitting occurs when a model learns training data too well, failing to generalize to new data.</p>
  </li>
  <li>
    <p>Regularization Techniques (e.g., L2 weight decay, dropout) help reduce overfitting and improve generalization.</p>
  </li>
  <li>
    <p>Validation Set is used to tune hyperparameters, while the Test Set evaluates the final performance of the model.</p>
  </li>
  <li>
    <p>Cross-validation splits data into multiple subsets, systematically training and validating to get a more robust performance estimate.</p>
  </li>
  <li>
    <p>Gradient Exploding and Vanishing problems occur in deep networks, making training unstable or ineffective.</p>
  </li>
  <li>
    <p>Residual Connections (skip connections) in networks like ResNet help mitigate vanishing gradients by shortcutting data paths.</p>
  </li>
  <li>
    <p>Scaling Laws suggest that increasing model size and data generally leads to better performance.</p>
  </li>
  <li>
    <p>Compute Efficiency is critical; training large models requires optimized hardware (GPUs, TPUs) and algorithms.</p>
  </li>
  <li>
    <p>Ethical Considerations include bias, fairness, and potential harm—ML models must be carefully tested and monitored.</p>
  </li>
  <li>
    <p>Data Augmentation artificially expands training datasets to improve model robustness (especially in image and speech tasks).</p>
  </li>
  <li>
    <p>Data Preprocessing (e.g., tokenization, normalization) is essential for effective model training.</p>
  </li>
  <li>
    <p>Tokenization splits text into tokens (words or subwords), the fundamental units processed by language models.</p>
  </li>
  <li>
    <p>Vector Embeddings represent tokens or concepts as numerical vectors, preserving semantic relationships.</p>
  </li>
  <li>
    <p>Positional Embeddings add information about the position of each token to help a Transformer understand sequence order.</p>
  </li>
  <li>
    <p>Attention Weights reveal how a model distributes focus across different parts of the input.</p>
  </li>
  <li>
    <p>Beam Search is a decoding strategy in language models that keeps multiple candidate outputs at each step to find the best overall sequence.</p>
  </li>
  <li>
    <p>Greedy Search picks the most probable token at each step, but can lead to suboptimal final outputs.</p>
  </li>
  <li>
    <p>Temperature in sampling adjusts the creativity of language generation: higher temperature = more randomness.</p>
  </li>
  <li>
    <p>Top-k and Top-p (Nucleus) sampling methods restrict the candidate tokens to the k most likely or a cumulative probability p, balancing diversity and coherence.</p>
  </li>
  <li>
    <p>Perplexity measures how well a probability model predicts a sample; lower perplexity indicates better predictive performance.</p>
  </li>
  <li>
    <p>Precision and Recall are metrics for classification tasks, focusing on correctness and completeness, respectively.</p>
  </li>
  <li>
    <p>F1 Score is the harmonic mean of precision and recall, balancing both metrics into a single value.</p>
  </li>
  <li>
    <p>Accuracy is the fraction of correct predictions, but it can be misleading in imbalanced datasets.</p>
  </li>
  <li>
    <p>Area Under the ROC Curve (AUC) measures a classifier’s performance across various thresholds.</p>
  </li>
  <li>
    <p>Confusion Matrix shows the counts of true positives, false positives, false negatives, and true negatives.</p>
  </li>
  <li>
    <p>Uncertainty Estimation methods (e.g., Monte Carlo Dropout) gauge how confident a model is in its predictions.</p>
  </li>
  <li>
    <p>Active Learning involves querying new data examples that the model is least confident about, improving data efficiency.</p>
  </li>
  <li>
    <p>Online Learning updates the model incrementally as new data arrives, rather than retraining from scratch.</p>
  </li>
  <li>
    <p>Evolutionary Algorithms and Genetic Algorithms optimize models or hyperparameters using bio-inspired mutation and selection.</p>
  </li>
  <li>
    <p>Bayesian Methods incorporate prior knowledge and update beliefs with incoming data, useful for uncertainty quantification.</p>
  </li>
  <li>
    <p>Ensemble Methods (e.g., Random Forest, Gradient Boosting) combine multiple models to improve performance and stability.</p>
  </li>
  <li>
    <p>Bagging (Bootstrap Aggregating) trains multiple models on different subsets of the data, then averages their predictions.</p>
  </li>
  <li>
    <p>Boosting iteratively trains new models to correct errors made by previously trained models.</p>
  </li>
  <li>
    <p>Gradient Boosted Decision Trees (GBDTs) are powerful for structured data, often outperforming simple neural networks.</p>
  </li>
  <li>
    <p>Autoregressive Models predict the next value (or token) based on previous outputs in a sequence.</p>
  </li>
  <li>
    <p>Autoencoder is a neural network designed to encode data into a latent representation and then decode it back, learning compressed data representations.</p>
  </li>
  <li>
    <p>Variational Autoencoder (VAE) introduces a probabilistic twist to generate new data that resembles the training set.</p>
  </li>
  <li>
    <p>Generative Adversarial Network (GAN) pits a generator against a discriminator, producing realistic images, text, or other data.</p>
  </li>
  <li>
    <p>Self-Supervised Learning leverages large amounts of unlabeled data by creating artificial training tasks (e.g., predicting missing parts).</p>
  </li>
  <li>
    <p>Foundation Models are large pre-trained models that can be adapted to a wide range of downstream tasks.</p>
  </li>
  <li>
    <p>Multimodal Learning integrates data from multiple sources (e.g., text, images, audio) to create richer representations.</p>
  </li>
  <li>
    <p>Data Labeling is often the most time-consuming part of ML, requiring careful annotation for accuracy.</p>
  </li>
  <li>
    <p>Edge Computing brings ML inference closer to the data source, reducing latency and bandwidth usage.</p>
  </li>
  <li>
    <p>Federated Learning trains models across decentralized devices or servers holding local data samples, without exchanging them.</p>
  </li>
  <li>
    <p>Privacy-Preserving ML includes techniques like differential privacy and homomorphic encryption to protect sensitive data.</p>
  </li>
  <li>
    <p>Explainable AI (XAI) aims to make the decisions of complex models more interpretable to humans.</p>
  </li>
  <li>
    <p>Bias and Fairness in ML need careful oversight, as models can inadvertently learn and amplify societal biases.</p>
  </li>
  <li>
    <p>Concept Drift occurs when the statistical properties of the target variable change over time, impacting model performance.</p>
  </li>
  <li>
    <p>AB Testing compares two or more versions of a model to see which performs better in a real-world environment.</p>
  </li>
  <li>
    <p>GPU Acceleration exploits parallel computing on graphics cards to drastically speed up ML training.</p>
  </li>
  <li>
    <p>TPUs (Tensor Processing Units) are specialized hardware accelerators by Google for efficient deep learning workloads.</p>
  </li>
  <li>
    <p>Open-Source Frameworks (e.g., TensorFlow, PyTorch) provide building blocks and tools for ML model development.</p>
  </li>
  <li>
    <p>Model Serving is the practice of deploying trained models so they can handle real-time or batch predictions.</p>
  </li>
  <li>
    <p>Scalability is crucial for handling large datasets or heavy traffic, requiring distributed training and inference strategies.</p>
  </li>
  <li>
    <p>MLOps combines ML development with operations practices, focusing on reproducibility, testing, and continuous integration.</p>
  </li>
  <li>
    <p>Version Control for data and models ensures consistent experiment tracking and collaboration.</p>
  </li>
  <li>
    <p>Deployment Strategies (e.g., containers, microservices) organize how models are packaged and served at scale.</p>
  </li>
  <li>
    <p>Monitoring tracks model performance post-deployment, watching for degradations or anomalies.</p>
  </li>
  <li>
    <p>Retraining and Model Updates keep models current as new data and changing conditions arise.</p>
  </li>
  <li>
    <p>Time Complexity (O-notation) measures how an algorithm’s runtime scales with input size; O(1) denotes constant time.</p>
  </li>
  <li>
    <p>Future of ML promises increasingly sophisticated and general models, but must address ethical, social, and environmental considerations.</p>
  </li>
</ol>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
