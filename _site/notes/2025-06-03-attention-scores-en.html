<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Stabilizing Attention Scores in Transformers</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Stabilizing Attention Scores in Transformers | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Stabilizing Attention Scores in Transformers" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-06-03-attention-scores-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-06-03-attention-scores-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Stabilizing Attention Scores in Transformers" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Stabilizing Attention Scores in Transformers","url":"https://lzwjava.github.io/notes/2025-06-03-attention-scores-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=14043099952524e763c1e5f4ef6c2245832fd05d">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=14043099952524e763c1e5f4ef6c2245832fd05d" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Stabilizing Attention Scores in Transformers | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-06-03-attention-scores-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-06-03-attention-scores-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/attention-scores-en" selected>English</option>
        <option value="/attention-scores-zh" >中文</option>
        <option value="/attention-scores-ja" >日本語</option>
        <option value="/attention-scores-es" >Español</option>
        <option value="/attention-scores-hi" >हिंदी</option>
        <option value="/attention-scores-fr" >Français</option>
        <option value="/attention-scores-de" >Deutsch</option>
        <option value="/attention-scores-ar" >العربية</option>
        <option value="/attention-scores-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Dividing by the square root of the key dimension, \(\sqrt{d_k}\), in the scaled dot-product attention mechanism (as in the formula \(\text{Scaled Scores} = \frac{Q K^T}{\sqrt{d_k}}\)) is a deliberate choice to stabilize gradients and prevent large values, particularly in the context of transformers. Here’s why \(\sqrt{d_k}\) is used instead of just \(d_k\):</p>

<ol>
  <li><strong>Variance of Dot Product</strong>:
    <ul>
      <li>The dot product \( Q K^T \) computes the similarity between query (\( Q \)) and key (\( K \)) vectors, where each vector has dimension \( d_k \). If the elements of \( Q \) and \( K \) are assumed to be independent and have a mean of 0 and variance of 1 (common after initialization or normalization), the dot product \( Q_i \cdot K_j \) (for a single pair of query and key vectors) has a variance of \( d_k \). This is because the variance of the sum of \( d_k \) independent products of two standard normal variables scales linearly with \( d_k \).</li>
      <li>Without scaling, the magnitude of \( Q K^T \) grows with \( d_k \), leading to very large values for large \( d_k \) (common in transformers, where \( d_k \) might be 64, 128, or larger). Large values in the attention scores can cause issues when passed through the softmax function.</li>
    </ul>
  </li>
  <li><strong>Softmax Stability</strong>:
    <ul>
      <li>The attention scores \( \frac{Q K^T}{\sqrt{d_k}} \) are fed into a softmax to compute attention weights. If the scores are too large (as they would be without scaling or with insufficient scaling), the softmax function can produce very sharp distributions, where one element dominates (approaching 1) and others are near 0. This leads to vanishing gradients for most elements, making it hard for the model to learn effectively.</li>
      <li>Dividing by \(\sqrt{d_k}\) ensures that the variance of the scaled scores is approximately 1, keeping the scores in a range where the softmax function behaves well, producing more balanced attention weights and stable gradients.</li>
    </ul>
  </li>
  <li><strong>Why Not \( d_k \)?</strong>:
    <ul>
      <li>Dividing by \( d_k \) instead of \(\sqrt{d_k}\) would over-scale the dot product, reducing the variance of the scores to \( \frac{1}{d_k} \). For large \( d_k \), this would make the scores very small, causing the softmax to produce nearly uniform distributions (since small inputs to softmax result in outputs close to \( \frac{1}{n} \)). This would dilute the attention mechanism’s ability to focus on relevant keys, as the differences between scores would be diminished.</li>
      <li>Over-scaling with \( d_k \) could also lead to numerical instability in some cases, as very small values might be harder to handle precisely in floating-point arithmetic.</li>
    </ul>
  </li>
  <li><strong>Why \(\sqrt{d_k}\)?</strong>:
    <ul>
      <li>Dividing by \(\sqrt{d_k}\) normalizes the variance of the dot product to approximately 1, as \( \text{Var}\left(\frac{Q K^T}{\sqrt{d_k}}\right) = \frac{\text{Var}(Q K^T)}{d_k} = \frac{d_k}{d_k} = 1 \). This keeps the scores in a reasonable range, ensuring that the softmax produces meaningful attention weights without being overly sharp or overly uniform.</li>
      <li>The square root arises naturally from the mathematical property of the dot product’s variance. It strikes a balance between preventing exploding values (which would happen without scaling) and avoiding over-suppression of the scores (which would happen with \( d_k \)).</li>
    </ul>
  </li>
  <li><strong>Empirical Effectiveness</strong>:
    <ul>
      <li>The choice of \(\sqrt{d_k}\) was introduced in the original Transformer paper (“Attention is All You Need” by Vaswani et al., 2017) and has been empirically validated across many transformer-based models. It works well in practice for a wide range of \( d_k \) values, ensuring stable training and good performance.</li>
    </ul>
  </li>
</ol>

<p>In summary, dividing by \(\sqrt{d_k}\) is a mathematically motivated choice that stabilizes the variance of the attention scores, ensures the softmax operates in a well-behaved regime, and avoids the issues of either exploding or vanishing gradients. Dividing by \( d_k \) would overly reduce the scores, leading to less discriminative attention weights and potentially harming model performance.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
