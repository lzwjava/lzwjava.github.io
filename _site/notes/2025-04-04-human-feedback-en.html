<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Human Feedback Reinforcement Learning (RLHF) Explained</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Human Feedback Reinforcement Learning (RLHF) Explained | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Human Feedback Reinforcement Learning (RLHF) Explained" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-04-04-human-feedback-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-04-04-human-feedback-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Human Feedback Reinforcement Learning (RLHF) Explained" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Human Feedback Reinforcement Learning (RLHF) Explained","url":"https://lzwjava.github.io/notes/2025-04-04-human-feedback-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=f549981044f1ce96aa77b3f87c491433b5c4e22d">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=f549981044f1ce96aa77b3f87c491433b5c4e22d" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Human Feedback Reinforcement Learning (RLHF) Explained | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-04-04-human-feedback-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-04-04-human-feedback-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.09</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/human-feedback-en" selected>English</option>
        <option value="/human-feedback-zh" >中文</option>
        <option value="/human-feedback-ja" >日本語</option>
        <option value="/human-feedback-es" >Español</option>
        <option value="/human-feedback-hi" >हिंदी</option>
        <option value="/human-feedback-fr" >Français</option>
        <option value="/human-feedback-de" >Deutsch</option>
        <option value="/human-feedback-ar" >العربية</option>
        <option value="/human-feedback-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is a machine learning technique used to fine-tune AI models, particularly large language models (LLMs), to better align with human preferences and instructions. Instead of relying solely on predefined reward functions, RLHF incorporates direct feedback from humans to guide the learning process.</p>

<p><strong>Why is RLHF Important?</strong></p>

<ul>
  <li><strong>Subjective Tasks:</strong> RLHF excels in tasks where the desired outcome is difficult to define with explicit rules or numerical rewards, such as generating creative text, engaging in natural conversations, or producing helpful and harmless content.</li>
  <li><strong>Nuance and Alignment:</strong> It helps AI models understand and adhere to subtle human preferences, ethical considerations, and desired interaction styles.</li>
  <li><strong>Improved Performance:</strong> Models trained with RLHF often demonstrate significantly improved performance and user satisfaction compared to those trained solely with traditional reinforcement learning or supervised learning.</li>
</ul>

<p><strong>How RLHF Works (Typically in three stages):</strong></p>

<ol>
  <li><strong>Pre-training and Supervised Fine-tuning (SFT):</strong>
    <ul>
      <li>A base language model is first pre-trained on a massive dataset of text and code to learn general language understanding and generation.</li>
      <li>This pre-trained model is then often fine-tuned using supervised learning on a smaller dataset of high-quality demonstrations of the desired behavior (e.g., humans writing ideal responses to prompts). This step helps the model understand the format and style of the expected outputs.</li>
    </ul>
  </li>
  <li><strong>Reward Model Training:</strong>
    <ul>
      <li>This is a crucial step in RLHF. A separate <strong>reward model</strong> is trained to predict human preferences.</li>
      <li>Human annotators are presented with different outputs from the SFT model (or a later version) for the same input prompt. They rank or rate these outputs based on various criteria (e.g., helpfulness, coherence, safety).</li>
      <li>This preference data (e.g., “output A is better than output B”) is used to train the reward model. The reward model learns to assign a scalar reward score to any given model output, reflecting how much a human would prefer it.</li>
    </ul>
  </li>
  <li><strong>Reinforcement Learning Fine-tuning:</strong>
    <ul>
      <li>The original language model (initialized from the SFT model) is further fine-tuned using reinforcement learning.</li>
      <li>The reward model trained in the previous step serves as the environment’s reward function.</li>
      <li>The RL agent (the language model) generates responses to prompts, and the reward model scores these responses.</li>
      <li>The RL algorithm (often Proximal Policy Optimization - PPO) updates the language model’s policy (how it generates text) to maximize the rewards predicted by the reward model. This encourages the language model to generate outputs that are more aligned with human preferences.</li>
      <li>To prevent the RL fine-tuning from deviating too far from the SFT model’s behavior (which might lead to undesirable outcomes), a regularization term (e.g., KL divergence penalty) is often included in the RL objective.</li>
    </ul>
  </li>
</ol>

<p><strong>How to Do RLHF (Simplified Steps):</strong></p>

<ol>
  <li><strong>Collect Human Preference Data:</strong>
    <ul>
      <li>Design prompts or tasks relevant to your desired AI behavior.</li>
      <li>Generate multiple responses to these prompts using your current model.</li>
      <li>Recruit human annotators to compare these responses and indicate their preferences (e.g., rank them, choose the best, or rate them).</li>
      <li>Store this data as pairs of (prompt, preferred response, less preferred response) or similar formats.</li>
    </ul>
  </li>
  <li><strong>Train a Reward Model:</strong>
    <ul>
      <li>Choose a suitable model architecture for your reward model (often a transformer-based model similar to the language model).</li>
      <li>Train the reward model on the collected human preference data. The goal is for the reward model to assign higher scores to the responses that humans preferred. A common loss function used is based on maximizing the margin between the scores of preferred and less preferred responses.</li>
    </ul>
  </li>
  <li><strong>Fine-tune the Language Model with Reinforcement Learning:</strong>
    <ul>
      <li>Initialize your language model with the weights from the SFT step (if you performed one).</li>
      <li>Use a reinforcement learning algorithm (like PPO).</li>
      <li>For each training step:
        <ul>
          <li>Sample a prompt.</li>
          <li>Have the language model generate a response.</li>
          <li>Use the trained reward model to get a reward score for the generated response.</li>
          <li>Update the language model’s parameters based on the reward signal to encourage actions (token generation) that lead to higher rewards.</li>
          <li>Include a regularization term (e.g., KL divergence) to keep the updated policy close to the SFT policy.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Code Example (Conceptual and Simplified using PyTorch):</strong></p>

<p>This is a highly simplified conceptual example to illustrate the core ideas. A full RLHF implementation is significantly more complex and involves libraries like Hugging Face Transformers, Accelerate, and RL libraries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Assume you have collected human preference data:
# List of tuples: (prompt, preferred_response, less_preferred_response)
</span><span class="n">preference_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"Write a short story about a cat."</span><span class="p">,</span> <span class="s">"Whiskers the cat lived in a cozy cottage..."</span><span class="p">,</span> <span class="s">"A cat story. The end."</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Summarize this article:"</span><span class="p">,</span> <span class="s">"The article discusses..."</span><span class="p">,</span> <span class="s">"Article summary."</span><span class="p">),</span>
    <span class="c1"># ... more data
</span><span class="p">]</span>

<span class="c1"># 1. Load pre-trained language model and tokenizer
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"gpt2"</span>  <span class="c1"># Or another suitable pre-trained model
</span><span class="n">policy_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span>
<span class="n">policy_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 2. Define a simple Reward Model
</span><span class="k">class</span> <span class="nc">RewardModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">.</span><span class="n">transformer</span>  <span class="c1"># Use the transformer layers
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">v_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">base_model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">base_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v_head</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Get reward from the last token
</span>        <span class="k">return</span> <span class="n">reward</span>

<span class="n">reward_model</span> <span class="o">=</span> <span class="n">RewardModel</span><span class="p">(</span><span class="n">policy_model</span><span class="p">)</span>
<span class="n">reward_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">reward_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">reward_model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">reward_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="c1"># Encourage higher reward for preferred
</span>
<span class="c1"># Train the Reward Model
</span><span class="n">num_reward_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_reward_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">preferred</span><span class="p">,</span> <span class="n">less_preferred</span> <span class="ow">in</span> <span class="n">preference_data</span><span class="p">:</span>
        <span class="n">preferred_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">preferred</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">less_preferred_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">less_preferred</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">preferred_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">preferred_tokens</span><span class="p">)</span>
        <span class="n">less_preferred_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">less_preferred_tokens</span><span class="p">)</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">preferred_reward</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># We want preferred &gt; less preferred
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">reward_criterion</span><span class="p">(</span><span class="n">preferred_reward</span><span class="p">,</span> <span class="n">less_preferred_reward</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">reward_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">reward_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Reward Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># 3. Reinforcement Learning Fine-tuning (Conceptual - PPO is complex)
</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy_model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-6</span><span class="p">)</span>

<span class="n">num_rl_episodes</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rl_episodes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">preference_data</span><span class="p">]:</span> <span class="c1"># Sample prompts
</span>        <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output_sequences</span> <span class="o">=</span> <span class="n">policy_model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_tokens</span><span class="p">.</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
            <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">generated_response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">response_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">generated_response</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">response_tokens</span><span class="p">)</span>

        <span class="c1"># (Simplified) Policy Update - In reality, PPO uses more sophisticated methods
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">reward</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># Try to maximize reward
</span>        <span class="n">policy_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">policy_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"RL Episode </span><span class="si">{</span><span class="n">episode</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">, Reward: </span><span class="si">{</span><span class="n">reward</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s">, Response: </span><span class="si">{</span><span class="n">generated_response</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"RLHF Training Done!"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key Challenges in RLHF:</strong></p>

<ul>
  <li><strong>Data Collection:</strong> Obtaining high-quality and diverse human preference data can be expensive and time-consuming.</li>
  <li><strong>Subjectivity and Bias:</strong> Human preferences can be subjective and may contain biases, which can be learned by the reward model.</li>
  <li><strong>Reward Hacking:</strong> The RL agent might find ways to exploit the reward model that don’t align with true human preferences.</li>
  <li><strong>Scalability:</strong> Scaling RLHF to very large models and datasets requires significant computational resources and careful engineering.</li>
  <li><strong>Instability:</strong> Training RLHF models can be unstable and require careful hyperparameter tuning.</li>
</ul>

<p>Despite these challenges, RLHF has proven to be a powerful technique for aligning AI models with human values and is a key component in the development of state-of-the-art language models like ChatGPT and others.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
