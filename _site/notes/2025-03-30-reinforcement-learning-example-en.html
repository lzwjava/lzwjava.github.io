<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Reinforcement Learning Example</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning Example | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Reinforcement Learning Example" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-03-30-reinforcement-learning-example-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-03-30-reinforcement-learning-example-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning Example" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Reinforcement Learning Example","url":"https://lzwjava.github.io/notes/2025-03-30-reinforcement-learning-example-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=d77ab656c331efecfc5a135772ef37260bbd88e3">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=d77ab656c331efecfc5a135772ef37260bbd88e3" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Reinforcement Learning Example | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-03-30-reinforcement-learning-example-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-03-30-reinforcement-learning-example-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/reinforcement-learning-example-en" selected>English</option>
        <option value="/reinforcement-learning-example-zh" >中文</option>
        <option value="/reinforcement-learning-example-ja" >日本語</option>
        <option value="/reinforcement-learning-example-es" >Español</option>
        <option value="/reinforcement-learning-example-hi" >हिंदी</option>
        <option value="/reinforcement-learning-example-fr" >Français</option>
        <option value="/reinforcement-learning-example-de" >Deutsch</option>
        <option value="/reinforcement-learning-example-ar" >العربية</option>
        <option value="/reinforcement-learning-example-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Below, I’ll provide a few simple conceptual examples of Reinforcement Learning (RL) followed by a basic Python code example using a simple scenario. RL involves an agent learning to maximize a reward by interacting with an environment, so the examples and code will reflect that.</p>

<h3 id="simple-examples-of-reinforcement-learning">Simple Examples of Reinforcement Learning</h3>

<ol>
  <li><strong>Child Learning to Walk</strong>
    <ul>
      <li><strong>Agent</strong>: The child.</li>
      <li><strong>Environment</strong>: The floor or room.</li>
      <li><strong>Action</strong>: Take a step, crawl, or stand still.</li>
      <li><strong>Reward</strong>: Moving forward (+1), falling (-1).</li>
      <li><strong>Learning</strong>: The child learns through trial and error that taking balanced steps leads to progress.</li>
    </ul>
  </li>
  <li><strong>Training a Robot Arm to Pick Up Objects</strong>
    <ul>
      <li><strong>Agent</strong>: The robot arm.</li>
      <li><strong>Environment</strong>: A table with objects.</li>
      <li><strong>Action</strong>: Move up, down, left, right, or grasp.</li>
      <li><strong>Reward</strong>: Successfully picking up an object (+10), dropping it (-5).</li>
      <li><strong>Learning</strong>: The arm adjusts its movements to maximize successful grabs.</li>
    </ul>
  </li>
  <li><strong>Grid World Game</strong>
    <ul>
      <li><strong>Agent</strong>: A character in a grid.</li>
      <li><strong>Environment</strong>: A 3x3 grid with a goal and obstacles.</li>
      <li><strong>Action</strong>: Move up, down, left, or right.</li>
      <li><strong>Reward</strong>: Reaching the goal (+10), hitting a wall (-1).</li>
      <li><strong>Learning</strong>: The character learns the shortest path to the goal.</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="simple-python-code-example-q-learning-in-a-grid-world">Simple Python Code Example: Q-Learning in a Grid World</h3>

<p>Here’s a basic implementation of Q-Learning, a popular RL algorithm, in a simple 1D “world” where an agent moves left or right to reach a goal. The agent learns by updating a Q-table based on rewards.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Environment setup: 1D world with 5 positions (0 to 4), goal at position 4
</span><span class="n">state_space</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Positions: [0, 1, 2, 3, 4]
</span><span class="n">action_space</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Actions: 0 = move left, 1 = move right
</span><span class="n">goal</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Initialize Q-table with zeros (states x actions)
</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">))</span>

<span class="c1"># Hyperparameters
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">exploration_rate</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">exploration_decay</span> <span class="o">=</span> <span class="mf">0.995</span>
<span class="n">min_exploration_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">episodes</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Reward function
</span><span class="k">def</span> <span class="nf">get_reward</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">goal</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">10</span>  <span class="c1"># Big reward for reaching the goal
</span>    <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Small penalty for each step
</span>
<span class="c1"># Step function: Move agent and get new state
</span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Move left
</span>        <span class="n">new_state</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">state</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Move right
</span>        <span class="n">new_state</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">state_space</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">state</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
    <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_state</span> <span class="o">==</span> <span class="n">goal</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Start at position 0
</span>    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># Exploration vs Exploitation
</span>        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">exploration_rate</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_space</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Explore
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>  <span class="c1"># Exploit
</span>        
        <span class="c1"># Take action and observe result
</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        
        <span class="c1"># Update Q-table using the Q-learning formula
</span>        <span class="n">old_value</span> <span class="o">=</span> <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="n">next_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span>
        <span class="n">new_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_max</span><span class="p">)</span>
        <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_value</span>
        
        <span class="c1"># Move to new state
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
    
    <span class="c1"># Decay exploration rate
</span>    <span class="n">exploration_rate</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">min_exploration_rate</span><span class="p">,</span> <span class="n">exploration_rate</span> <span class="o">*</span> <span class="n">exploration_decay</span><span class="p">)</span>

<span class="c1"># Test the learned policy
</span><span class="n">state</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Testing the learned policy:"</span><span class="p">)</span>
<span class="k">while</span> <span class="n">state</span> <span class="o">!=</span> <span class="n">goal</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
    <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Step </span><span class="si">{</span><span class="n">steps</span><span class="si">}</span><span class="s">: Moved to state </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s">, Action: </span><span class="si">{</span><span class="s">'Left'</span> <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="s">'Right'</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Reached goal in </span><span class="si">{</span><span class="n">steps</span><span class="si">}</span><span class="s"> steps!"</span><span class="p">)</span>

<span class="c1"># Print the Q-table
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Learned Q-table:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">q_table</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h3 id="explanation-of-the-code">Explanation of the Code</h3>

<ol>
  <li><strong>Environment</strong>: A 1D line with 5 positions (0 to 4). The goal is at position 4.</li>
  <li><strong>Actions</strong>: The agent can move left (0) or right (1).</li>
  <li><strong>Rewards</strong>: +10 for reaching the goal, -1 for each step (to encourage efficiency).</li>
  <li><strong>Q-Table</strong>: A table storing the expected future rewards for each state-action pair.</li>
  <li><strong>Q-Learning</strong>: The agent updates the Q-table using the formula:<br />
<code class="language-plaintext highlighter-rouge">Q(s, a) = (1 - α) * Q(s, a) + α * (reward + γ * max(Q(s', a')))</code>, where:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">α</code> = learning rate</li>
      <li><code class="language-plaintext highlighter-rouge">γ</code> = discount factor</li>
      <li><code class="language-plaintext highlighter-rouge">s</code> = current state, <code class="language-plaintext highlighter-rouge">a</code> = action, <code class="language-plaintext highlighter-rouge">s'</code> = next state</li>
    </ul>
  </li>
  <li><strong>Exploration vs Exploitation</strong>: The agent sometimes picks random actions (exploration) and sometimes uses the Q-table (exploitation), controlled by <code class="language-plaintext highlighter-rouge">exploration_rate</code>.</li>
</ol>

<hr />

<h3 id="sample-output">Sample Output</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Testing the learned policy:
Step 1: Moved to state 1, Action: Right
Step 2: Moved to state 2, Action: Right
Step 3: Moved to state 3, Action: Right
Step 4: Moved to state 4, Action: Right
Reached goal in 4 steps!

Learned Q-table:
[[-2.5   2.3]
 [-1.8   4.5]
 [-1.2   6.8]
 [-0.5   9. ]
 [ 0.    0. ]]
</code></pre></div></div>

<p>The agent learns to always move right from any starting position to reach the goal efficiently. The Q-table shows higher values for “move right” actions as you approach the goal.</p>

<p>This is a very basic example, but it captures the essence of RL. For more complex scenarios (e.g., 2D grids or games), you’d expand the state and action spaces accordingly!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
