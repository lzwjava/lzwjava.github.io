<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Linear Transformations - Gemini</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Linear Transformations - Gemini | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Linear Transformations - Gemini" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-03-29-linear-transformation-gemini-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-03-29-linear-transformation-gemini-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Linear Transformations - Gemini" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Linear Transformations - Gemini","url":"https://lzwjava.github.io/notes/2025-03-29-linear-transformation-gemini-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=8d22dca821f2b0135cd0964fd149518dac4c5463">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=8d22dca821f2b0135cd0964fd149518dac4c5463" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Linear Transformations - Gemini | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-03-29-linear-transformation-gemini-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-03-29-linear-transformation-gemini-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.09</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/linear-transformation-gemini-en" selected>English</option>
        <option value="/linear-transformation-gemini-zh" >中文</option>
        <option value="/linear-transformation-gemini-ja" >日本語</option>
        <option value="/linear-transformation-gemini-es" >Español</option>
        <option value="/linear-transformation-gemini-hi" >हिंदी</option>
        <option value="/linear-transformation-gemini-fr" >Français</option>
        <option value="/linear-transformation-gemini-de" >Deutsch</option>
        <option value="/linear-transformation-gemini-ar" >العربية</option>
        <option value="/linear-transformation-gemini-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Okay, let’s dive into a detailed tutorial on linear transformations, covering the definition, examples, matrix representation, kernel, image, and properties like injectivity and surjectivity. This is a fundamental topic in linear algebra.</p>

<h2 id="6-linear-transformations-a-detailed-tutorial">6. Linear Transformations: A Detailed Tutorial</h2>

<p>A <strong>linear transformation</strong> is a function between two vector spaces that preserves the operations of vector addition and scalar multiplication. Let \(V\) and \(W\) be vector spaces over the same field \(F\) (which is often the set of real numbers \(\mathbb{R}\) or complex numbers \(\mathbb{C}\)).</p>

<p><strong>Definition:</strong> A function \(T: V \rightarrow W\) is a linear transformation if it satisfies the following two properties for all vectors \(\mathbf{u}, \mathbf{v} \in V\) and all scalars \(c \in F\):</p>

<ol>
  <li><strong>Additivity:</strong> \(T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})\)</li>
  <li><strong>Homogeneity (Scalar Multiplication):</strong> \(T(c\mathbf{u}) = cT(\mathbf{u})\)</li>
</ol>

<p>These two properties can be combined into a single condition:
\(T(c\mathbf{u} + d\mathbf{v}) = cT(\mathbf{u}) + dT(\mathbf{v})\) for all \(\mathbf{u}, \mathbf{v} \in V\) and all scalars \(c, d \in F\).</p>

<p><strong>Key Consequences of Linearity:</strong></p>

<ul>
  <li>\(T(\mathbf{0}_V) = \mathbf{0}_W\), where \(\mathbf{0}_V\) is the zero vector in \(V\) and \(\mathbf{0}_W\) is the zero vector in \(W\). (Proof: \(T(\mathbf{0}_V) = T(0\mathbf{u}) = 0T(\mathbf{u}) = \mathbf{0}_W\) for any \(\mathbf{u} \in V\)).</li>
  <li>\(T(-\mathbf{u}) = -T(\mathbf{u})\). (Proof: \(T(-\mathbf{u}) = T((-1)\mathbf{u}) = (-1)T(\mathbf{u}) = -T(\mathbf{u})\)).</li>
</ul>

<h3 id="examples-of-linear-transformations">Examples of Linear Transformations</h3>

<p>Let’s look at some examples to understand the concept better.</p>

<p><strong>Example 1: Transformation in \(\mathbb{R}^2\) (Rotation)</strong></p>

<p>Consider a transformation \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) that rotates every vector in \(\mathbb{R}^2\) counterclockwise by an angle \(\theta\). If \(\mathbf{v} = \begin{pmatrix} x \ y \end{pmatrix}\), then \(T(\mathbf{v}) = \begin{pmatrix} x\cos\theta - y\sin\theta \ x\sin\theta + y\cos\theta \end{pmatrix}\).</p>

<p>Let’s check if this is a linear transformation. Let \(\mathbf{u} = \begin{pmatrix} x_1 \ y_1 \end{pmatrix}\) and \(\mathbf{v} = \begin{pmatrix} x_2 \ y_2 \end{pmatrix}\), and let \(c\) be a scalar.</p>

<ul>
  <li>
    <p><strong>Additivity:</strong>
  \(T(\mathbf{u} + \mathbf{v}) = T\left(\begin{pmatrix} x_1 + x_2 \ y_1 + y_2 \end{pmatrix}\right) = \begin{pmatrix} (x_1 + x_2)\cos\theta - (y_1 + y_2)\sin\theta \ (x_1 + x_2)\sin\theta + (y_1 + y_2)\cos\theta \end{pmatrix}\)
  \(= \begin{pmatrix} (x_1\cos\theta - y_1\sin\theta) + (x_2\cos\theta - y_2\sin\theta) \ (x_1\sin\theta + y_1\cos\theta) + (x_2\sin\theta + y_2\cos\theta) \end{pmatrix} = T(\mathbf{u}) + T(\mathbf{v})\)</p>
  </li>
  <li>
    <p><strong>Homogeneity:</strong>
  \(T(c\mathbf{u}) = T\left(\begin{pmatrix} cx_1 \ cy_1 \end{pmatrix}\right) = \begin{pmatrix} (cx_1)\cos\theta - (cy_1)\sin\theta \ (cx_1)\sin\theta + (cy_1)\cos\theta \end{pmatrix}\)
  \(= \begin{pmatrix} c(x_1\cos\theta - y_1\sin\theta) \ c(x_1\sin\theta + y_1\cos\theta) \end{pmatrix} = c \begin{pmatrix} x_1\cos\theta - y_1\sin\theta \ x_1\sin\theta + y_1\cos\theta \end{pmatrix} = cT(\mathbf{u})\)</p>
  </li>
</ul>

<p>Thus, rotation is a linear transformation.</p>

<p><strong>Example 2: Transformation in \(\mathbb{R}^2\) (Projection onto the x-axis)</strong></p>

<p>Consider \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x \ 0 \end{pmatrix}\). This transformation projects every vector onto the x-axis. You can verify that this is also a linear transformation using the definition.</p>

<p><strong>Example 3: Transformation in \(\mathbb{R}^2\) (Translation)</strong></p>

<p>Consider \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x + a \ y + b \end{pmatrix}\), where \(a\) and \(b\) are constants (not both zero).</p>

<p>Let’s check the first property:
\(T(\mathbf{u} + \mathbf{v}) = T\left(\begin{pmatrix} x_1 + x_2 \ y_1 + y_2 \end{pmatrix}\right) = \begin{pmatrix} (x_1 + x_2) + a \ (y_1 + y_2) + b \end{pmatrix} = \begin{pmatrix} x_1 + a + x_2 + a \ y_1 + b + y_2 + b \end{pmatrix}\)
\(T(\mathbf{u}) + T(\mathbf{v}) = \begin{pmatrix} x_1 + a \ y_1 + b \end{pmatrix} + \begin{pmatrix} x_2 + a \ y_2 + b \end{pmatrix} = \begin{pmatrix} x_1 + a + x_2 + a \ y_1 + b + y_2 + b \end{pmatrix}\)</p>

<p>This seems correct, let’s recheck.
\(T(\mathbf{u} + \mathbf{v}) = \begin{pmatrix} x_1 + x_2 + a \ y_1 + y_2 + b \end{pmatrix}\)
\(T(\mathbf{u}) + T(\mathbf{v}) = \begin{pmatrix} x_1 + a \ y_1 + b \end{pmatrix} + \begin{pmatrix} x_2 + a \ y_2 + b \end{pmatrix} = \begin{pmatrix} x_1 + x_2 + 2a \ y_1 + y_2 + 2b \end{pmatrix}\)</p>

<p>If \(a \neq 0\) or \(b \neq 0\), then \(T(\mathbf{u} + \mathbf{v}) \neq T(\mathbf{u}) + T(\mathbf{v})\). Also, \(T(\mathbf{0}) = T\left(\begin{pmatrix} 0 \ 0 \end{pmatrix}\right) = \begin{pmatrix} a \ b \end{pmatrix} \neq \begin{pmatrix} 0 \ 0 \end{pmatrix}\) if \(a\) or \(b\) is non-zero. Therefore, translation is generally <strong>not</strong> a linear transformation.</p>

<p><strong>Example 4: Transformation from \(\mathbb{R}^n\) to \(\mathbb{R}^m\) defined by a matrix</strong></p>

<p>Let \(A\) be an \(m \times n\) matrix. The transformation \(T: \mathbb{R}^n \rightarrow \mathbb{R}^m\) defined by \(T(\mathbf{x}) = A\mathbf{x}\) (where \(\mathbf{x}\) is an \(n \times 1\) column vector) is a linear transformation. This is because matrix multiplication satisfies the properties of additivity and homogeneity:
\(A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}\)
\(A(c\mathbf{u}) = c(A\mathbf{u})\)</p>

<p><strong>Example 5: Differentiation of Polynomials</strong></p>

<p>Let \(P_n\) be the vector space of polynomials of degree at most \(n\). The transformation \(D: P_n \rightarrow P_{n-1}\) defined by \(D(p(x)) = p’(x)\) (the derivative of \(p(x)\)) is a linear transformation.
If \(p(x)\) and \(q(x)\) are polynomials and \(c\) is a scalar:
\(D(p(x) + q(x)) = (p(x) + q(x))’ = p’(x) + q’(x) = D(p(x)) + D(q(x))\)
\(D(cp(x)) = (cp(x))’ = cp’(x) = cD(p(x))\)</p>

<p><strong>Example 6: Integration of Functions</strong></p>

<p>Let \(C[a, b]\) be the vector space of continuous functions on the interval \([a, b]\). The transformation \(I: C[a, b] \rightarrow \mathbb{R}\) defined by \(I(f) = \int_a^b f(x) dx\) is a linear transformation.
\(I(f + g) = \int_a^b (f(x) + g(x)) dx = \int_a^b f(x) dx + \int_a^b g(x) dx = I(f) + I(g)\)
\(I(cf) = \int_a^b cf(x) dx = c \int_a^b f(x) dx = cI(f)\)</p>

<h3 id="matrix-representation-of-a-linear-transformation">Matrix Representation of a Linear Transformation</h3>

<p>A fundamental result in linear algebra is that any linear transformation between finite-dimensional vector spaces can be represented by a matrix.</p>

<p>Let \(V\) be an \(n\)-dimensional vector space with basis \(\mathcal{B} = {\mathbf{b}_1, \mathbf{b}_2, …, \mathbf{b}_n}\) and \(W\) be an \(m\)-dimensional vector space with basis \(\mathcal{C} = {\mathbf{c}_1, \mathbf{c}_2, …, \mathbf{c}_m}\). Let \(T: V \rightarrow W\) be a linear transformation.</p>

<p>To find the matrix representation of \(T\) with respect to the bases \(\mathcal{B}\) and \(\mathcal{C}\) (denoted as \([T]_{\mathcal{B}}^{\mathcal{C}}\) or simply \([T]\) when the bases are understood), we need to determine the images of the basis vectors of \(V\) under \(T\) and express these images as linear combinations of the basis vectors of \(W\).</p>

<p>For each \(\mathbf{b}<em>j \in \mathcal{B}\), \(T(\mathbf{b}_j)\) is a vector in \(W\), so it can be uniquely written as a linear combination of the basis vectors in \(\mathcal{C}\):
\(T(\mathbf{b}_j) = a</em>{1j}\mathbf{c}<em>1 + a</em>{2j}\mathbf{c}<em>2 + … + a</em>{mj}\mathbf{c}<em>m = \sum</em>{i=1}^{m} a_{ij}\mathbf{c}_i\)</p>

<p>The coefficients of this linear combination form the \(j\)-th column of the matrix representation \([T]<em>{\mathcal{B}}^{\mathcal{C}}\):
$[T]</em>{\mathcal{B}}^{\mathcal{C}} = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} <br />
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} <br />
\vdots &amp; \vdots &amp; \ddots &amp; \vdots <br />
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix}$</p>

<p>If \(\mathbf{v} \in V\) has a coordinate vector \([\mathbf{v}]<em>{\mathcal{B}} = \begin{pmatrix} x_1 \ x_2 \ \vdots \ x_n \end{pmatrix}\) with respect to the basis \(\mathcal{B}\), then the coordinate vector of \(T(\mathbf{v})\) with respect to the basis \(\mathcal{C}\), denoted as \([T(\mathbf{v})]</em>{\mathcal{C}}\), is given by the matrix product:
\([T(\mathbf{v})]<em>{\mathcal{C}} = [T]</em>{\mathcal{B}}^{\mathcal{C}} [\mathbf{v}]_{\mathcal{B}}\)</p>

<p><strong>Example: Matrix Representation</strong></p>

<p>Let \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^3\) be a linear transformation defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\). Let the standard bases for \(\mathbb{R}^2\) and \(\mathbb{R}^3\) be \(\mathcal{B} = {\mathbf{e}_1, \mathbf{e}_2} = \left{ \begin{pmatrix} 1 \ 0 \end{pmatrix}, \begin{pmatrix} 0 \ 1 \end{pmatrix} \right}\) and \(\mathcal{C} = {\mathbf{f}_1, \mathbf{f}_2, \mathbf{f}_3} = \left{ \begin{pmatrix} 1 \ 0 \ 0 \end{pmatrix}, \begin{pmatrix} 0 \ 1 \ 0 \end{pmatrix}, \begin{pmatrix} 0 \ 0 \ 1 \end{pmatrix} \right}\).</p>

<p>We find the images of the basis vectors of \(\mathbb{R}^2\) under \(T\):
\(T(\mathbf{e}_1) = T\left(\begin{pmatrix} 1 \ 0 \end{pmatrix}\right) = \begin{pmatrix} 1 + 0 \ 2(1) - 0 \ 3(0) \end{pmatrix} = \begin{pmatrix} 1 \ 2 \ 0 \end{pmatrix} = 1\mathbf{f}_1 + 2\mathbf{f}_2 + 0\mathbf{f}_3\)
\(T(\mathbf{e}_2) = T\left(\begin{pmatrix} 0 \ 1 \end{pmatrix}\right) = \begin{pmatrix} 0 + 1 \ 2(0) - 1 \ 3(1) \end{pmatrix} = \begin{pmatrix} 1 \ -1 \ 3 \end{pmatrix} = 1\mathbf{f}_1 - 1\mathbf{f}_2 + 3\mathbf{f}_3\)</p>

<p>The matrix representation of \(T\) with respect to the standard bases is:
\([T]_{\mathcal{B}}^{\mathcal{C}} = \begin{pmatrix} 1 &amp; 1 \ 2 &amp; -1 \ 0 &amp; 3 \end{pmatrix}\)</p>

<p>Now, let’s take an arbitrary vector \(\mathbf{v} = \begin{pmatrix} x \ y \end{pmatrix}\) in \(\mathbb{R}^2\). Its coordinate vector with respect to \(\mathcal{B}\) is \([\mathbf{v}]<em>{\mathcal{B}} = \begin{pmatrix} x \ y \end{pmatrix}\).
\([T(\mathbf{v})]</em>{\mathcal{C}} = [T]<em>{\mathcal{B}}^{\mathcal{C}} [\mathbf{v}]</em>{\mathcal{B}} = \begin{pmatrix} 1 &amp; 1 \ 2 &amp; -1 \ 0 &amp; 3 \end{pmatrix} \begin{pmatrix} x \ y \end{pmatrix} = \begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\)
The coordinate vector with respect to \(\mathcal{C}\) is indeed \(\begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\), which corresponds to the vector \(T(\mathbf{v})\) we defined earlier.</p>

<h3 id="kernel-null-space-of-a-linear-transformation">Kernel (Null Space) of a Linear Transformation</h3>

<p>The <strong>kernel</strong> (or null space) of a linear transformation \(T: V \rightarrow W\), denoted by \(\text{ker}(T)\) or \(N(T)\), is the set of all vectors in \(V\) that are mapped to the zero vector in \(W\):
\(\text{ker}(T) = {\mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0}_W}\)</p>

<p><strong>Properties of the Kernel:</strong></p>

<ul>
  <li>The kernel of a linear transformation is always a subspace of the domain \(V\).
    <ul>
      <li><strong>Contains the zero vector:</strong> \(T(\mathbf{0}_V) = \mathbf{0}_W\), so \(\mathbf{0}_V \in \text{ker}(T)\).</li>
      <li><strong>Closed under addition:</strong> If \(\mathbf{u}, \mathbf{v} \in \text{ker}(T)\), then \(T(\mathbf{u}) = \mathbf{0}_W\) and \(T(\mathbf{v}) = \mathbf{0}_W\). Thus, \(T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}) = \mathbf{0}_W + \mathbf{0}_W = \mathbf{0}_W\), so \(\mathbf{u} + \mathbf{v} \in \text{ker}(T)\).</li>
      <li><strong>Closed under scalar multiplication:</strong> If \(\mathbf{u} \in \text{ker}(T)\) and \(c\) is a scalar, then \(T(c\mathbf{u}) = cT(\mathbf{u}) = c\mathbf{0}_W = \mathbf{0}_W\), so \(c\mathbf{u} \in \text{ker}(T)\).</li>
    </ul>
  </li>
</ul>

<p><strong>Example: Finding the Kernel</strong></p>

<p>Consider the linear transformation \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^3\) defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\).
To find the kernel, we need to solve \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} 0 \ 0 \ 0 \end{pmatrix}\):
\(\begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix} = \begin{pmatrix} 0 \ 0 \ 0 \end{pmatrix}\)</p>

<p>This gives the system of linear equations:
\(x + y = 0\)
\(2x - y = 0\)
\(3y = 0\)</p>

<p>From the third equation, \(y = 0\). Substituting this into the first equation, \(x + 0 = 0\), so \(x = 0\). The second equation is also satisfied: \(2(0) - 0 = 0\).
The only solution is \(x = 0\) and \(y = 0\). Therefore, \(\text{ker}(T) = \left{ \begin{pmatrix} 0 \ 0 \end{pmatrix} \right}\), which is the zero subspace of \(\mathbb{R}^2\).</p>

<h3 id="image-range-of-a-linear-transformation">Image (Range) of a Linear Transformation</h3>

<p>The <strong>image</strong> (or range) of a linear transformation \(T: V \rightarrow W\), denoted by \(\text{im}(T)\) or \(R(T)\), is the set of all vectors in \(W\) that are the image of some vector in \(V\):
\(\text{im}(T) = {\mathbf{w} \in W \mid \mathbf{w} = T(\mathbf{v}) \text{ for some } \mathbf{v} \in V}\)</p>

<p><strong>Properties of the Image:</strong></p>

<ul>
  <li>The image of a linear transformation is always a subspace of the codomain \(W\).
    <ul>
      <li><strong>Contains the zero vector:</strong> \(T(\mathbf{0}_V) = \mathbf{0}_W\), so \(\mathbf{0}_W \in \text{im}(T)\).</li>
      <li><strong>Closed under addition:</strong> If \(\mathbf{w}_1, \mathbf{w}_2 \in \text{im}(T)\), then there exist \(\mathbf{v}_1, \mathbf{v}_2 \in V\) such that \(T(\mathbf{v}_1) = \mathbf{w}_1\) and \(T(\mathbf{v}_2) = \mathbf{w}_2\). Then \(\mathbf{w}_1 + \mathbf{w}_2 = T(\mathbf{v}_1) + T(\mathbf{v}_2) = T(\mathbf{v}_1 + \mathbf{v}_2)\). Since \(\mathbf{v}_1 + \mathbf{v}_2 \in V\), \(\mathbf{w}_1 + \mathbf{w}_2 \in \text{im}(T)\).</li>
      <li><strong>Closed under scalar multiplication:</strong> If \(\mathbf{w} \in \text{im}(T)\) and \(c\) is a scalar, then there exists \(\mathbf{v} \in V\) such that \(T(\mathbf{v}) = \mathbf{w}\). Then \(c\mathbf{w} = cT(\mathbf{v}) = T(c\mathbf{v})\). Since \(c\mathbf{v} \in V\), \(c\mathbf{w} \in \text{im}(T)\).</li>
    </ul>
  </li>
  <li>If \(V\) is finite-dimensional with a basis \({\mathbf{b}_1, \mathbf{b}_2, …, \mathbf{b}_n}\), then the image of \(T\) is the span of the images of the basis vectors:
  \(\text{im}(T) = \text{span}{T(\mathbf{b}_1), T(\mathbf{b}_2), …, T(\mathbf{b}_n)}\)</li>
</ul>

<p><strong>Example: Finding the Image</strong></p>

<p>Consider the linear transformation \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^3\) defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\).
Using the standard basis of \(\mathbb{R}^2\), \({\begin{pmatrix} 1 \ 0 \end{pmatrix}, \begin{pmatrix} 0 \ 1 \end{pmatrix}}\), we have:
\(T\left(\begin{pmatrix} 1 \ 0 \end{pmatrix}\right) = \begin{pmatrix} 1 \ 2 \ 0 \end{pmatrix}\)
\(T\left(\begin{pmatrix} 0 \ 1 \end{pmatrix}\right) = \begin{pmatrix} 1 \ -1 \ 3 \end{pmatrix}\)</p>

<p>The image of \(T\) is the span of these two vectors:
\(\text{im}(T) = \text{span}\left{ \begin{pmatrix} 1 \ 2 \ 0 \end{pmatrix}, \begin{pmatrix} 1 \ -1 \ 3 \end{pmatrix} \right}\)
This is a subspace of \(\mathbb{R}^3\). Since these two vectors are linearly independent (one is not a scalar multiple of the other), the image is a plane passing through the origin in \(\mathbb{R}^3\).</p>

<p><strong>Relationship between Matrix Representation and Image:</strong></p>

<p>If \(T: \mathbb{R}^n \rightarrow \mathbb{R}^m\) is given by \(T(\mathbf{x}) = A\mathbf{x}\), where \(A\) is an \(m \times n\) matrix, then the image of \(T\) is the column space of the matrix \(A\), i.e., the span of the columns of \(A\).</p>

<h3 id="properties-of-linear-transformations-injectivity-and-surjectivity">Properties of Linear Transformations: Injectivity and Surjectivity</h3>

<p><strong>Injectivity (One-to-one)</strong></p>

<p>A linear transformation \(T: V \rightarrow W\) is <strong>injective</strong> (or one-to-one) if for every \(\mathbf{w} \in W\), there is at most one \(\mathbf{v} \in V\) such that \(T(\mathbf{v}) = \mathbf{w}\). Equivalently, if \(T(\mathbf{u}) = T(\mathbf{v})\), then \(\mathbf{u} = \mathbf{v}\).</p>

<p><strong>Theorem:</strong> A linear transformation \(T: V \rightarrow W\) is injective if and only if its kernel is the zero subspace, i.e., \(\text{ker}(T) = {\mathbf{0}_V}\).</p>

<p><strong>Proof:</strong></p>
<ul>
  <li><strong>(\(\Rightarrow\)) Assume \(T\) is injective.</strong> If \(\mathbf{v} \in \text{ker}(T)\), then \(T(\mathbf{v}) = \mathbf{0}_W\). We also know that \(T(\mathbf{0}_V) = \mathbf{0}_W\). Since \(T\) is injective and \(T(\mathbf{v}) = T(\mathbf{0}_V)\), it must be that \(\mathbf{v} = \mathbf{0}_V\). Thus, \(\text{ker}(T) = {\mathbf{0}_V}\).</li>
  <li><strong>(\(\Leftarrow\)) Assume \(\text{ker}(T) = {\mathbf{0}_V}\).</strong> Suppose \(T(\mathbf{u}) = T(\mathbf{v})\) for some \(\mathbf{u}, \mathbf{v} \in V\). Then \(T(\mathbf{u}) - T(\mathbf{v}) = \mathbf{0}_W\). By linearity, \(T(\mathbf{u} - \mathbf{v}) = \mathbf{0}_W\). This means that \(\mathbf{u} - \mathbf{v} \in \text{ker}(T)\). Since \(\text{ker}(T) = {\mathbf{0}_V}\), we have \(\mathbf{u} - \mathbf{v} = \mathbf{0}_V\), which implies \(\mathbf{u} = \mathbf{v}\). Therefore, \(T\) is injective.</li>
</ul>

<p><strong>Example: Checking for Injectivity</strong></p>

<p>For the transformation \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^3\) defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\), we found that \(\text{ker}(T) = \left{ \begin{pmatrix} 0 \ 0 \end{pmatrix} \right}\). Therefore, this transformation is injective.</p>

<p><strong>Surjectivity (Onto)</strong></p>

<p>A linear transformation \(T: V \rightarrow W\) is <strong>surjective</strong> (or onto) if for every \(\mathbf{w} \in W\), there exists at least one \(\mathbf{v} \in V\) such that \(T(\mathbf{v}) = \mathbf{w}\). In other words, the image of \(T\) is equal to the codomain \(W\), i.e., \(\text{im}(T) = W\).</p>

<p><strong>Theorem (Rank-Nullity Theorem):</strong> For a linear transformation \(T: V \rightarrow W\), where \(V\) is a finite-dimensional vector space,
\(\text{dim}(\text{ker}(T)) + \text{dim}(\text{im}(T)) = \text{dim}(V)\)
Here, \(\text{dim}(\text{ker}(T))\) is called the <strong>nullity</strong> of \(T\), and \(\text{dim}(\text{im}(T))\) is called the <strong>rank</strong> of \(T\).</p>

<p><strong>Relationship between Surjectivity and Dimensions:</strong></p>

<p>If \(T: V \rightarrow W\) is a linear transformation between finite-dimensional vector spaces, then:</p>
<ul>
  <li>If \(\text{dim}(V) &lt; \text{dim}(W)\), \(T\) cannot be surjective. (By Rank-Nullity Theorem, \(\text{dim}(\text{im}(T)) \leq \text{dim}(V) &lt; \text{dim}(W)\)).</li>
  <li>If \(\text{dim}(V) &gt; \text{dim}(W)\), \(T\) cannot be injective (because \(\text{dim}(\text{ker}(T)) = \text{dim}(V) - \text{dim}(\text{im}(T)) \geq \text{dim}(V) - \text{dim}(W) &gt; 0\), so the kernel is not just the zero vector).</li>
  <li>If \(\text{dim}(V) = \text{dim}(W)\), then \(T\) is injective if and only if it is surjective. (If \(T\) is injective, \(\text{dim}(\text{ker}(T)) = 0\), so \(\text{dim}(\text{im}(T)) = \text{dim}(V) = \text{dim}(W)\), meaning \(\text{im}(T) = W\), so \(T\) is surjective. Conversely, if \(T\) is surjective, \(\text{dim}(\text{im}(T)) = \text{dim}(W) = \text{dim}(V)\), so \(\text{dim}(\text{ker}(T)) = 0\), meaning \(T\) is injective).</li>
</ul>

<p><strong>Example: Checking for Surjectivity</strong></p>

<p>For the transformation \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^3\) defined by \(T\left(\begin{pmatrix} x \ y \end{pmatrix}\right) = \begin{pmatrix} x + y \ 2x - y \ 3y \end{pmatrix}\), we found that \(\text{im}(T) = \text{span}\left{ \begin{pmatrix} 1 \ 2 \ 0 \end{pmatrix}, \begin{pmatrix} 1 \ -1 \ 3 \end{pmatrix} \right}\). The dimension of the image (rank of \(T\)) is 2, as the two spanning vectors are linearly independent. The dimension of the domain is \(\text{dim}(\mathbb{R}^2) = 2\). By the Rank-Nullity Theorem, \(\text{dim}(\text{ker}(T)) + \text{dim}(\text{im}(T)) = 2\), so \(\text{dim}(\text{ker}(T)) + 2 = 2\), which gives \(\text{dim}(\text{ker}(T)) = 0\), consistent with our earlier finding.</p>

<p>Since the dimension of the image (2) is less than the dimension of the codomain (3), the image is a proper subspace of the codomain, and thus the transformation is not surjective. There are vectors in \(\mathbb{R}^3\) that are not in the image of \(T\). For example, \(\begin{pmatrix} 0 \ 0 \ 1 \end{pmatrix}\) cannot be expressed as a linear combination of \(\begin{pmatrix} 1 \ 2 \ 0 \end{pmatrix}\) and \(\begin{pmatrix} 1 \ -1 \ 3 \end{pmatrix}\).</p>

<p><strong>Isomorphism</strong></p>

<p>A linear transformation \(T: V \rightarrow W\) is called an <strong>isomorphism</strong> if it is both injective and surjective. If there exists an isomorphism between two vector spaces \(V\) and \(W\), then \(V\) and \(W\) are said to be <strong>isomorphic</strong>, denoted by \(V \cong W\). Isomorphic vector spaces have essentially the same algebraic structure.</p>

<p>For finite-dimensional vector spaces, two vector spaces are isomorphic if and only if they have the same dimension. If \(\text{dim}(V) = \text{dim}(W) = n\), then a linear transformation \(T: V \rightarrow W\) is an isomorphism if and only if it is either injective or surjective.</p>

<p><strong>Summary of Key Concepts:</strong></p>

<ul>
  <li><strong>Linear Transformation:</strong> A function between vector spaces that preserves vector addition and scalar multiplication.</li>
  <li><strong>Matrix Representation:</strong> A way to represent a linear transformation between finite-dimensional vector spaces as a matrix.</li>
  <li><strong>Kernel (Null Space):</strong> The set of vectors in the domain that are mapped to the zero vector in the codomain. It is a subspace of the domain.</li>
  <li><strong>Image (Range):</strong> The set of vectors in the codomain that are the image of some vector in the domain. It is a subspace of the codomain.</li>
  <li><strong>Injectivity (One-to-one):</strong> A linear transformation is injective if and only if its kernel is the zero subspace.</li>
  <li><strong>Surjectivity (Onto):</strong> A linear transformation is surjective if and only if its image is equal to the codomain.</li>
  <li><strong>Rank-Nullity Theorem:</strong> For a linear transformation \(T: V \rightarrow W\) with finite-dimensional \(V\), \(\text{dim}(\text{ker}(T)) + \text{dim}(\text{im}(T)) = \text{dim}(V)\).</li>
  <li><strong>Isomorphism:</strong> A linear transformation that is both injective and surjective.</li>
</ul>

<p>This detailed tutorial should provide a solid foundation for understanding linear transformations. Remember to practice with various examples to solidify your understanding of these concepts. Good luck with your studies!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
