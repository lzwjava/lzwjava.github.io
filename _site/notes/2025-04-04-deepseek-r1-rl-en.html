<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>DeepSeek R1</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>DeepSeek R1 | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="DeepSeek R1" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-04-04-deepseek-r1-rl-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-04-04-deepseek-r1-rl-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="DeepSeek R1" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"DeepSeek R1","url":"https://lzwjava.github.io/notes/2025-04-04-deepseek-r1-rl-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=c96c645be2c8c2eeccd0cbfd2e1883e382a66dae">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=c96c645be2c8c2eeccd0cbfd2e1883e382a66dae" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       DeepSeek R1 | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-04-04-deepseek-r1-rl-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-04-04-deepseek-r1-rl-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/deepseek-r1-rl-en" selected>English</option>
        <option value="/deepseek-r1-rl-zh" >中文</option>
        <option value="/deepseek-r1-rl-ja" >日本語</option>
        <option value="/deepseek-r1-rl-es" >Español</option>
        <option value="/deepseek-r1-rl-hi" >हिंदी</option>
        <option value="/deepseek-r1-rl-fr" >Français</option>
        <option value="/deepseek-r1-rl-de" >Deutsch</option>
        <option value="/deepseek-r1-rl-ar" >العربية</option>
        <option value="/deepseek-r1-rl-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>DeepSeek R1 employs a novel approach to reinforcement learning (RL) that significantly emphasizes <strong>reasoning capabilities</strong> in large language models (LLMs). Their technique differs from traditional Reinforcement Learning from Human Feedback (RLHF) in several key aspects.</p>

<p>Here’s a breakdown of the techniques DeepSeek used in R1’s reinforcement learning:</p>

<p><strong>1. Pure Reinforcement Learning (RL) for Initial Reasoning (DeepSeek-R1-Zero):</strong></p>

<ul>
  <li><strong>No Initial Supervised Fine-tuning (SFT) on Instructions:</strong> Unlike many LLMs that first undergo SFT on human-written instructions, DeepSeek-R1-Zero was trained with <strong>pure RL</strong> directly on the base model (DeepSeek-V3-Base).</li>
  <li><strong>Group Relative Policy Optimization (GRPO):</strong> They utilized GRPO as their core RL algorithm. GRPO is designed to be more efficient than Proximal Policy Optimization (PPO) by eliminating the need for a separate critic network. It estimates baseline rewards by comparing a group of generated outputs, assigning relative scores based on their quality. This encourages the model to generate better responses compared to its own previous attempts.</li>
  <li><strong>Rule-Based Reward System:</strong> Instead of relying solely on human preferences for the initial RL phase, DeepSeek-R1-Zero used a <strong>rule-based reward system</strong>. This system primarily focused on:
    <ul>
      <li><strong>Accuracy Rewards:</strong> Rewarding the model for providing correct answers, especially in tasks with verifiable solutions like math problems (checking if the final answer is correct).</li>
      <li><strong>Format Rewards:</strong> Rewarding the model for adhering to a specific output format, particularly using <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code> tags to enclose its reasoning process. This encouraged the emergence of chain-of-thought reasoning.</li>
    </ul>
  </li>
  <li><strong>Emergent Reasoning Behaviors:</strong> This pure RL approach allowed DeepSeek-R1-Zero to naturally develop impressive reasoning skills, including self-verification, reflection, and the generation of long chain-of-thought explanations, without explicit human demonstrations for these behaviors.</li>
</ul>

<p><strong>2. Multi-Stage Training for Enhanced Readability and General Capabilities (DeepSeek-R1):</strong></p>

<p>To address the limitations of DeepSeek-R1-Zero (like poor readability and language mixing), DeepSeek-R1 employed a more comprehensive multi-stage training pipeline:</p>

<ul>
  <li><strong>Cold-Start Data Fine-tuning:</strong> Before the main RL phase, the base model was fine-tuned on a small dataset of high-quality, human-written (or generated and refined) long chain-of-thought reasoning examples. This “cold start” data helped guide the model towards producing more readable and coherent reasoning steps.</li>
  <li><strong>Reasoning-Oriented Reinforcement Learning (Second RL Stage):</strong> The model then underwent a second phase of large-scale RL (similar to DeepSeek-R1-Zero) but with an additional <strong>language consistency reward</strong>. This reward penalized the model for mixing languages within its reasoning process.</li>
  <li><strong>Supervised Fine-tuning (SFT):</strong> After the reasoning-oriented RL, the model was further fine-tuned on a diverse dataset that included both reasoning data (synthesized using rejection sampling from the RL model, judged by DeepSeek-V3) and general non-reasoning data (augmented with chain-of-thought). This SFT stage aimed to improve the model’s helpfulness and harmlessness while preserving its strong reasoning abilities.</li>
  <li><strong>RL for All Scenarios (Third RL Stage):</strong> A final RL phase was conducted using prompts from a wider range of scenarios to further refine the model’s overall capabilities and alignment with desired behaviors.</li>
</ul>

<p><strong>Key Differences from Traditional RLHF:</strong></p>

<ul>
  <li><strong>Reduced Reliance on Extensive Human Preference Data:</strong> While some human evaluation might have been involved in judging the quality of synthesized data, the core RL training in DeepSeek-R1 heavily leveraged rule-based rewards, especially in the initial stages. This reduces the cost and complexity of collecting large amounts of direct human preference comparisons.</li>
  <li><strong>Emphasis on Emergent Reasoning:</strong> The pure RL approach aimed to incentivize the model to <em>discover</em> effective reasoning strategies on its own, rather than solely learning from human-provided examples of reasoning.</li>
  <li><strong>Multi-Stage Approach:</strong> DeepSeek’s pipeline involves a carefully orchestrated sequence of pre-training, targeted fine-tuning, and multiple RL stages with different reward signals to achieve both strong reasoning and general language capabilities.</li>
</ul>

<p><strong>Code to Show Reinforcement Learning (Conceptual and Simplified):</strong></p>

<p>It’s challenging to provide a direct, runnable code example that fully replicates DeepSeek’s RL training process due to its complexity and scale. However, the following conceptual PyTorch-like snippet illustrates the core idea of GRPO and a rule-based reward:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Assume you have a pre-trained language model and tokenizer
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"gpt2"</span>  <span class="c1"># Replace with a more suitable base model
</span><span class="n">policy_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy_model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-6</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span>
<span class="n">policy_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_responses</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">num_responses</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">policy_model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_tokens</span><span class="p">.</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">num_responses</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">responses</span>

<span class="k">def</span> <span class="nf">calculate_accuracy_reward</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
    <span class="c1"># Simplified example for a math problem: "What is 2 + 2?"
</span>    <span class="k">if</span> <span class="s">"2 + 2"</span> <span class="ow">in</span> <span class="n">response</span> <span class="ow">and</span> <span class="s">"4"</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>

<span class="k">def</span> <span class="nf">calculate_format_reward</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s">"&lt;think&gt;"</span> <span class="ow">in</span> <span class="n">response</span> <span class="ow">and</span> <span class="s">"&lt;/think&gt;"</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.5</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>

<span class="k">def</span> <span class="nf">calculate_combined_reward</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
    <span class="n">accuracy_reward</span> <span class="o">=</span> <span class="n">calculate_accuracy_reward</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    <span class="n">format_reward</span> <span class="o">=</span> <span class="n">calculate_format_reward</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy_reward</span> <span class="o">+</span> <span class="n">format_reward</span>

<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="n">generate_responses</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">calculate_combined_reward</span><span class="p">(</span><span class="n">resp</span><span class="p">)</span> <span class="k">for</span> <span class="n">resp</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">]).</span><span class="nb">float</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Simplified GRPO-like update: Encourage higher reward responses
</span>    <span class="n">best_reward_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">best_response</span> <span class="o">=</span> <span class="n">responses</span><span class="p">[</span><span class="n">best_reward_index</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">best_response</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">policy_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">inputs</span><span class="p">.</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">best_response</span><span class="p">,</span> <span class="n">rewards</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># Training loop (very simplified)
</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">training_prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Solve: 2 + 2 = ?"</span><span class="p">,</span> <span class="s">"Explain the concept of gravity &lt;think&gt;"</span><span class="p">,</span> <span class="s">"Write a short story about a cat."</span><span class="p">]</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">training_prompts</span><span class="p">[</span><span class="n">episode</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_prompts</span><span class="p">)]</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">best_response</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Episode </span><span class="si">{</span><span class="n">episode</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Best Response: '</span><span class="si">{</span><span class="n">best_response</span><span class="si">}</span><span class="s">', Rewards: </span><span class="si">{</span><span class="n">rewards</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Simplified RL Training Done!"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Important Notes on the Code:</strong></p>

<ul>
  <li><strong>Simplification:</strong> This code is a highly simplified illustration. Real RL training for LLMs involves much more complex algorithms (like the full GRPO implementation), reward functions, handling of action spaces, exploration strategies, and stability techniques.</li>
  <li><strong>Reward Function:</strong> The reward functions here are extremely basic and task-specific. In reality, they would be more nuanced and potentially involve other AI models as reward predictors.</li>
  <li><strong>GRPO Implementation:</strong> The policy update in this example is a very rough approximation of GRPO. The actual GRPO algorithm involves comparing probabilities of generated tokens within a group and updating the policy based on relative performance.</li>
  <li><strong>Scale:</strong> Training a model like DeepSeek-R1 requires massive computational resources and datasets, far beyond what can be demonstrated in a simple code snippet.</li>
</ul>

<p>This explanation and the conceptual code provide a foundational understanding of the innovative reinforcement learning techniques employed by DeepSeek in the development of their R1 model, particularly their focus on pure RL and rule-based rewards for fostering strong reasoning capabilities.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
