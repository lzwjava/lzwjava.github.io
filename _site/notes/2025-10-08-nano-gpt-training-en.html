<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Training Custom Datasets with nanoGPT</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Training Custom Datasets with nanoGPT | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Training Custom Datasets with nanoGPT" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-10-08-nano-gpt-training-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-10-08-nano-gpt-training-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Training Custom Datasets with nanoGPT" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Training Custom Datasets with nanoGPT","url":"https://lzwjava.github.io/notes/2025-10-08-nano-gpt-training-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=abab3ad3c8719b30cb909e241d77f45f13866ab6">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=abab3ad3c8719b30cb909e241d77f45f13866ab6" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Training Custom Datasets with nanoGPT | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-10-08-nano-gpt-training-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-10-08-nano-gpt-training-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/nano-gpt-training-en" selected>English</option>
        <option value="/nano-gpt-training-zh" >中文</option>
        <option value="/nano-gpt-training-ja" >日本語</option>
        <option value="/nano-gpt-training-es" >Español</option>
        <option value="/nano-gpt-training-hi" >हिंदी</option>
        <option value="/nano-gpt-training-fr" >Français</option>
        <option value="/nano-gpt-training-de" >Deutsch</option>
        <option value="/nano-gpt-training-ar" >العربية</option>
        <option value="/nano-gpt-training-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h3 id="preparing-and-training-a-custom-text-dataset-with-nanogpt">Preparing and Training a Custom Text Dataset with nanoGPT</h3>

<p>nanoGPT is designed for easy training on text data, including custom datasets. The process involves preparing your text into binary token files, configuring the training parameters, and running the training script. Below, I’ll outline the steps based on the repository’s examples (e.g., Shakespeare or OpenWebText). You’ll need Python 3.8+ and a GPU for efficient training (CPU works but is slower).</p>

<h4 id="1-install-dependencies">1. Install Dependencies</h4>
<p>First, set up the environment:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install torch numpy transformers datasets tiktoken wandb tqdm
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">torch</code>: For PyTorch (install with CUDA if using GPU: e.g., <code class="language-plaintext highlighter-rouge">pip install torch --index-url https://download.pytorch.org/whl/cu118</code>).</li>
  <li>Others handle tokenization, data loading, and logging.</li>
</ul>

<h4 id="2-prepare-your-custom-dataset">2. Prepare Your Custom Dataset</h4>
<p>nanoGPT expects your data as binary files (<code class="language-plaintext highlighter-rouge">train.bin</code> and <code class="language-plaintext highlighter-rouge">val.bin</code>) containing tokenized integers. You’ll need to write a simple preparation script to process your raw text.</p>

<ul>
  <li>
    <p><strong>Place Your Text File</strong>: Put your raw text (e.g., <code class="language-plaintext highlighter-rouge">input.txt</code>) in a new folder under <code class="language-plaintext highlighter-rouge">data/</code>, like <code class="language-plaintext highlighter-rouge">data/my_dataset/</code>.</p>
  </li>
  <li>
    <p><strong>Create a Preparation Script</strong>: Copy and adapt an example from the repo (e.g., <code class="language-plaintext highlighter-rouge">data/shakespeare_char/prepare.py</code> for character-level or <code class="language-plaintext highlighter-rouge">data/openwebtext/prepare.py</code> for GPT-2 BPE token-level).</p>

    <p><strong>Example for Character-Level Tokenization</strong> (simple for small datasets; treats each character as a token):</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Save as data/my_dataset/prepare.py
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">random_split</span>

<span class="c1"># Load your text (replace with your file path)
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'data/my_dataset/input.txt'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>

<span class="c1"># Encode as characters
</span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">ch</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">itos</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">s</span><span class="p">):</span> <span class="k">return</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">l</span><span class="p">):</span> <span class="k">return</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>

<span class="c1"># Tokenize the entire text
</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

<span class="c1"># Split into train/val (90/10)
</span><span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>

<span class="c1"># Save as .bin files
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">val_data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">train_data</span><span class="p">.</span><span class="n">tofile</span><span class="p">(</span><span class="s">'data/my_dataset/train.bin'</span><span class="p">)</span>
<span class="n">val_data</span><span class="p">.</span><span class="n">tofile</span><span class="p">(</span><span class="s">'data/my_dataset/val.bin'</span><span class="p">)</span>

<span class="c1"># Print stats
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Length of dataset in characters: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Vocab size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>Run it:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python data/my_dataset/prepare.py
</code></pre></div>    </div>
    <p>This creates <code class="language-plaintext highlighter-rouge">train.bin</code> and <code class="language-plaintext highlighter-rouge">val.bin</code>.</p>
  </li>
  <li>
    <p><strong>For GPT-2 BPE Tokenization</strong> (better for larger datasets; uses subwords):
Adapt <code class="language-plaintext highlighter-rouge">data/openwebtext/prepare.py</code>. You’ll need to install <code class="language-plaintext highlighter-rouge">tiktoken</code> (already in deps) and handle your text similarly, but use <code class="language-plaintext highlighter-rouge">tiktoken.get_encoding("gpt2").encode()</code> instead of character encoding. Split your text into train/val chunks (e.g., 90/10), then save as NumPy arrays to <code class="language-plaintext highlighter-rouge">.bin</code>.</p>
  </li>
  <li>
    <p><strong>Tips</strong>:</p>
    <ul>
      <li>Keep your dataset clean (e.g., remove special chars if needed).</li>
      <li>For very large files, process in chunks to avoid memory issues.</li>
      <li>Vocab size: ~65 for chars (Shakespeare); ~50k for BPE.</li>
    </ul>
  </li>
</ul>

<h4 id="3-configure-training">3. Configure Training</h4>
<p>Create a config file by copying an example (e.g., <code class="language-plaintext highlighter-rouge">config/train_shakespeare_char.py</code>) to <code class="language-plaintext highlighter-rouge">config/train_my_dataset.py</code> and edit it.</p>

<p>Key parameters to tweak:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example config snippet
</span><span class="n">out_dir</span> <span class="o">=</span> <span class="s">'out-my_dataset'</span>  <span class="c1"># Output folder for checkpoints
</span><span class="n">dataset</span> <span class="o">=</span> <span class="s">'my_dataset'</span>      <span class="c1"># Matches your data folder name
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>             <span class="c1"># Adjust based on GPU memory (e.g., 12 for small GPU)
</span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">256</span>            <span class="c1"># Context length (tokens per example)
</span><span class="n">n_layer</span> <span class="o">=</span> <span class="mi">6</span>                 <span class="c1"># Transformer layers
</span><span class="n">n_head</span> <span class="o">=</span> <span class="mi">6</span>                  <span class="c1"># Attention heads
</span><span class="n">n_embd</span> <span class="o">=</span> <span class="mi">384</span>                <span class="c1"># Embedding dimension
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>            <span class="c1"># Training steps
</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">6e-4</span>                   <span class="c1"># Learning rate
</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>               <span class="c1"># Dropout rate
</span><span class="n">init_from</span> <span class="o">=</span> <span class="s">'scratch'</span>       <span class="c1"># 'scratch' for new model; 'gpt2' to load pretrained
</span></code></pre></div></div>
<ul>
  <li>For finetuning (start from pretrained GPT-2): Set <code class="language-plaintext highlighter-rouge">init_from = 'gpt2'</code> (or ‘gpt2-medium’).</li>
  <li>For Apple Silicon: Add <code class="language-plaintext highlighter-rouge">device = 'mps'</code>.</li>
  <li>Save checkpoints every 500 iters by default.</li>
</ul>

<h4 id="4-run-training">4. Run Training</h4>
<p>Train from scratch or finetune:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py config/train_my_dataset.py
</code></pre></div></div>
<ul>
  <li>For multi-GPU (e.g., 8 GPUs): <code class="language-plaintext highlighter-rouge">torchrun --standalone --nproc_per_node=8 train.py config/train_my_dataset.py</code>.</li>
  <li>Monitor with WandB (logs losses, samples; enable in config with <code class="language-plaintext highlighter-rouge">wandb_log = True</code>).</li>
  <li>Training time: Minutes for tiny models on GPU; hours/days for larger ones.</li>
</ul>

<h4 id="5-generate-samples-test-the-model">5. Generate Samples (Test the Model)</h4>
<p>After training, sample text:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python sample.py --out_dir=out-my_dataset --start="Your prompt here"
</code></pre></div></div>
<p>This loads the latest checkpoint and generates.</p>

<h4 id="common-issues">Common Issues</h4>
<ul>
  <li><strong>Memory Errors</strong>: Reduce <code class="language-plaintext highlighter-rouge">batch_size</code> or <code class="language-plaintext highlighter-rouge">block_size</code>.</li>
  <li><strong>No GPU</strong>: Add <code class="language-plaintext highlighter-rouge">--device=cpu --compile=False</code> to commands.</li>
  <li><strong>Large Datasets</strong>: Use distributed training for scaling.</li>
</ul>

<p>For more details, check the repo’s README and data/config examples.</p>

<p><a href="https://github.com/karpathy/nanoGPT">nanoGPT GitHub</a></p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    
      <p class="model">x-ai/grok-4-fast</p>
    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
