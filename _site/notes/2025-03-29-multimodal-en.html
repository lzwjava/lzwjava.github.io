<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Multimodal LLMs</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Multimodal LLMs | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Multimodal LLMs" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-03-29-multimodal-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-03-29-multimodal-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multimodal LLMs" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Multimodal LLMs","url":"https://lzwjava.github.io/notes/2025-03-29-multimodal-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=e2bed1763237c6907af2330d857dddfa57c6c9a4">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=e2bed1763237c6907af2330d857dddfa57c6c9a4" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Multimodal LLMs | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-03-29-multimodal-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-03-29-multimodal-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/multimodal-en" selected>English</option>
        <option value="/multimodal-zh" >中文</option>
        <option value="/multimodal-ja" >日本語</option>
        <option value="/multimodal-es" >Español</option>
        <option value="/multimodal-hi" >हिंदी</option>
        <option value="/multimodal-fr" >Français</option>
        <option value="/multimodal-de" >Deutsch</option>
        <option value="/multimodal-ar" >العربية</option>
        <option value="/multimodal-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Let’s break this down step by step to explain how multimodal implementation works in large language models (LLMs) and address your questions about images and text being converted to vectors.</p>

<h3 id="multimodal-llms-how-they-work">Multimodal LLMs: How They Work</h3>
<p>A multimodal LLM is designed to process and generate responses based on multiple types of data, such as text, images, audio, or even structured data like tables. Unlike traditional LLMs that handle only text, multimodal models integrate these different “modalities” into a unified framework. Here’s how it’s typically implemented:</p>

<ol>
  <li><strong>Separate Encoders for Each Modality</strong>:
    <ul>
      <li><strong>Text</strong>: Text is processed using a tokenizer (e.g., splitting it into words or subwords) and then converted into numerical representations called embeddings (vectors) using a vocabulary or a pre-trained embedding layer. This is standard in models like BERT or GPT.</li>
      <li><strong>Images</strong>: Images are processed using a vision model, such as a convolutional neural network (CNN) or a Vision Transformer (ViT). These models extract features from the image (like edges, shapes, or objects) and convert them into a vector representation in a high-dimensional space.</li>
      <li>Other modalities (e.g., audio) follow a similar process with specialized encoders (e.g., converting sound waves into spectrograms and then vectors).</li>
    </ul>
  </li>
  <li><strong>Unified Representation</strong>:
    <ul>
      <li>Once each modality is encoded into vectors, the model aligns these representations so they can “talk” to each other. This might involve projecting them into a shared embedding space where text vectors and image vectors are compatible. Techniques like cross-attention mechanisms (borrowed from Transformers) help the model understand relationships between modalities—for example, linking the word “cat” in text to an image of a cat.</li>
    </ul>
  </li>
  <li><strong>Training</strong>:
    <ul>
      <li>The model is trained on datasets that pair modalities (e.g., images with captions) so it learns to associate text descriptions with visual features. This could involve contrastive learning (e.g., CLIP) or joint training where the model predicts text from images or vice versa.</li>
    </ul>
  </li>
  <li><strong>Output Generation</strong>:
    <ul>
      <li>When generating a response, the model uses its decoder (or a unified Transformer architecture) to produce text, images, or both, depending on the task. For example, it might generate a caption for an image or answer a question about a picture.</li>
    </ul>
  </li>
</ol>

<h3 id="does-an-image-change-to-a-vector-too">Does an Image Change to a Vector Too?</h3>
<p>Yes, absolutely! Just like text, images are converted into vectors in multimodal LLMs:</p>
<ul>
  <li><strong>How It Works</strong>: An image is fed into a vision encoder (e.g., a pre-trained ResNet or ViT). This encoder processes the raw pixel data and outputs a fixed-size vector (or a sequence of vectors) that captures the image’s semantic content—like objects, colors, or patterns.</li>
  <li><strong>Example</strong>: A photo of a dog might be transformed into a 512-dimensional vector that encodes “dog-like” features. This vector doesn’t look like the image to us but contains numerical information the model can use.</li>
  <li><strong>Difference from Text</strong>: While text vectors come from a vocabulary (e.g., word embeddings for “dog” or “cat”), image vectors come from spatial and visual features extracted by the vision model. Both end up as numbers in a vector space, though.</li>
</ul>

<h3 id="text-to-vectors-constructing-a-vocabulary">Text to Vectors: Constructing a Vocabulary</h3>
<p>You mentioned text being changed to vectors by constructing a vocabulary—here’s how that happens:</p>
<ul>
  <li><strong>Tokenization</strong>: Text is broken into smaller units (tokens), like words or subwords (e.g., “playing” might split into “play” and “##ing” in models like BERT).</li>
  <li><strong>Vocabulary</strong>: A pre-defined vocabulary maps each token to a unique ID. For example, “dog” might be ID 250, and “cat” might be ID 300.</li>
  <li><strong>Embedding Layer</strong>: Each token ID is converted into a dense vector (e.g., a 768-dimensional vector) using an embedding matrix. These vectors are learned during training to capture semantic meaning—words with similar meanings (like “dog” and “puppy”) end up with similar vectors.</li>
  <li><strong>Contextualization</strong>: In modern LLMs, a Transformer then refines these vectors based on context (e.g., “bank” in “river bank” vs. “money bank” gets different vectors).</li>
</ul>

<h3 id="key-similarity-between-text-and-images">Key Similarity Between Text and Images</h3>
<p>Both text and images are ultimately represented as vectors in a high-dimensional space. The magic of multimodal models lies in aligning these spaces so the model can reason across them. For instance:</p>
<ul>
  <li>An image of a dog and the word “dog” might be mapped to nearby points in this shared space.</li>
  <li>The model can then answer questions like “What’s in this picture?” by bridging the image vector to text vectors.</li>
</ul>

<h3 id="challenges-in-multimodal-implementation">Challenges in Multimodal Implementation</h3>
<ul>
  <li><strong>Alignment</strong>: Ensuring text and image vectors are meaningfully related is tricky and requires large paired datasets.</li>
  <li><strong>Computation</strong>: Processing images alongside text is resource-intensive compared to text-only models.</li>
  <li><strong>Interpretation</strong>: The model needs to “understand” when to prioritize one modality over another based on the input.</li>
</ul>

<p>Does that clarify things? If you’d like me to dive deeper into any part—like how vision encoders work or what a vector space looks like—let me know!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
