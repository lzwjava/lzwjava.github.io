<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Distributed Training with Ray on Mixed Hardware</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Distributed Training with Ray on Mixed Hardware | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Distributed Training with Ray on Mixed Hardware" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-07-27-heterogeneous-distributed-training-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-07-27-heterogeneous-distributed-training-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Distributed Training with Ray on Mixed Hardware" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Distributed Training with Ray on Mixed Hardware","url":"https://lzwjava.github.io/notes/2025-07-27-heterogeneous-distributed-training-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=abc7b8c71821541dfbfc7ccdde8f43ff4b55d846">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=abc7b8c71821541dfbfc7ccdde8f43ff4b55d846" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Distributed Training with Ray on Mixed Hardware | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-07-27-heterogeneous-distributed-training-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-07-27-heterogeneous-distributed-training-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.09</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/heterogeneous-distributed-training-en" selected>English</option>
        <option value="/heterogeneous-distributed-training-zh" >中文</option>
        <option value="/heterogeneous-distributed-training-ja" >日本語</option>
        <option value="/heterogeneous-distributed-training-es" >Español</option>
        <option value="/heterogeneous-distributed-training-hi" >हिंदी</option>
        <option value="/heterogeneous-distributed-training-fr" >Français</option>
        <option value="/heterogeneous-distributed-training-de" >Deutsch</option>
        <option value="/heterogeneous-distributed-training-ar" >العربية</option>
        <option value="/heterogeneous-distributed-training-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Yes, you can use Python for distributed training of machine learning models across your MacBook Air M2 (which uses Apple Silicon with the MPS backend for GPU acceleration) and your Intel i9 desktop with an NVIDIA 4070Ti (which supports CUDA). However, this setup is heterogeneous (different architectures, OSes, and accelerators), so it’s not as straightforward as a homogeneous cluster (e.g., all NVIDIA GPUs). Native distributed training in frameworks like PyTorch won’t work seamlessly out of the box due to incompatible backends—PyTorch on your Mac uses MPS (Metal Performance Shaders), while on the desktop it uses CUDA, and communication libraries like NCCL (required for efficient GPU-to-GPU sync) are NVIDIA-only and unavailable on Apple Silicon.</p>

<p>That said, you can achieve distributed training using higher-level orchestration libraries like Ray, which abstracts away the hardware differences. Other options like Dask or custom frameworks exist but are more limited for deep learning. I’ll outline the feasibility, recommended approach, and alternatives below.</p>

<h3 id="recommended-approach-use-ray-for-distributed-training">Recommended Approach: Use Ray for Distributed Training</h3>
<p>Ray is a Python-based distributed computing framework that’s hardware-agnostic and supports scaling ML workloads across mixed machines (e.g., macOS on Apple Silicon and Windows/Linux on NVIDIA). It installs on both platforms and can handle heterogeneous accelerators by running tasks on each machine’s available hardware (MPS on Mac, CUDA on desktop).</p>

<h4 id="how-it-works">How It Works</h4>
<ul>
  <li><strong>Setup</strong>: Install Ray on both machines via pip (<code class="language-plaintext highlighter-rouge">pip install "ray[default,train]"</code>). Start a Ray cluster: one machine as the head node (e.g., your desktop), and connect the Mac as a worker node over the network. Ray handles communication via its own protocol.</li>
  <li><strong>Training Pattern</strong>: Use Ray Train for scaling frameworks like PyTorch or TensorFlow. For heterogeneous setups:
    <ul>
      <li>Employ a “parameter server” architecture: A central coordinator (on one machine) manages model weights.</li>
      <li>Define workers that run on specific hardware: Use decorators like <code class="language-plaintext highlighter-rouge">@ray.remote(num_gpus=1)</code> for your NVIDIA desktop (CUDA) and <code class="language-plaintext highlighter-rouge">@ray.remote(num_cpus=2)</code> or similar for the Mac (MPS or CPU fallback).</li>
      <li>Each worker computes gradients on its local device, sends them to the parameter server for averaging, and receives updated weights.</li>
      <li>Ray automatically distributes data batches and syncs across machines.</li>
    </ul>
  </li>
  <li><strong>Example Workflow</strong>:
    <ol>
      <li>Define your model in PyTorch (set device to <code class="language-plaintext highlighter-rouge">"mps"</code> on Mac, <code class="language-plaintext highlighter-rouge">"cuda"</code> on desktop).</li>
      <li>Use Ray’s API to wrap your training loop.</li>
      <li>Run the script on the head node; Ray dispatches tasks to workers.</li>
    </ol>
  </li>
  <li><strong>Performance</strong>: Training will be slower than a pure NVIDIA cluster due to network overhead and no direct GPU-to-GPU communication (e.g., via NCCL). The Mac’s M2 GPU is weaker than the 4070Ti, so balance workloads accordingly (e.g., smaller batches on Mac).</li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Best for data-parallel training or hyperparameter tuning; model-parallel (splitting a large model across devices) is trickier in heterogeneous setups.</li>
      <li>For very large models (e.g., 1B+ parameters), add techniques like mixed precision, gradient checkpointing, or integration with DeepSpeed.</li>
      <li>Network latency between machines can bottleneck; ensure they’re on the same fast LAN.</li>
      <li>Tested examples show it working on Apple M4 (similar to M2) + older NVIDIA GPUs, but optimize for your 4070Ti’s strength.</li>
    </ul>
  </li>
</ul>

<p>A practical example and code are available in a framework called “distributed-hetero-ml”, which simplifies this for heterogeneous hardware.</p>

<h4 id="why-ray-fits-your-setup">Why Ray Fits Your Setup</h4>
<ul>
  <li>Cross-platform: Works on macOS (Apple Silicon), Windows, and Linux.</li>
  <li>Integrates with PyTorch: Use Ray Train to scale your existing code.</li>
  <li>No need for identical hardware: It detects and uses MPS on Mac and CUDA on desktop.</li>
</ul>

<h3 id="alternative-dask-for-distributed-workloads">Alternative: Dask for Distributed Workloads</h3>
<p>Dask is another Python library for parallel computing, suitable for distributed data processing and some ML tasks (e.g., via Dask-ML or XGBoost).</p>
<ul>
  <li><strong>How</strong>: Set up a Dask cluster (one scheduler on your desktop, workers on both machines). Use libraries like CuPy/RAPIDS on the NVIDIA side for GPU accel, and fall back to CPU/MPS on Mac.</li>
  <li><strong>Use Cases</strong>: Good for ensemble methods, hyperparameter search, or scikit-learn-style models. For deep learning, pair with PyTorch/TensorFlow, but sync is manual and less efficient than Ray.</li>
  <li><strong>Limitations</strong>: Not optimized for synchronized deep learning training (e.g., no built-in parameter server); better for embarrassingly parallel tasks. GPU support requires CUDA on NVIDIA, but Mac would use CPU or limited MPS integration.</li>
  <li><strong>Setup</strong>: <code class="language-plaintext highlighter-rouge">pip install dask distributed</code>, then start a cluster and submit jobs.</li>
</ul>

<h3 id="other-options-and-considerations">Other Options and Considerations</h3>
<ul>
  <li><strong>Horovod</strong>: This distributed training wrapper (for PyTorch/TensorFlow) installs on macOS, but lacks explicit MPS support. It can use Gloo or MPI for communication (cross-platform), but efficiency drops without NCCL on the Mac. Possible but untested for your exact mix—try it if Ray doesn’t fit.</li>
  <li><strong>Custom Frameworks</strong>: Tools like HetSeq allow heterogeneous GPU training, but they’re geared toward mixed NVIDIA/AMD, not Apple Silicon + CUDA.</li>
  <li><strong>Challenges Across All</strong>:
    <ul>
      <li>OS Differences: Assume your desktop runs Linux/Windows; ensure compatible Python versions (3.10+ recommended).</li>
      <li>Dependencies: Install PyTorch with MPS on Mac (<code class="language-plaintext highlighter-rouge">pip install torch --extra-index-url https://download.pytorch.org/whl/cpu</code>) and CUDA on desktop.</li>
      <li>Data Sharing: Use a shared filesystem (e.g., NFS) or cloud storage for datasets.</li>
      <li>Testing: Start small—train a simple CNN on MNIST to verify.</li>
    </ul>
  </li>
  <li><strong>When to Avoid</strong>: If models are small, train solely on the 4070Ti (faster). For production, consider cloud GPUs for homogeneity.</li>
</ul>

<p>If you provide more details (e.g., framework like PyTorch/TensorFlow, model type, OS on desktop), I can refine this.</p>

<p><a href="https://www.dilawar.ai/2025/07/04/Multi-Cluster%20Distributed%20Training%20on%20Heterogeneous%20Hardware/">Being GPU Poor makes you creative</a><br />
<a href="https://docs.ray.io/en/latest/ray-overview/installation.html">Ray Documentation - Installation</a><br />
<a href="https://docs.ray.io/en/latest/train/train.html">Ray Train: Scalable Model Training</a><br />
<a href="https://docs.dask.org/en/stable/gpu.html">Dask Documentation - GPUs</a><br />
<a href="https://horovod.readthedocs.io/en/latest/install_include.html">Horovod Installation Guide</a><br />
<a href="https://github.com/yifding/hetseq">HetSeq GitHub</a><br />
<a href="https://developer.apple.com/metal/pytorch/">Accelerated PyTorch training on Mac</a></p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
