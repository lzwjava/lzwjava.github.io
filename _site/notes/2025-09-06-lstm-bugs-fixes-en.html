<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>LSTM Implementation Bugs and Fixes</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LSTM Implementation Bugs and Fixes | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="LSTM Implementation Bugs and Fixes" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-09-06-lstm-bugs-fixes-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-09-06-lstm-bugs-fixes-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-06T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LSTM Implementation Bugs and Fixes" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2025-09-06T00:00:00+08:00","datePublished":"2025-09-06T00:00:00+08:00","description":"李智维","headline":"LSTM Implementation Bugs and Fixes","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/notes/2025-09-06-lstm-bugs-fixes-en"},"url":"https://lzwjava.github.io/notes/2025-09-06-lstm-bugs-fixes-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=96cc9607af4bca266a9f113e490b23ee7fa72ec6">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=96cc9607af4bca266a9f113e490b23ee7fa72ec6" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       LSTM Implementation Bugs and Fixes | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-09-06-lstm-bugs-fixes-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-09-06-lstm-bugs-fixes-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.09</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/lstm-bugs-fixes-en" selected>English</option>
        <option value="/lstm-bugs-fixes-zh" >中文</option>
        <option value="/lstm-bugs-fixes-ja" >日本語</option>
        <option value="/lstm-bugs-fixes-es" >Español</option>
        <option value="/lstm-bugs-fixes-hi" >हिंदी</option>
        <option value="/lstm-bugs-fixes-fr" >Français</option>
        <option value="/lstm-bugs-fixes-de" >Deutsch</option>
        <option value="/lstm-bugs-fixes-ar" >العربية</option>
        <option value="/lstm-bugs-fixes-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Got it—here’s a clear, end-to-end walkthrough of what this LSTM-from-scratch script is doing, plus the subtle bugs that will break training and how to fix them. I’ll keep it linear and easy to scan (no wide tables).</p>

<h1 id="what-the-code-tries-to-do">What the code tries to do</h1>

<p>You’re implementing a minimal character/word-level LSTM language model in NumPy:</p>

<ol>
  <li><strong>Setup &amp; sizes</strong></li>
</ol>

<ul>
  <li><code class="language-plaintext highlighter-rouge">hidden_size</code>, <code class="language-plaintext highlighter-rouge">vocab_size</code>, datasets (<code class="language-plaintext highlighter-rouge">training_set</code>, <code class="language-plaintext highlighter-rouge">validation_set</code>, <code class="language-plaintext highlighter-rouge">test_set</code>), token↔index maps, and functions like <code class="language-plaintext highlighter-rouge">sigmoid</code>, <code class="language-plaintext highlighter-rouge">tanh</code>, <code class="language-plaintext highlighter-rouge">softmax</code>, <code class="language-plaintext highlighter-rouge">one_hot_encode_sequence</code> are expected to come from <code class="language-plaintext highlighter-rouge">data.py</code> / <code class="language-plaintext highlighter-rouge">utils.py</code>.</li>
  <li>Each time step’s input is a one-hot vector <code class="language-plaintext highlighter-rouge">x ∈ R^{vocab_size×1}</code>.</li>
  <li>You concatenate previous hidden state and current input:
<code class="language-plaintext highlighter-rouge">z = [h_{t-1}; x_t] ∈ R^{(hidden_size+vocab_size)×1}</code>.
That’s why <code class="language-plaintext highlighter-rouge">W_f, W_i, W_g, W_o ∈ R^{hidden_size×(hidden_size+vocab_size)}</code>.</li>
</ul>

<ol>
  <li><strong>Parameter initialization</strong></li>
</ol>

<ul>
  <li><code class="language-plaintext highlighter-rouge">init_lstm(...)</code> allocates weights for the four gates (forget f, input i, candidate g, output o), plus the output projection <code class="language-plaintext highlighter-rouge">W_v</code> to vocabulary.</li>
  <li><code class="language-plaintext highlighter-rouge">init_orthogonal</code> makes each weight matrix orthonormal (via QR). That’s a decent choice for RNNs/LSTMs.</li>
</ul>

<ol>
  <li><strong>Forward pass (per sequence)</strong>
For each time step:</li>
</ol>

<ul>
  <li>Build <code class="language-plaintext highlighter-rouge">z = [h_prev; x]</code>.</li>
  <li>
    <p>Compute gates:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">f_t = σ(W_f z + b_f)</code></li>
      <li><code class="language-plaintext highlighter-rouge">i_t = σ(W_i z + b_i)</code></li>
      <li><code class="language-plaintext highlighter-rouge">g_t = tanh(W_g z + b_g)</code></li>
      <li><code class="language-plaintext highlighter-rouge">C_t = f_t ⊙ C_{t-1} + i_t ⊙ g_t</code></li>
      <li><code class="language-plaintext highlighter-rouge">o_t = σ(W_o z + b_o)</code></li>
      <li><code class="language-plaintext highlighter-rouge">h_t = o_t ⊙ tanh(C_t)</code></li>
    </ul>
  </li>
  <li>
    <p>Project to vocab and softmax:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">v_t = W_v h_t + b_v</code></li>
      <li><code class="language-plaintext highlighter-rouge">ŷ_t = softmax(v_t)</code></li>
    </ul>
  </li>
  <li>The code stores lots of intermediates (<code class="language-plaintext highlighter-rouge">z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s</code>) for backprop.</li>
</ul>

<ol>
  <li><strong>Loss</strong></li>
</ol>

<ul>
  <li>Cross-entropy at each step with a one-hot target:
<code class="language-plaintext highlighter-rouge">L_t = − log ŷ_t[target]</code>
(The code uses an elementwise <code class="language-plaintext highlighter-rouge">-mean(log(ŷ)*one_hot)</code>, which is equivalent to <code class="language-plaintext highlighter-rouge">-log(ŷ[target])</code> up to a constant scaling.)</li>
</ul>

<ol>
  <li><strong>Backward pass (BPTT through LSTM)</strong></li>
</ol>

<ul>
  <li>Classic LSTM gradients, accumulated from <code class="language-plaintext highlighter-rouge">t = T−1 → 0</code>.</li>
  <li>Clip global gradient norm, then SGD update.</li>
</ul>

<ol>
  <li><strong>Training loop</strong></li>
</ol>

<ul>
  <li>For each epoch: evaluate on validation set (just loss), then train on training set (loss + grads + update). Finally, plot training/validation losses.</li>
</ul>

<hr />

<h1 id="critical-issues-these-will-silently-break-learning">Critical issues (these will silently break learning)</h1>

<h2 id="1-off-by-one-indexing-for-h-and-c-in-backprop">1) Off-by-one indexing for <code class="language-plaintext highlighter-rouge">h</code> and <code class="language-plaintext highlighter-rouge">C</code> in backprop</h2>

<p>In <code class="language-plaintext highlighter-rouge">forward</code>, you push the <em>initial</em> states first:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">h_s[0] = h_init</code>, then after step 0 you append <code class="language-plaintext highlighter-rouge">h_0</code> → so <code class="language-plaintext highlighter-rouge">h_s</code> has length <code class="language-plaintext highlighter-rouge">T+1</code> with <code class="language-plaintext highlighter-rouge">h_s[t+1] = h_t</code>.</li>
  <li>Same for <code class="language-plaintext highlighter-rouge">C_s</code>: <code class="language-plaintext highlighter-rouge">C_s[0] = C_init</code>, then <code class="language-plaintext highlighter-rouge">C_s[t+1] = C_t</code>.</li>
</ul>

<p>But in <code class="language-plaintext highlighter-rouge">backward(...)</code> you use <code class="language-plaintext highlighter-rouge">h[t]</code> and <code class="language-plaintext highlighter-rouge">C[t]</code> as if they were <code class="language-plaintext highlighter-rouge">h_t</code> and <code class="language-plaintext highlighter-rouge">C_t</code>. They aren’t; they’re shifted by one.</p>

<p><strong>Fix (simple rule of thumb):</strong></p>

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">h[t+1]</code> where you want <code class="language-plaintext highlighter-rouge">h_t</code>.</li>
  <li>Use <code class="language-plaintext highlighter-rouge">C[t+1]</code> where you want <code class="language-plaintext highlighter-rouge">C_t</code>.</li>
  <li>For “previous cell state” you want <code class="language-plaintext highlighter-rouge">C_prev = C[t]</code> (not <code class="language-plaintext highlighter-rouge">C[t-1]</code>).</li>
</ul>

<p>So inside the <code class="language-plaintext highlighter-rouge">for t in reversed(range(T)):</code> loop:</p>

<ul>
  <li>Current state: <code class="language-plaintext highlighter-rouge">h_t = h[t+1]</code>, <code class="language-plaintext highlighter-rouge">C_t = C[t+1]</code></li>
  <li>Previous state: <code class="language-plaintext highlighter-rouge">C_{t-1} = C[t]</code></li>
</ul>

<p>Your current line:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">C_prev</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>is wrong for <code class="language-plaintext highlighter-rouge">t==0</code> (wraps to the last element) and off by one in general. It must be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">C_prev</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>       <span class="c1"># previous cell state
# and use C_t = C[t+1] as "current"
</span></code></pre></div></div>

<p>And anywhere you use <code class="language-plaintext highlighter-rouge">h[t]</code> intending the current hidden state, change to <code class="language-plaintext highlighter-rouge">h[t+1]</code>.</p>

<h2 id="2-wrong-derivatives-for-several-gates">2) Wrong derivatives for several gates</h2>

<p>You sometimes apply the nonlinearity again instead of its derivative, or forget the derivative flag.</p>

<ul>
  <li>
    <p><strong>Cell state path:</strong>
Correct:
<code class="language-plaintext highlighter-rouge">dC_t += dh_t ⊙ o_t ⊙ (1 - tanh(C_t)^2)</code>
Your code:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dC</span> <span class="o">+=</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">tanh</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">]),</span> <span class="n">derivative</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>That’s <code class="language-plaintext highlighter-rouge">tanh</code> applied twice. Replace with:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dC</span> <span class="o">+=</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">o_t</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">C_t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Forget gate:</strong>
Correct: <code class="language-plaintext highlighter-rouge">df = dC_t ⊙ C_{t-1} ⊙ f_t ⊙ (1 - f_t)</code>
Your code:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">C_prev</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">df</span>
</code></pre></div>    </div>

    <p>Missing the derivative term. Should be:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">C_prev</span>
<span class="n">df</span> <span class="o">*=</span> <span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>      <span class="c1"># if f[t] stores σ pre-activation output
</span></code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Input gate:</strong>
You did:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">di</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">g</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
<span class="n">di</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">di</span>
</code></pre></div>    </div>

    <p>This is fine <strong>if</strong> <code class="language-plaintext highlighter-rouge">sigmoid(x, True)</code> returns σ’(x) <em>not</em> σ(x). More robust (matching how you stored <code class="language-plaintext highlighter-rouge">i[t]</code> as the gate output) is:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">di</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">g</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
<span class="n">di</span> <span class="o">*=</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Candidate gate:</strong>
You did:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dg</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
<span class="n">dg</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">derivative</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">dg</span>
</code></pre></div>    </div>

    <p>If <code class="language-plaintext highlighter-rouge">g[t]</code> stores <code class="language-plaintext highlighter-rouge">tanh(preact)</code>, then <code class="language-plaintext highlighter-rouge">tanh’(preact) = 1 - g[t]^2</code>. So:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dg</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
<span class="n">dg</span> <span class="o">*=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Output gate:</strong>
You did:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">do</span> <span class="o">=</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
<span class="n">do</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">derivative</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">do</span>
</code></pre></div>    </div>

    <p>With the indexing fix (<code class="language-plaintext highlighter-rouge">C_t = C[t+1]</code>, <code class="language-plaintext highlighter-rouge">o_t = o[t]</code>) and derivative as above:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">do</span> <span class="o">=</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">C_t</span><span class="p">)</span>
<span class="n">do</span> <span class="o">*=</span> <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Next cell gradient:</strong>
Correct:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dC_next</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="3-using-h0--c0-to-size-dh_next-and-dc_next">3) Using <code class="language-plaintext highlighter-rouge">h[0]</code> / <code class="language-plaintext highlighter-rouge">C[0]</code> to size <code class="language-plaintext highlighter-rouge">dh_next</code> and <code class="language-plaintext highlighter-rouge">dC_next</code></h2>

<p>You want shapes of the <strong>current</strong> h/C (end-of-sequence), not the initial zeros. Use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dh_next</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">dC_next</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="4-cross-entropy-arithmetic-stability">4) Cross-entropy arithmetic stability</h2>

<p><code class="language-plaintext highlighter-rouge">loss += -np.mean(np.log(outputs[t]) * targets[t])</code> will be fine if <code class="language-plaintext highlighter-rouge">softmax</code> clamps/epsilons internally. If not, add a small epsilon:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-12</span>
<span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="5-training-stability-tweaks">5) Training stability tweaks</h2>

<ul>
  <li>
    <p><strong>Forget-gate bias:</strong> initialize <code class="language-plaintext highlighter-rouge">b_f</code> to a positive value (e.g., <code class="language-plaintext highlighter-rouge">1.0</code>) so early in training the LSTM tends to keep memory:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b_f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="mf">1.0</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Learning rate:</strong> <code class="language-plaintext highlighter-rouge">lr=1e-1</code> with raw SGD is aggressive. Start <code class="language-plaintext highlighter-rouge">1e-2</code> or use Adam.</li>
  <li><strong>Gradient clipping:</strong> Good. Consider per-matrix norms if needed.</li>
</ul>

<hr />

<h1 id="a-corrected-core-for-the-backprop-loop">A corrected core for the backprop loop</h1>

<p>This sketch shows the indexing and derivative fixes. It assumes <code class="language-plaintext highlighter-rouge">f[t], i[t], g[t], o[t]</code> are <em>outputs</em> of their nonlinearities (as in your forward):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">dh_next</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">dC_next</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
    <span class="c1"># Short names with correct indexing
</span>    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>                 <span class="c1"># softmax output
</span>    <span class="n">y_true</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">h_t</span>   <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">C_t</span>   <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">C_tm1</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

    <span class="c1"># Output layer
</span>    <span class="n">dv</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dv</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_true</span><span class="p">)]</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">W_v_d</span> <span class="o">+=</span> <span class="n">dv</span> <span class="o">@</span> <span class="n">h_t</span><span class="p">.</span><span class="n">T</span>
    <span class="n">b_v_d</span> <span class="o">+=</span> <span class="n">dv</span>

    <span class="c1"># Backprop into h
</span>    <span class="n">dh</span> <span class="o">=</span> <span class="n">W_v</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dv</span>
    <span class="n">dh</span> <span class="o">+=</span> <span class="n">dh_next</span>

    <span class="c1"># Output gate
</span>    <span class="n">do</span> <span class="o">=</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">C_t</span><span class="p">)</span>
    <span class="n">do</span> <span class="o">*=</span> <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">W_o_d</span> <span class="o">+=</span> <span class="n">do</span> <span class="o">@</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">T</span>
    <span class="n">b_o_d</span> <span class="o">+=</span> <span class="n">do</span>

    <span class="c1"># Cell state
</span>    <span class="n">dC</span> <span class="o">=</span> <span class="n">dC_next</span> <span class="o">+</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">C_t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Candidate (g), Input (i), Forget (f)
</span>    <span class="n">dg</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">dg</span> <span class="o">*=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>               <span class="c1"># since g[t] = tanh(preact)
</span>
    <span class="n">di</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">g</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">di</span> <span class="o">*=</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">C_tm1</span>
    <span class="n">df</span> <span class="o">*=</span> <span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>

    <span class="c1"># Accumulate gate weight/bias grads
</span>    <span class="n">W_g_d</span> <span class="o">+=</span> <span class="n">dg</span> <span class="o">@</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">T</span><span class="p">;</span>  <span class="n">b_g_d</span> <span class="o">+=</span> <span class="n">dg</span>
    <span class="n">W_i_d</span> <span class="o">+=</span> <span class="n">di</span> <span class="o">@</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">T</span><span class="p">;</span>  <span class="n">b_i_d</span> <span class="o">+=</span> <span class="n">di</span>
    <span class="n">W_f_d</span> <span class="o">+=</span> <span class="n">df</span> <span class="o">@</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">T</span><span class="p">;</span>  <span class="n">b_f_d</span> <span class="o">+=</span> <span class="n">df</span>

    <span class="c1"># Propagate to z and split to h_{t-1} part
</span>    <span class="n">dz</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_f</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">df</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">W_i</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">di</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">W_g</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dg</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">W_o</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">do</span><span class="p">)</span>
    <span class="n">dh_next</span> <span class="o">=</span> <span class="n">dz</span><span class="p">[:</span><span class="n">hidden_size</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">dC_next</span> <span class="o">=</span> <span class="n">dC</span> <span class="o">*</span> <span class="n">f</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</code></pre></div></div>

<hr />

<h1 id="smaller-but-important-details--recommendations">Smaller but important details &amp; recommendations</h1>

<ul>
  <li><strong>Orthogonal init:</strong> Good for recurrent weights. For input parts, you could also scale by <code class="language-plaintext highlighter-rouge">1/√fan_in</code>. Not critical here.</li>
  <li><strong>Batching:</strong> The code trains on 1 sequence at a time (no batching). Fine for clarity, but slow/noisy. Mini-batching would require careful shape changes.</li>
  <li><strong>Teacher forcing:</strong> You’re using ground-truth next token as the target; all good. If you ever do free-running generation, sample from <code class="language-plaintext highlighter-rouge">softmax</code> outputs and feed back into the model.</li>
  <li><strong>Loss reporting:</strong> You sum per-time-step losses across sequences and then average by dataset length; that mixes scales if sequences vary in length. Consider normalizing by total tokens.</li>
  <li><strong>Plotting:</strong> Works locally. In headless environments, remember to switch backends or savefig.</li>
</ul>

<hr />

<h1 id="tldr-of-the-fixes-you-should-make-right-now">TL;DR of the fixes you should make right now</h1>

<ol>
  <li><strong>Indexing</strong>: Replace every <code class="language-plaintext highlighter-rouge">h[t]</code> with <code class="language-plaintext highlighter-rouge">h[t+1]</code>, every <code class="language-plaintext highlighter-rouge">C[t]</code> with <code class="language-plaintext highlighter-rouge">C[t+1]</code>, and set <code class="language-plaintext highlighter-rouge">C_prev = C[t]</code> in backprop.</li>
  <li>
    <p><strong>Derivatives</strong>:</p>

    <ul>
      <li>Use <code class="language-plaintext highlighter-rouge">(1 - tanh(C_t)**2)</code> for the cell state path.</li>
      <li>Use <code class="language-plaintext highlighter-rouge">gate * (1 - gate)</code> for the σ-gates since you stored gate outputs.</li>
      <li>Use <code class="language-plaintext highlighter-rouge">(1 - g[t]**2)</code> for the candidate gate since <code class="language-plaintext highlighter-rouge">g[t]=tanh(preact)</code>.</li>
    </ul>
  </li>
  <li><strong>Init</strong>: Set <code class="language-plaintext highlighter-rouge">b_f</code> to ~<code class="language-plaintext highlighter-rouge">1.0</code>.</li>
  <li><strong>LR</strong>: Drop to <code class="language-plaintext highlighter-rouge">1e-2</code> (or switch to Adam).</li>
  <li><strong>Stability</strong>: Add <code class="language-plaintext highlighter-rouge">eps</code> in <code class="language-plaintext highlighter-rouge">log</code>.</li>
</ol>

<p>Make those changes and your loss should start decreasing in a much more sensible way. If you want, paste your updated <code class="language-plaintext highlighter-rouge">backward(...)</code> and I’ll sanity-check it.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
