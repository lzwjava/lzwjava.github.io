<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>K, Q, V Mechanism in Transformers</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>K, Q, V Mechanism in Transformers | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="K, Q, V Mechanism in Transformers" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-06-02-attention-kqv-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-06-02-attention-kqv-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="K, Q, V Mechanism in Transformers" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"K, Q, V Mechanism in Transformers","url":"https://lzwjava.github.io/notes/2025-06-02-attention-kqv-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=1a65a1806afd7d2a5aff7d8f6fa48c6cec25a967">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=1a65a1806afd7d2a5aff7d8f6fa48c6cec25a967" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       K, Q, V Mechanism in Transformers | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-06-02-attention-kqv-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-06-02-attention-kqv-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/attention-kqv-en" selected>English</option>
        <option value="/attention-kqv-zh" >中文</option>
        <option value="/attention-kqv-ja" >日本語</option>
        <option value="/attention-kqv-es" >Español</option>
        <option value="/attention-kqv-hi" >हिंदी</option>
        <option value="/attention-kqv-fr" >Français</option>
        <option value="/attention-kqv-de" >Deutsch</option>
        <option value="/attention-kqv-ar" >العربية</option>
        <option value="/attention-kqv-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>The <strong>Key (K), Query (Q), and Value (V)</strong> mechanism is a fundamental component of the <strong>self-attention</strong> mechanism in Transformer-based large language models (LLMs). This mechanism, introduced in the seminal paper <em>“Attention is All You Need”</em> (Vaswani et al., 2017), enables models to weigh the importance of different words in a sequence when processing or generating text. Below, I provide a comprehensive introduction to how the <strong>K, Q, V</strong> mechanism works in the context of Transformer LLMs, covering its intuition, mathematical formulation, implementation in self-attention, and its role in the broader architecture.</p>

<hr />

<h3 id="1-intuition-behind-k-q-v-in-self-attention">1. <strong>Intuition Behind K, Q, V in Self-Attention</strong></h3>
<p>The self-attention mechanism allows a Transformer model to process an input sequence by focusing on relevant parts of the sequence for each word (or token). The <strong>K, Q, V</strong> components are the building blocks of this process, enabling the model to dynamically determine which parts of the input are most relevant to each other.</p>

<ul>
  <li><strong>Query (Q):</strong> Represents the “question” a token asks about other tokens in the sequence. For each token, the query vector encodes what information the token is looking for from the rest of the sequence.</li>
  <li><strong>Key (K):</strong> Represents the “description” of each token in the sequence. The key vector encodes what information a token can provide to others.</li>
  <li><strong>Value (V):</strong> Represents the actual content or information that a token carries. Once the model determines which tokens are relevant (via Q and K interactions), it retrieves the corresponding value vectors to construct the output.</li>
</ul>

<p>The interaction between <strong>Q</strong> and <strong>K</strong> determines how much attention each token should pay to every other token, and the <strong>V</strong> vectors are then weighted and combined based on this attention to produce the output for each token.</p>

<p>Think of it like a library search:</p>
<ul>
  <li><strong>Query</strong>: Your search query (e.g., “machine learning”).</li>
  <li><strong>Key</strong>: The titles or metadata of books in the library, which you compare to your query to find relevant books.</li>
  <li><strong>Value</strong>: The actual content of the books you retrieve after identifying relevant ones.</li>
</ul>

<hr />

<h3 id="2-how-k-q-v-work-in-self-attention">2. <strong>How K, Q, V Work in Self-Attention</strong></h3>
<p>The self-attention mechanism computes a weighted sum of the <strong>Value</strong> vectors, where the weights are determined by the similarity between <strong>Query</strong> and <strong>Key</strong> vectors. Here’s a step-by-step breakdown of the process:</p>

<h4 id="step-1-input-representation">Step 1: Input Representation</h4>
<ul>
  <li>The input to a Transformer layer is a sequence of tokens (e.g., words or subwords), each represented as a high-dimensional embedding vector (e.g., dimension \( d_{\text{model}} = 512 \)).</li>
  <li>For a sequence of \( n \) tokens, the input is a matrix \( X \in \mathbb{R}^{n \times d_{\text{model}}} \), where each row is the embedding of a token.</li>
</ul>

<h4 id="step-2-linear-transformations-to-generate-k-q-v">Step 2: Linear Transformations to Generate K, Q, V</h4>
<ul>
  <li>For each token, three vectors are computed: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>. These are obtained by applying learned linear transformations to the input embeddings:
\[
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
\]
    <ul>
      <li>\( W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} \) are learned weight matrices.</li>
      <li>Typically, \( d_k = d_v \), and they are often set to \( d_{\text{model}} / h \) (where \( h \) is the number of attention heads, explained later).</li>
      <li>The result is:
        <ul>
          <li>\( Q \in \mathbb{R}^{n \times d_k} \): Query matrix for all tokens.</li>
          <li>\( K \in \mathbb{R}^{n \times d_k} \): Key matrix for all tokens.</li>
          <li>\( V \in \mathbb{R}^{n \times d_v} \): Value matrix for all tokens.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="step-3-compute-attention-scores">Step 3: Compute Attention Scores</h4>
<ul>
  <li>The attention mechanism computes how much each token should attend to every other token by calculating the <strong>dot product</strong> between the query vector of one token and the key vectors of all tokens:
\[
\text{Attention Scores} = Q K^T
\]
    <ul>
      <li>This produces a matrix \( \in \mathbb{R}^{n \times n} \), where each entry \( (i, j) \) represents the unnormalized similarity between the query of token \( i \) and the key of token \( j \).</li>
    </ul>
  </li>
  <li>To stabilize gradients and prevent large values, the scores are scaled by the square root of the key dimension:
\[
\text{Scaled Scores} = \frac{Q K^T}{\sqrt{d_k}}
\]
    <ul>
      <li>This is called <strong>scaled dot-product attention</strong>.</li>
    </ul>
  </li>
</ul>

<h4 id="step-4-apply-softmax-to-get-attention-weights">Step 4: Apply Softmax to Get Attention Weights</h4>
<ul>
  <li>The scaled scores are passed through a <strong>softmax</strong> function to convert them into probabilities (attention weights) that sum to 1 for each token:
\[
\text{Attention Weights} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right)
\]
    <ul>
      <li>The result is a matrix \( \in \mathbb{R}^{n \times n} \), where each row represents the attention distribution for a token over all tokens in the sequence.</li>
      <li>High attention weights indicate that the corresponding tokens are highly relevant to each other.</li>
    </ul>
  </li>
</ul>

<h4 id="step-5-compute-the-output">Step 5: Compute the Output</h4>
<ul>
  <li>The attention weights are used to compute a weighted sum of the <strong>Value</strong> vectors:
\[
\text{Attention Output} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\]
    <ul>
      <li>The output is a matrix \( \in \mathbb{R}^{n \times d_v} \), where each row is a new representation of a token, incorporating information from all other tokens based on their relevance.</li>
    </ul>
  </li>
</ul>

<h4 id="step-6-multi-head-attention">Step 6: Multi-Head Attention</h4>
<ul>
  <li>In practice, Transformers use <strong>multi-head attention</strong>, where the above process is performed multiple times in parallel (with different \( W_Q, W_K, W_V \)) to capture different types of relationships:
    <ul>
      <li>The input is split into \( h \) heads, each with smaller \( Q, K, V \) vectors of dimension \( d_k = d_{\text{model}} / h \).</li>
      <li>Each head computes its own attention output.</li>
      <li>The outputs from all heads are concatenated and passed through a final linear transformation:
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}<em>1, \text{head}_2, \dots, \text{head}_h) W_O
\]
where \( W_O \in \mathbb{R}^{h \cdot d_v \times d</em>{\text{model}}} \) is a learned output projection matrix.</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="3-role-of-k-q-v-in-transformer-llms">3. <strong>Role of K, Q, V in Transformer LLMs</strong></h3>
<p>The <strong>K, Q, V</strong> mechanism is used in different parts of the Transformer architecture, depending on the type of attention:</p>

<ul>
  <li><strong>Self-Attention in Encoder (e.g., BERT):</strong>
    <ul>
      <li>All tokens attend to all other tokens in the input sequence (bidirectional attention).</li>
      <li>\( Q, K, V \) are all derived from the same input sequence \( X \).</li>
      <li>This allows the model to capture context from both preceding and following tokens, which is useful for tasks like text classification or question answering.</li>
    </ul>
  </li>
  <li><strong>Self-Attention in Decoder (e.g., GPT):</strong>
    <ul>
      <li>In autoregressive models like GPT, the decoder uses <strong>masked self-attention</strong> to prevent attending to future tokens (since the model generates text sequentially).</li>
      <li>The mask ensures that for each token \( i \), the attention scores for tokens \( j &gt; i \) are set to \(-\infty\) before the softmax, effectively giving them zero weight.</li>
      <li>\( Q, K, V \) are still derived from the input sequence, but the attention is causal (only attending to previous tokens).</li>
    </ul>
  </li>
  <li><strong>Cross-Attention in Encoder-Decoder Models (e.g., T5):</strong>
    <ul>
      <li>In encoder-decoder architectures, the decoder uses cross-attention to attend to the encoder’s output.</li>
      <li>Here, \( Q \) is derived from the decoder’s input, while \( K \) and \( V \) come from the encoder’s output, allowing the decoder to focus on relevant parts of the input sequence when generating output.</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="4-why-k-q-v-work-so-well">4. <strong>Why K, Q, V Work So Well</strong></h3>
<p>The <strong>K, Q, V</strong> mechanism is powerful for several reasons:</p>
<ul>
  <li><strong>Dynamic Contextualization</strong>: It allows each token to gather information from other tokens based on their content, rather than relying on fixed patterns (e.g., as in RNNs or CNNs).</li>
  <li><strong>Parallelization</strong>: Unlike recurrent neural networks, self-attention processes all tokens simultaneously, making it highly efficient on modern hardware like GPUs.</li>
  <li><strong>Flexibility</strong>: Multi-head attention enables the model to capture diverse relationships (e.g., syntactic, semantic) by learning different projections for \( Q, K, V \).</li>
  <li><strong>Scalability</strong>: The mechanism scales well to long sequences (though computational cost grows quadratically with sequence length, mitigated by techniques like sparse attention or efficient Transformers).</li>
</ul>

<hr />

<h3 id="5-mathematical-summary">5. <strong>Mathematical Summary</strong></h3>
<p>The scaled dot-product attention formula is:
\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\]
For multi-head attention:
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}<em>1, \dots, \text{head}_h) W_O
\]
where:
\[
\text{head}_i = \text{Attention}(Q W</em>{Q_i}, K W_{K_i}, V W_{V_i})
\]</p>

<hr />

<h3 id="6-practical-example">6. <strong>Practical Example</strong></h3>
<p>Consider the sentence: <em>“The cat sat on the mat.”</em></p>
<ul>
  <li><strong>Input</strong>: Each word is converted to an embedding vector (e.g., via a word embedding layer).</li>
  <li><strong>Q, K, V Computation</strong>: For each token, the model computes query, key, and value vectors using learned weights.</li>
  <li><strong>Attention Scores</strong>: For the word “cat,” the query vector might assign high attention scores to “sat” and “mat” because they are contextually relevant, while giving lower scores to “the.”</li>
  <li><strong>Output</strong>: The attention mechanism produces a new representation for “cat” that incorporates information from “sat” and “mat,” weighted by their attention scores.</li>
</ul>

<hr />

<h3 id="7-extensions-and-optimizations">7. <strong>Extensions and Optimizations</strong></h3>
<ul>
  <li><strong>Efficient Attention</strong>: To handle long sequences, variants like Performer, Linformer, or Longformer reduce the computational complexity of attention.</li>
  <li><strong>Positional Encodings</strong>: Since self-attention is permutation-invariant, Transformers add positional encodings to the input embeddings to capture word order, which indirectly influences \( Q, K, V \).</li>
  <li><strong>Layer Stacking</strong>: Transformers stack multiple layers of self-attention, allowing \( Q, K, V \) to operate on increasingly abstract representations.</li>
</ul>

<hr />

<h3 id="8-conclusion">8. <strong>Conclusion</strong></h3>
<p>The <strong>K, Q, V</strong> mechanism is the heart of the Transformer’s self-attention, enabling LLMs to model complex relationships in text efficiently and effectively. By computing attention scores based on queries and keys, and aggregating information from value vectors, Transformers achieve state-of-the-art performance in tasks like language modeling, translation, and more. Understanding this mechanism is key to grasping how modern LLMs process and generate human-like text.</p>

<p>If you’d like a deeper dive into any specific aspect (e.g., multi-head attention, code implementation, or applications in specific models), let me know!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
