<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Softmax Function in Deep Learning</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Softmax Function in Deep Learning | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Softmax Function in Deep Learning" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-06-03-softmax-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-06-03-softmax-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Softmax Function in Deep Learning" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Softmax Function in Deep Learning","url":"https://lzwjava.github.io/notes/2025-06-03-softmax-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=134275dfd5e0307e2691e56027c2f6239b430290">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=134275dfd5e0307e2691e56027c2f6239b430290" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Softmax Function in Deep Learning | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-06-03-softmax-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-06-03-softmax-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/softmax-en" selected>English</option>
        <option value="/softmax-zh" >中文</option>
        <option value="/softmax-ja" >日本語</option>
        <option value="/softmax-es" >Español</option>
        <option value="/softmax-hi" >हिंदी</option>
        <option value="/softmax-fr" >Français</option>
        <option value="/softmax-de" >Deutsch</option>
        <option value="/softmax-ar" >العربية</option>
        <option value="/softmax-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h3 id="what-is-softmax">What is Softmax?</h3>

<p>The <strong>Softmax function</strong> is a mathematical function used in machine learning, particularly in deep learning, to convert a vector of raw scores (often called logits) into a probability distribution. It ensures that the output values are positive and sum to 1, making them interpretable as probabilities.</p>

<p>The Softmax function is defined as follows for a vector \( z = [z_1, z_2, \dots, z_n] \):</p>

<p>\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
\]</p>

<p>Where:</p>
<ul>
  <li>\( z_i \): The input score (logit) for the \( i \)-th class.</li>
  <li>\( e^{z_i} \): The exponential of the input score, which ensures positivity.</li>
  <li>\( \sum_{j=1}^n e^{z_j} \): The sum of exponentials of all input scores, used for normalization.</li>
  <li>The output \( \text{Softmax}(z_i) \) represents the probability of the \( i \)-th class.</li>
</ul>

<p>Key properties:</p>
<ul>
  <li><strong>Output range</strong>: Each output value is between 0 and 1.</li>
  <li><strong>Sum to 1</strong>: The sum of all output values equals 1, making it a valid probability distribution.</li>
  <li><strong>Amplifies differences</strong>: The exponential function in Softmax emphasizes larger input values, making the output probabilities more decisive for larger logits.</li>
</ul>

<h3 id="how-softmax-is-applied-in-deep-learning">How Softmax is Applied in Deep Learning</h3>

<p>The Softmax function is commonly used in the <strong>output layer</strong> of neural networks for <strong>multi-class classification</strong> tasks. Here’s how it is applied:</p>

<ol>
  <li><strong>Context in Neural Networks</strong>:
    <ul>
      <li>In a neural network, the final layer often produces raw scores (logits) for each class. For example, in a classification problem with 3 classes (e.g., cat, dog, bird), the network might output logits like \([2.0, 1.0, 0.5]\).</li>
      <li>These logits are not directly interpretable as probabilities because they can be negative, unbounded, and don’t sum to 1.</li>
    </ul>
  </li>
  <li><strong>Role of Softmax</strong>:
    <ul>
      <li>The Softmax function transforms these logits into probabilities. For the example above:
\[
\text{Softmax}([2.0, 1.0, 0.5]) = \left[ \frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \right]
\]
This might result in probabilities like \([0.665, 0.245, 0.090]\), indicating a 66.5% chance for class 1 (cat), 24.5% for class 2 (dog), and 9.0% for class 3 (bird).</li>
    </ul>
  </li>
  <li><strong>Applications</strong>:
    <ul>
      <li><strong>Multi-class classification</strong>: Softmax is used in tasks like image classification (e.g., identifying objects in images), natural language processing (e.g., sentiment analysis with multiple categories), or any problem where an input must be assigned to one of several classes.</li>
      <li><strong>Loss calculation</strong>: Softmax is typically paired with the <strong>cross-entropy loss</strong> function, which measures the difference between the predicted probability distribution and the true distribution (one-hot encoded labels). This loss guides the training of the neural network.</li>
      <li><strong>Decision-making</strong>: The output probabilities can be used to select the most likely class (e.g., by taking the class with the highest probability).</li>
    </ul>
  </li>
  <li><strong>Examples in Deep Learning</strong>:
    <ul>
      <li><strong>Image Classification</strong>: In a convolutional neural network (CNN) like ResNet, the final fully connected layer produces logits for each class (e.g., 1000 classes in ImageNet). Softmax converts these into probabilities to predict the object in an image.</li>
      <li><strong>Natural Language Processing</strong>: In models like transformers (e.g., BERT), Softmax is used in the output layer for tasks like text classification or next-word prediction, where probabilities are needed over a vocabulary or set of classes.</li>
      <li><strong>Reinforcement Learning</strong>: Softmax can be used to convert action scores into probabilities for selecting actions in a policy-based method.</li>
    </ul>
  </li>
  <li><strong>Implementation in Frameworks</strong>:
    <ul>
      <li>In frameworks like <strong>PyTorch</strong> or <strong>TensorFlow</strong>, Softmax is often implemented as a built-in function:
        <ul>
          <li>PyTorch: <code class="language-plaintext highlighter-rouge">torch.nn.Softmax(dim=1)</code> or <code class="language-plaintext highlighter-rouge">torch.nn.functional.softmax()</code></li>
          <li>TensorFlow: <code class="language-plaintext highlighter-rouge">tf.nn.softmax()</code></li>
        </ul>
      </li>
      <li>Many frameworks combine Softmax with cross-entropy loss in a single operation (e.g., <code class="language-plaintext highlighter-rouge">torch.nn.CrossEntropyLoss</code> in PyTorch) for numerical stability, as computing Softmax separately can lead to issues like overflow with large logits.</li>
    </ul>
  </li>
</ol>

<h3 id="practical-considerations">Practical Considerations</h3>
<ul>
  <li><strong>Numerical Stability</strong>: Direct computation of Softmax can lead to overflow due to the exponential function. A common trick is to subtract the maximum logit value from all logits (\( z_i - \max(z) \)) before applying Softmax, which doesn’t change the output but prevents large exponentials.</li>
  <li><strong>Softmax vs. Sigmoid</strong>: For <strong>binary classification</strong>, the sigmoid function is often used instead of Softmax, as it handles two classes more efficiently. Softmax generalizes sigmoid to multiple classes.</li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Softmax assumes mutual exclusivity (one correct class). For multi-label classification (where multiple classes can be true), sigmoid is preferred.</li>
      <li>Softmax can be overly confident in predictions due to the exponential function, which may amplify small differences in logits.</li>
    </ul>
  </li>
</ul>

<h3 id="example-calculation">Example Calculation</h3>
<p>Suppose a neural network outputs logits \([1.5, 0.8, -0.2]\) for a 3-class problem:</p>
<ol>
  <li>Compute exponentials: \( e^{1.5} \approx 4.482, e^{0.8} \approx 2.225, e^{-0.2} \approx 0.819 \).</li>
  <li>Sum the exponentials: \( 4.482 + 2.225 + 0.819 = 7.526 \).</li>
  <li>Compute probabilities:
    <ul>
      <li>Class 1: \( \frac{4.482}{7.526} \approx 0.596 \)</li>
      <li>Class 2: \( \frac{2.225}{7.526} \approx 0.296 \)</li>
      <li>Class 3: \( \frac{0.819}{7.526} \approx 0.109 \)</li>
    </ul>
  </li>
  <li>Output: \([0.596, 0.296, 0.109]\), a valid probability distribution.</li>
</ol>

<h3 id="visualization-of-softmax">Visualization of Softmax</h3>
<p>To illustrate how Softmax transforms logits into probabilities, consider the following chart showing logits and their corresponding Softmax probabilities:</p>

<pre><code class="language-chartjs">{
  "type": "bar",
  "data": {
    "labels": ["Class 1", "Class 2", "Class 3"],
    "datasets": [
      {
        "label": "Logits",
        "data": [1.5, 0.8, -0.2],
        "backgroundColor": "rgba(75, 192, 192, 0.6)",
        "borderColor": "rgba(75, 192, 192, 1)",
        "borderWidth": 1
      },
      {
        "label": "Softmax Probabilities",
        "data": [0.596, 0.296, 0.109],
        "backgroundColor": "rgba(255, 99, 132, 0.6)",
        "borderColor": "rgba(255, 99, 132, 1)",
        "borderWidth": 1
      }
    ]
  },
  "options": {
    "scales": {
      "y": {
        "beginAtZero": true,
        "title": {
          "display": true,
          "text": "Value"
        }
      },
      "x": {
        "title": {
          "display": true,
          "text": "Classes"
        }
      }
    },
    "plugins": {
      "legend": {
        "display": true
      },
      "title": {
        "display": true,
        "text": "Logits vs. Softmax Probabilities"
      }
    }
  }
}
</code></pre>

<p>This chart compares the raw logits with the probabilities after applying Softmax, highlighting how Softmax normalizes the values into a probability distribution.</p>

<h3 id="summary">Summary</h3>
<p>The Softmax function is a cornerstone of multi-class classification in deep learning, transforming raw scores into a probability distribution. It is widely used in neural network output layers for tasks like image and text classification, enabling models to assign probabilities to multiple classes and facilitate training with loss functions like cross-entropy. Its exponential nature makes it sensitive to differences in logits, and careful implementation ensures numerical stability.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
