<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>TinyGrad Lightweight ML Library</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>TinyGrad Lightweight ML Library | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="TinyGrad Lightweight ML Library" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-05-31-tinygrad-ml-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-05-31-tinygrad-ml-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TinyGrad Lightweight ML Library" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"TinyGrad Lightweight ML Library","url":"https://lzwjava.github.io/notes/2025-05-31-tinygrad-ml-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=a4d27634f8c10e50ca5e84b73005cabee07ef762">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=a4d27634f8c10e50ca5e84b73005cabee07ef762" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       TinyGrad Lightweight ML Library | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-05-31-tinygrad-ml-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-05-31-tinygrad-ml-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/tinygrad-ml-en" selected>English</option>
        <option value="/tinygrad-ml-zh" >中文</option>
        <option value="/tinygrad-ml-ja" >日本語</option>
        <option value="/tinygrad-ml-es" >Español</option>
        <option value="/tinygrad-ml-hi" >हिंदी</option>
        <option value="/tinygrad-ml-fr" >Français</option>
        <option value="/tinygrad-ml-de" >Deutsch</option>
        <option value="/tinygrad-ml-ar" >العربية</option>
        <option value="/tinygrad-ml-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <h3 id="introduction-to-tinygrad">Introduction to TinyGrad</h3>
<p>TinyGrad is a lightweight, minimalist machine learning and deep learning library in Python, designed for education and simplicity. It implements automatic differentiation (autograd) like PyTorch but is much smaller—around 1,000 lines of code. It’s great for learning ML concepts or building small models without the overhead of larger libraries. It supports tensors, neural networks, and basic operations, including GPU acceleration via PyTorch or Metal.</p>

<p>You can find the official repository at: <a href="https://github.com/geohot/tinygrad">tinygrad GitHub</a>. Note: It’s experimental and not as robust as PyTorch or TensorFlow for production use.</p>

<h3 id="installation">Installation</h3>
<p>Install TinyGrad via pip:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tinygrad
</code></pre></div></div>

<p>It has minimal dependencies but optionally uses PyTorch for some backends. For GPU support, ensure you have PyTorch installed.</p>

<h3 id="basic-usage">Basic Usage</h3>
<p>Start by importing and setting the context (TinyGrad requires specifying if you’re training or inferring, as gradients are computed differently).</p>

<h4 id="importing-and-context">Importing and Context</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tinygrad</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">tinygrad.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">BatchNorm2d</span>  <span class="c1"># For neural nets
</span>
<span class="c1"># Set context: training (for gradients) or inference
</span><span class="n">Tensor</span><span class="p">.</span><span class="n">training</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># Enable gradient tracking
</span></code></pre></div></div>

<h4 id="creating-and-manipulating-tensors">Creating and Manipulating Tensors</h4>
<p>Tensors are the core data structure, similar to NumPy arrays or PyTorch tensors.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create tensors from lists, NumPy arrays, or by shape
</span><span class="n">a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>          <span class="c1"># From list
</span><span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>            <span class="c1"># Zeros tensor of shape (3,)
</span><span class="n">c</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>          <span class="c1"># Random tensor of shape (2, 3)
</span>
<span class="c1"># Basic operations
</span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>                      <span class="c1"># Element-wise addition
</span><span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">2</span>                      <span class="c1"># Scalar multiplication
</span><span class="n">f</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>  <span class="c1"># Matrix multiplication (a is 1D, transposed implicitly)
</span>
<span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>               <span class="c1"># Convert to NumPy for printing or further use
</span></code></pre></div></div>

<h4 id="automatic-differentiation-backpropagation">Automatic Differentiation (Backpropagation)</h4>
<p>TinyGrad automatically computes gradients using the chain rule.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Enable gradient tracking
</span><span class="n">Tensor</span><span class="p">.</span><span class="n">training</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>             <span class="c1"># Some operation; y is a scalar
</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>                  <span class="c1"># Compute gradients
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>         <span class="c1"># Gradients w.r.t. x: should be [2, 2, 2]
</span></code></pre></div></div>

<p>For exporting to NumPy, use <code class="language-plaintext highlighter-rouge">.numpy()</code>—gradients accumulate unless reset.</p>

<h4 id="neural-networks-and-training">Neural Networks and Training</h4>
<p>TinyGrad includes basic layers and optimizers. Here’s a simple MLP example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tinygrad.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">optim</span>

<span class="c1"># Define a simple model (e.g., linear layer)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>          <span class="c1"># Input 3, output 1
</span>
<span class="c1"># Dummy data
</span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>         <span class="c1"># Batch of 4 samples, 3 features
</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># Target
</span>
<span class="c1"># Forward pass
</span><span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">sigmoid</span><span class="p">()</span>      <span class="c1"># Assuming binary classification
</span>
<span class="c1"># Loss (e.g., MSE)
</span><span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Backprop and optimize
</span><span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">model</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">).</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>For convolution networks, use <code class="language-plaintext highlighter-rouge">Conv2d</code> from <code class="language-plaintext highlighter-rouge">tinygrad.nn</code>.</p>

<h3 id="advanced-features">Advanced Features</h3>
<ul>
  <li><strong>Loss Functions and Activations</strong>: Available in <code class="language-plaintext highlighter-rouge">tinygrad.nn</code> (e.g., <code class="language-plaintext highlighter-rouge">sigmoid</code>, <code class="language-plaintext highlighter-rouge">relu</code>, <code class="language-plaintext highlighter-rouge">cross_entropy</code>).</li>
  <li><strong>Optimizers</strong>: <code class="language-plaintext highlighter-rouge">SGD</code>, <code class="language-plaintext highlighter-rouge">Adam</code> in <code class="language-plaintext highlighter-rouge">tinygrad.nn.optim</code>.</li>
  <li><strong>Layers</strong>: <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">Conv2d</code>, <code class="language-plaintext highlighter-rouge">BatchNorm</code>, etc.</li>
  <li><strong>Saving/Loading</strong>: Models can be saved as state dictionaries (similar to PyTorch).</li>
  <li><strong>GPU/Acceleration</strong>: TinyGrad can run on GPU via PyTorch backend: <code class="language-plaintext highlighter-rouge">TESOR_SET_DEVICE='cuda:0'</code>. It also supports Metal on macOS.</li>
  <li><strong>Vision/Denoising Examples</strong>: The repo has examples like training a ResNet on MNIST.</li>
</ul>

<p>For full examples, check the <code class="language-plaintext highlighter-rouge">examples/</code> folder in the GitHub repo, such as image classification or reinforcement learning.</p>

<h3 id="example-training-a-neural-network">Example: Training a Neural Network</h3>
<p>Here’s a complete, simple script for training a network on XOR:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tinygrad</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">tinygrad.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">optim</span>

<span class="c1"># Data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">model</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">Tensor</span><span class="p">.</span><span class="n">training</span> <span class="o">=</span> <span class="bp">True</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="n">sigmoid</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Test
</span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]])))</span>  <span class="c1"># Should be ~0
</span></code></pre></div></div>

<h3 id="tips-and-caveats">Tips and Caveats</h3>
<ul>
  <li><strong>Mode Switching</strong>: Always set <code class="language-plaintext highlighter-rouge">Tensor.training = True</code> for training; switch to <code class="language-plaintext highlighter-rouge">False</code> for inference to avoid gradient computation.</li>
  <li><strong>Shape Handling</strong>: Tensors follow NumPy-like broadcasting, but watch for overflows in small models.</li>
  <li><strong>Debugging</strong>: Use <code class="language-plaintext highlighter-rouge">.numpy()</code> to inspect values. If something fails, check for shape mismatches.</li>
  <li><strong>Performance</strong>: It’s slow for large models—use PyTorch for production.</li>
  <li><strong>Updates</strong>: TinyGrad evolves quickly; check the repo for the latest version.</li>
</ul>

<p>For more details, visit the <a href="https://github.com/geohot/tinygrad/blob/master/README.md">docs/README.md</a> or run examples locally. If you have a specific use case, provide more details for tailored advice!</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    
      <p class="model">x-ai/grok-code-fast-1</p>
    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
