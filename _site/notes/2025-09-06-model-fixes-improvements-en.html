<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Model Shape Fixes and Improvements</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model Shape Fixes and Improvements | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Model Shape Fixes and Improvements" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-09-06-model-fixes-improvements-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-09-06-model-fixes-improvements-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-06T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Model Shape Fixes and Improvements" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zhiwei Li"},"dateModified":"2025-09-06T00:00:00+08:00","datePublished":"2025-09-06T00:00:00+08:00","description":"李智维","headline":"Model Shape Fixes and Improvements","mainEntityOfPage":{"@type":"WebPage","@id":"https://lzwjava.github.io/notes/2025-09-06-model-fixes-improvements-en"},"url":"https://lzwjava.github.io/notes/2025-09-06-model-fixes-improvements-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=4925afb30c6847378c6da8dff3fa09459fa6a358">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=4925afb30c6847378c6da8dff3fa09459fa6a358" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Model Shape Fixes and Improvements | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-09-06-model-fixes-improvements-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-09-06-model-fixes-improvements-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/model-fixes-improvements-en" selected>English</option>
        <option value="/model-fixes-improvements-zh" >中文</option>
        <option value="/model-fixes-improvements-ja" >日本語</option>
        <option value="/model-fixes-improvements-es" >Español</option>
        <option value="/model-fixes-improvements-hi" >हिंदी</option>
        <option value="/model-fixes-improvements-fr" >Français</option>
        <option value="/model-fixes-improvements-de" >Deutsch</option>
        <option value="/model-fixes-improvements-ar" >العربية</option>
        <option value="/model-fixes-improvements-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>Here’s what your script is doing, piece by piece, plus a couple of important fixes you’ll likely want to make.</p>

<h1 id="what-the-model-is">What the model is</h1>

<ul>
  <li>
    <p>You define a tiny <strong>character/word-level next-token predictor</strong>:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">nn.LSTM(input_size=vocab_size, hidden_size=50, num_layers=1)</code> processes a sequence of one-hot vectors (size = <code class="language-plaintext highlighter-rouge">vocab_size</code>) and produces a hidden state of size 50 at each time step.</li>
      <li><code class="language-plaintext highlighter-rouge">nn.Linear(50, vocab_size, bias=False)</code> maps each time step’s hidden state to <strong>logits over the vocabulary</strong>.</li>
    </ul>
  </li>
</ul>

<h1 id="expected-tensor-shapes-critical">Expected tensor shapes (critical)</h1>

<p>PyTorch LSTM’s default input shape is <strong>(seq_len, batch_size, input_size)</strong> because you didn’t set <code class="language-plaintext highlighter-rouge">batch_first=True</code>.</p>

<ul>
  <li>
    <p>For a single sequence in a batch of 1:</p>

    <ul>
      <li>Inputs to LSTM should be <code class="language-plaintext highlighter-rouge">(seq_len, 1, vocab_size)</code>.</li>
      <li>LSTM outputs <code class="language-plaintext highlighter-rouge">x</code> with shape <code class="language-plaintext highlighter-rouge">(seq_len, 1, hidden_size)</code>.</li>
      <li>You then <code class="language-plaintext highlighter-rouge">view(-1, hidden_size)</code> → <code class="language-plaintext highlighter-rouge">(seq_len, hidden_size)</code> so the linear layer produces <code class="language-plaintext highlighter-rouge">(seq_len, vocab_size)</code>.</li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">CrossEntropyLoss</code> expects:</p>

        <ul>
          <li><code class="language-plaintext highlighter-rouge">outputs</code>: <code class="language-plaintext highlighter-rouge">(N, C)</code> = <code class="language-plaintext highlighter-rouge">(seq_len, vocab_size)</code></li>
          <li><code class="language-plaintext highlighter-rouge">targets</code>: <code class="language-plaintext highlighter-rouge">(N,)</code> = <code class="language-plaintext highlighter-rouge">(seq_len,)</code> token indices</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="where-your-code-likely-goes-wrong-shape">Where your code likely goes wrong (shape)</h1>

<p>You do:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs_one_hot</span> <span class="o">=</span> <span class="n">inputs_one_hot</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>If <code class="language-plaintext highlighter-rouge">one_hot_encode_sequence</code> returns <code class="language-plaintext highlighter-rouge">(batch, seq_len, vocab)</code>, the correct permute for default LSTM is <strong><code class="language-plaintext highlighter-rouge">permute(1, 0, 2)</code></strong> (to get <code class="language-plaintext highlighter-rouge">(seq_len, batch, vocab)</code>). Your current <code class="language-plaintext highlighter-rouge">permute(0,2,1)</code> gives <code class="language-plaintext highlighter-rouge">(batch, vocab, seq_len)</code>, which is mismatched and will either error or silently learn nonsense if the earlier shape is different than assumed.</p>

<p>Two clean options (pick one):</p>

<p><strong>A) Keep default LSTM (batch_second):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># want (seq_len, batch, input_size)
</span><span class="n">inputs_one_hot</span> <span class="o">=</span> <span class="n">inputs_one_hot</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (seq_len, 1, vocab_size)
</span></code></pre></div></div>

<p><strong>B) Make it batch-first (often simpler):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># want (batch, seq_len, input_size)
# then DO NOT permute; leave as (1, seq_len, vocab_size)
</span></code></pre></div></div>

<h1 id="training-loop-logic">Training loop logic</h1>

<ul>
  <li>You keep two running sums per epoch: <code class="language-plaintext highlighter-rouge">epoch_training_loss</code> and <code class="language-plaintext highlighter-rouge">epoch_validation_loss</code>.</li>
  <li>
    <p><strong>Validation phase</strong> (<code class="language-plaintext highlighter-rouge">net.eval()</code> + no gradient steps):</p>

    <ul>
      <li>
        <p>For each (inputs, targets) in <code class="language-plaintext highlighter-rouge">validation_set</code>:</p>

        <ul>
          <li>one-hot encode inputs, convert targets to indices</li>
          <li>forward pass → logits <code class="language-plaintext highlighter-rouge">(seq_len, vocab_size)</code> (assuming shapes fixed)</li>
          <li>CE loss against target indices of shape <code class="language-plaintext highlighter-rouge">(seq_len,)</code></li>
          <li>accumulate loss</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Training phase</strong> (<code class="language-plaintext highlighter-rouge">net.train()</code>):</p>

    <ul>
      <li>Same preprocessing</li>
      <li>forward → loss → <code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> → <code class="language-plaintext highlighter-rouge">loss.backward()</code> → <code class="language-plaintext highlighter-rouge">optimizer.step()</code></li>
    </ul>
  </li>
  <li>You record average losses and print every 5 epochs.</li>
</ul>

<h1 id="inference--printout">Inference &amp; printout</h1>

<ul>
  <li>You pick a sample from <code class="language-plaintext highlighter-rouge">test_set</code>, run the model, get logits for each time step, take <code class="language-plaintext highlighter-rouge">argmax</code> per time step → predicted tokens.</li>
  <li>You print input words, target words, and predicted words.</li>
  <li>You plot training vs validation loss over epochs.</li>
</ul>

<h1 id="small-high-impact-improvements">Small, high-impact improvements</h1>

<ol>
  <li>
    <p><strong>Fix the input shape</strong> (most important). Use one of A or B above. If you choose batch-first, change the forward pass to:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>         <span class="c1"># x: (batch, seq, hidden)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># (batch*seq, hidden)
</span></code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Use <code class="language-plaintext highlighter-rouge">with torch.no_grad()</code> for eval/inference</strong> to avoid tracking gradients and for speed/memory:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs_one_hot</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets_idx</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>And for numpy conversion:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs_one_hot</span><span class="p">).</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Seed PyTorch too</strong> for reproducibility:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Device handling</strong> (if you use GPU):</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
<span class="n">net</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">inputs_one_hot</span> <span class="o">=</span> <span class="n">inputs_one_hot</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">targets_idx</span> <span class="o">=</span> <span class="n">targets_idx</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Target alignment</strong>: make sure <code class="language-plaintext highlighter-rouge">targets</code> are the <strong>next tokens</strong> aligned with each input time step. If your dataset pairs <code class="language-plaintext highlighter-rouge">(inputs, targets)</code> that way, you’re good. Otherwise, create <code class="language-plaintext highlighter-rouge">targets = inputs[1:]</code> and drop the last input or similar.</li>
  <li><strong>Drop <code class="language-plaintext highlighter-rouge">.bias=False</code> unless you have a reason</strong>; a bias on the output layer is usually harmless/helpful.</li>
  <li><strong>Padding/variable lengths</strong> (optional): if sequences have varying lengths, consider <code class="language-plaintext highlighter-rouge">pack_padded_sequence</code> to avoid training on padding tokens.</li>
  <li><strong>Monitoring</strong>: accuracy over the next-token predictions can be more interpretable than loss alone—compute <code class="language-plaintext highlighter-rouge">% correct argmax</code> on validation.</li>
</ol>

<h1 id="minimal-corrected-snippet-batch-first-version">Minimal corrected snippet (batch-first version)</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">l_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch, seq, vocab_size)
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                    <span class="c1"># (batch, seq, hidden)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># (batch*seq, hidden)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">l_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                      <span class="c1"># (batch*seq, vocab)
</span>        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># ... in your loop, remove the permute; ensure inputs_one_hot is (batch, seq, vocab)
# validation/inference use `with torch.no_grad()` and `.detach()`
</span></code></pre></div></div>

<p>If you share the exact shape that <code class="language-plaintext highlighter-rouge">one_hot_encode_sequence</code> returns (e.g., <code class="language-plaintext highlighter-rouge">(seq, vocab)</code> vs <code class="language-plaintext highlighter-rouge">(1, seq, vocab)</code>), I can tailor the permute/reshape lines precisely for your case.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
