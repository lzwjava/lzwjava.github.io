<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>vLLM: Fast LLM Inference Guide</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>vLLM: Fast LLM Inference Guide | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="vLLM: Fast LLM Inference Guide" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-10-21-vllm-inference-guide-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-10-21-vllm-inference-guide-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="vLLM: Fast LLM Inference Guide" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"vLLM: Fast LLM Inference Guide","url":"https://lzwjava.github.io/notes/2025-10-21-vllm-inference-guide-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=5592bdc6a776674fac55c39688916d90707b2273">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=5592bdc6a776674fac55c39688916d90707b2273" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       vLLM: Fast LLM Inference Guide | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-10-21-vllm-inference-guide-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-10-21-vllm-inference-guide-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/vllm-inference-guide-en" selected>English</option>
        <option value="/vllm-inference-guide-zh" >中文</option>
        <option value="/vllm-inference-guide-ja" >日本語</option>
        <option value="/vllm-inference-guide-es" >Español</option>
        <option value="/vllm-inference-guide-hi" >हिंदी</option>
        <option value="/vllm-inference-guide-fr" >Français</option>
        <option value="/vllm-inference-guide-de" >Deutsch</option>
        <option value="/vllm-inference-guide-ar" >العربية</option>
        <option value="/vllm-inference-guide-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>vLLM is an open-source library designed for fast and memory-efficient inference and serving of large language models (LLMs). Developed initially at the Sky Computing Lab at UC Berkeley, it has grown into a community-driven project with contributions from academia and industry. vLLM addresses key challenges in LLM deployment, such as high latency, memory fragmentation, and low throughput, making it ideal for production environments. It supports seamless integration with Hugging Face models and provides an OpenAI-compatible API for easy adoption.</p>

<h2 id="key-features">Key Features</h2>

<p>vLLM stands out for its performance and flexibility:</p>
<ul>
  <li><strong>PagedAttention</strong>: Efficiently manages key-value (KV) cache memory to reduce waste and enable higher throughput.</li>
  <li><strong>Continuous Batching</strong>: Dynamically batches incoming requests without waiting for full batches, improving resource utilization.</li>
  <li><strong>Optimized Kernels</strong>: Integrates FlashAttention, FlashInfer, and custom CUDA/HIP graphs for faster execution.</li>
  <li><strong>Quantization Support</strong>: Includes GPTQ, AWQ, INT4/INT8/FP8 for reduced memory footprint.</li>
  <li><strong>Decoding Algorithms</strong>: Supports parallel sampling, beam search, speculative decoding, and chunked prefill.</li>
  <li><strong>Distributed Inference</strong>: Tensor, pipeline, data, and expert parallelism for multi-GPU setups.</li>
  <li><strong>Hardware Compatibility</strong>: NVIDIA GPUs, AMD/Intel CPUs/GPUs, PowerPC CPUs, TPUs, and plugins for Intel Gaudi, IBM Spyre, Huawei Ascend.</li>
  <li><strong>Additional Tools</strong>: Streaming outputs, prefix caching, multi-LoRA support, and an OpenAI-compatible server.</li>
</ul>

<p>These features enable vLLM to achieve state-of-the-art serving throughput while being easy to use.</p>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li><strong>OS</strong>: Linux (primary support; some features on other platforms).</li>
  <li><strong>Python</strong>: 3.9 to 3.13.</li>
  <li><strong>Hardware</strong>: NVIDIA GPUs recommended for full features; CPU-only mode available but slower.</li>
  <li><strong>Dependencies</strong>: PyTorch (auto-detected via CUDA version), Hugging Face Transformers.</li>
</ul>

<h2 id="installation">Installation</h2>

<p>vLLM can be installed via pip. Use <code class="language-plaintext highlighter-rouge">uv</code> (a fast Python environment manager) for optimal setup:</p>

<ol>
  <li>Install <code class="language-plaintext highlighter-rouge">uv</code> following its <a href="https://docs.astral.sh/uv/getting-started/installation/">documentation</a>.</li>
  <li>
    <p>Create a virtual environment and install vLLM:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv venv --python 3.12 --seed
source .venv/bin/activate
uv pip install vllm --torch-backend=auto
</code></pre></div>    </div>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">--torch-backend=auto</code> auto-selects PyTorch based on your CUDA driver.</li>
      <li>For specific backends (e.g., CUDA 12.6): <code class="language-plaintext highlighter-rouge">--torch-backend=cu126</code>.</li>
    </ul>
  </li>
</ol>

<p>Alternatively, use <code class="language-plaintext highlighter-rouge">uv run</code> for one-off commands without a permanent environment:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   uv run --with vllm vllm --help
</code></pre></div></div>

<p>For Conda users:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   conda create -n myenv python=3.12 -y
   conda activate myenv
   pip install --upgrade uv
   uv pip install vllm --torch-backend=auto
</code></pre></div></div>

<p>For non-NVIDIA setups (e.g., AMD/Intel), refer to the <a href="https://docs.vllm.ai/en/stable/getting_started/installation.html">official installation guide</a> for platform-specific instructions, including CPU-only builds.</p>

<p>Attention backends (FLASH_ATTN, FLASHINFER, XFORMERS) are auto-selected; override with <code class="language-plaintext highlighter-rouge">VLLM_ATTENTION_BACKEND</code> environment variable if needed. Note: FlashInfer requires manual installation as it’s not in pre-built wheels.</p>

<h2 id="quick-start">Quick Start</h2>

<h3 id="offline-batched-inference">Offline Batched Inference</h3>

<p>Use vLLM for generating text from a list of prompts. Example script (<code class="language-plaintext highlighter-rouge">basic.py</code>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"Hello, my name is"</span><span class="p">,</span>
    <span class="s">"The president of the United States is"</span><span class="p">,</span>
    <span class="s">"The capital of France is"</span><span class="p">,</span>
    <span class="s">"The future of AI is"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">"facebook/opt-125m"</span><span class="p">)</span>  <span class="c1"># Downloads from Hugging Face by default
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Notes</strong>:
    <ul>
      <li>By default, uses model’s <code class="language-plaintext highlighter-rouge">generation_config.json</code> for sampling params; override with <code class="language-plaintext highlighter-rouge">generation_config="vllm"</code>.</li>
      <li>For chat/instruct models, apply chat templates manually or use <code class="language-plaintext highlighter-rouge">llm.chat(messages_list, sampling_params)</code>.</li>
      <li>Set <code class="language-plaintext highlighter-rouge">VLLM_USE_MODELSCOPE=True</code> for ModelScope models.</li>
    </ul>
  </li>
</ul>

<h3 id="online-serving-openai-compatible-api">Online Serving (OpenAI-Compatible API)</h3>

<p>Launch a server with:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vllm serve Qwen/Qwen2.5-1.5B-Instruct
</code></pre></div></div>

<p>This starts at <code class="language-plaintext highlighter-rouge">http://localhost:8000</code>. Customize with <code class="language-plaintext highlighter-rouge">--host</code> and <code class="language-plaintext highlighter-rouge">--port</code>.</p>

<p>Query via curl (completions endpoint):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'
</code></pre></div></div>

<p>Or chat completions:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"}
        ]
    }'
</code></pre></div></div>

<p>Using Python (OpenAI client):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">openai_api_key</span> <span class="o">=</span> <span class="s">"EMPTY"</span>
<span class="n">openai_api_base</span> <span class="o">=</span> <span class="s">"http://localhost:8000/v1"</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="n">openai_api_base</span><span class="p">)</span>

<span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s">"Qwen/Qwen2.5-1.5B-Instruct"</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s">"San Francisco is a"</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Completion result:"</span><span class="p">,</span> <span class="n">completion</span><span class="p">)</span>
</code></pre></div></div>

<p>Enable API key auth with <code class="language-plaintext highlighter-rouge">--api-key &lt;key&gt;</code> or <code class="language-plaintext highlighter-rouge">VLLM_API_KEY</code>.</p>

<h2 id="supported-models">Supported Models</h2>

<p>vLLM supports a vast array of generative and pooling models via native implementations or Hugging Face Transformers backend. Key categories include:</p>

<ul>
  <li><strong>Causal Language Models</strong>: Llama (3.1/3/2), Mistral, Gemma (2/3), Qwen, Phi (3/3.5), Mixtral, Falcon, BLOOM, GPT-NeoX/J/2, DeepSeek (V2/V3), InternLM (2/3), GLM (4/4.5), Command-R, DBRX, Yi, and more.</li>
  <li><strong>Mixture-of-Experts (MoE)</strong>: Mixtral, DeepSeek-V2/V3 MoE variants, Granite MoE.</li>
  <li><strong>Multimodal</strong>: LLaVA (1.5/1.6/Next), Phi-3-Vision, Qwen2-VL, InternVL2, CogVLM, Llama-3.2-Vision.</li>
  <li><strong>Vision-Language</strong>: CLIP, SigLIP (pooling/embedding).</li>
  <li><strong>Other</strong>: Encoder-decoder (T5, BART), diffusion models (Stable Diffusion), and custom architectures like Jamba, GritLM.</li>
</ul>

<p>Full support includes LoRA adapters, pipeline parallelism (PP), and V1 engine compatibility for most. For the complete list (over 100 architectures), see the <a href="https://docs.vllm.ai/en/stable/models/supported_models.html">supported models documentation</a>. Custom models can be integrated with minimal changes.</p>

<h2 id="deployment-options">Deployment Options</h2>

<h3 id="docker-deployment">Docker Deployment</h3>

<p>Use the official <code class="language-plaintext highlighter-rouge">vllm/vllm-openai</code> image for easy serving:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-1.5B-Instruct
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">--ipc=host</code> or <code class="language-plaintext highlighter-rouge">--shm-size=8g</code> for shared memory in multi-GPU setups.</li>
  <li>Supports Podman similarly.</li>
  <li>
    <p>For custom images: Build from source using <code class="language-plaintext highlighter-rouge">docker/Dockerfile</code> with BuildKit:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DOCKER_BUILDKIT=1 docker build . --target vllm-openai --tag vllm/custom --file docker/Dockerfile
</code></pre></div>    </div>
  </li>
  <li>Arm64/aarch64 builds: Use <code class="language-plaintext highlighter-rouge">--platform linux/arm64</code> (experimental; requires PyTorch Nightly).</li>
  <li>Add optional deps (e.g., audio) or Transformers from source in a custom Dockerfile.</li>
</ul>

<p>Other options include Kubernetes, AWS SageMaker, or direct integration with frameworks like Ray Serve.</p>

<h2 id="performance-tuning">Performance Tuning</h2>

<p>To optimize throughput and latency:</p>

<ul>
  <li><strong>GPU Selection</strong>: Use A100/H100 for high throughput; scale with tensor parallelism (<code class="language-plaintext highlighter-rouge">--tensor-parallel-size</code>).</li>
  <li><strong>Batch Size</strong>: Set <code class="language-plaintext highlighter-rouge">--max-num-seqs</code> and <code class="language-plaintext highlighter-rouge">--max-model-len</code> based on KV cache; aim for 80-90% GPU utilization.</li>
  <li><strong>Quantization</strong>: Enable AWQ/GPTQ (<code class="language-plaintext highlighter-rouge">--quantization awq</code>) to fit larger models.</li>
  <li><strong>Attention Backend</strong>: Prefer FlashInfer for newer GPUs; test with <code class="language-plaintext highlighter-rouge">VLLM_ATTENTION_BACKEND=FLASHINFER</code>.</li>
  <li><strong>Prefill/Decode Balance</strong>: Use <code class="language-plaintext highlighter-rouge">--chunked-prefill-size</code> for long inputs.</li>
  <li><strong>Benchmarking</strong>: Run <code class="language-plaintext highlighter-rouge">vllm benchmark</code> or ShareGPT datasets to measure TTFT (time-to-first-token) and TPOT (time-per-output-token).</li>
  <li><strong>Best Practices</strong>:
    <ul>
      <li>Monitor with Prometheus/Grafana.</li>
      <li>For TPUs: Use JAX backend; tune sharding.</li>
      <li>Avoid over-parallelism; start with single GPU and scale.</li>
    </ul>
  </li>
</ul>

<p>For detailed xPU configs, refer to hardware-specific guides.</p>

<h2 id="advanced-usage">Advanced Usage</h2>

<ul>
  <li><strong>Distributed Serving</strong>: <code class="language-plaintext highlighter-rouge">--tensor-parallel-size N</code> for multi-GPU; combine with Ray for clusters.</li>
  <li><strong>LoRA</strong>: Load adapters with <code class="language-plaintext highlighter-rouge">--lora-modules</code>.</li>
  <li><strong>Speculative Decoding</strong>: Integrate with Medusa for 2x speedups.</li>
  <li><strong>Custom Kernels</strong>: Extend via plugins for new hardware.</li>
  <li><strong>Monitoring</strong>: Enable <code class="language-plaintext highlighter-rouge">--enable-chunked-prefill</code> and log metrics.</li>
</ul>

<p>Explore the <a href="https://docs.vllm.ai/en/stable/usage/">user guide</a> for engines, engines, and integrations.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://docs.vllm.ai/">vLLM Official Documentation</a></li>
  <li><a href="https://docs.vllm.ai/en/stable/getting_started/quickstart.html">Quickstart Guide</a></li>
  <li><a href="https://docs.vllm.ai/en/stable/models/supported_models.html">Supported Models</a></li>
  <li><a href="https://docs.vllm.ai/en/stable/serving/deploying_with_docker.html">Docker Deployment</a></li>
  <li><a href="https://cloud.google.com/blog/topics/developers-practitioners/vllm-performance-tuning-the-ultimate-guide-to-xpu-inference-configuration">vLLM Performance Tuning Guide (Google Cloud)</a></li>
  <li><a href="https://www.newline.co/@zaoyang/ultimate-guide-to-vllm--aad8b65d">Ultimate Guide to vLLM (newline)</a></li>
  <li><a href="https://www.designveloper.com/blog/vllm-tutorial/">vLLM Tutorial for Beginners</a></li>
</ul>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    
      <p class="model">x-ai/grok-4-fast</p>
    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
