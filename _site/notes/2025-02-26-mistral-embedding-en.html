<!DOCTYPE html>
<html lang=" en-US">

<head>
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Zhiwei Li"
    href="https://lzwjava.github.io/feeds/feed.xml" />

  <title>Mistral AI Embeddings</title>

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' 
          data-cf-beacon='{"token": "70fc8c466cc1445098b3fc6f209c22c2"}'>
  </script>

  <!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66656236-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-66656236-1');
  </script>
   -->
  <meta charset="UTF-8">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Mistral AI Embeddings | Zhiwei Li</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Mistral AI Embeddings" />
<meta name="author" content="Zhiwei Li" />
<meta property="og:locale" content="en" />
<meta name="description" content="李智维" />
<meta property="og:description" content="李智维" />
<link rel="canonical" href="https://lzwjava.github.io/notes/2025-02-26-mistral-embedding-en" />
<meta property="og:url" content="https://lzwjava.github.io/notes/2025-02-26-mistral-embedding-en" />
<meta property="og:site_name" content="Zhiwei Li" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Mistral AI Embeddings" />
<meta name="twitter:site" content="@lzwjava" />
<meta name="twitter:creator" content="@lzwjava" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Zhiwei Li"},"description":"李智维","headline":"Mistral AI Embeddings","url":"https://lzwjava.github.io/notes/2025-02-26-mistral-embedding-en"}</script>
<!-- End Jekyll SEO tag -->


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- Facebook Meta Tags -->
  <!-- <meta property="og:url" content="https://lzwjava.github.io"> -->
  <meta property="og:type" content="website">
  <!-- <meta property="og:title" content="Zhiwei Li's Blog">
  <meta property="og:description" content="A personal blog featuring programming insights and projects."> -->
  
  
  
  
  
  
  
  
  <meta property="og:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@lzwjava">
  <meta property="twitter:domain" content="lzwjava.github.io">
  <!-- <meta property="twitter:url" content="https://lzwjava.github.io"> -->
  <!-- <meta name="twitter:title" content="Zhiwei Li's Blog">
  <meta name="twitter:description" content="A personal blog featuring programming insights and projects."> -->
  <meta name="twitter:image" content="https://lzwjava.github.io/assets/images/og/og4.jpg">


  <link rel="stylesheet" href="/assets/css/style.css?v=b02927194c03e0ffa5c9c53c506e757265427e0f">

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['\\(','\\)'], ['$', '$']],
            displayMath: [ ['$$','$$'], ['\\[','\\]']],
            processEscapes: false
          },
          "HTML-CSS": { linebreaks: { automatic: true } },
          "CommonHTML": {
            linebreaks: { automatic: true }
          },
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <!-- <script src="/assets/js/donatePopup.js?v=b02927194c03e0ffa5c9c53c506e757265427e0f" defer></script> -->
</head>

<body>
  <main id="content" class="main-content post-content" role="main">
  

  

  


  <div class="title-row post-title-row">
    <h2 class="title post-title">
       Mistral AI Embeddings | Generated by AI
    </h2>
  </div>

  <div class="button-container">
    <a href="/" class="button left-button">Home</a>

    <!-- PDF Button -->
     
    <!-- <a href="#" id="downloadPdfButton" class="button pdf-button" data-file-path="notes/2025-02-26-mistral-embedding-en.md">PDF</a> -->

    <!-- Audio Button -->



    <!--  -->

        <!-- Date Button -->
    
      
      <!-- <span>notes2025-02-26-mistral-embedding-en.md</span> -->
      

      <!-- <span></span> -->

      
        <a href="#" class="button">2025.10</a>
      
    

    <button id="themeTogglePost" class="button icon-button" aria-label="Toggle Theme" style="float: right;margin-bottom: 5px;">
      <!-- theme-icons.html -->
<svg id="sunIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

<svg id="moonIcon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor"
    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
    <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
</svg>
    </button>    

    <!-- Language Select Section -->
    
    
    
    
    
    
    

    
    <select id="languageSelect" class="button right-button">
        
        <option value="/mistral-embedding-en" selected>English</option>
        <option value="/mistral-embedding-zh" >中文</option>
        <option value="/mistral-embedding-ja" >日本語</option>
        <option value="/mistral-embedding-es" >Español</option>
        <option value="/mistral-embedding-hi" >हिंदी</option>
        <option value="/mistral-embedding-fr" >Français</option>
        <option value="/mistral-embedding-de" >Deutsch</option>
        <option value="/mistral-embedding-ar" >العربية</option>
        <option value="/mistral-embedding-hant" >繁體中文</option>
    </select>
  </div>

  <!-- Audio player for text-to-speech -->
  <div class="audio-container">
    <audio id="audioPlayer" controls loop style="display:none;">
      <source id="audioSource" src="" type="audio/mp3">
      Your browser does not support the audio element.
    </audio>
  </div>

  <hr>

  <p>To provide embedding examples using Mistral AI, you can leverage their embeddings API to generate numerical representations (embeddings) of text that capture semantic meaning. These embeddings are useful for tasks such as similarity detection, clustering, classification, or search. Below, I’ll demonstrate how to generate embeddings for a set of sentences and calculate their similarities using the Mistral AI embeddings endpoint. This example builds on the structure of the provided code but adapts it specifically for embeddings, as outlined in the documentation: <a href="https://docs.mistral.ai/capabilities/embeddings/">Mistral AI Embeddings</a>.</p>

<h3 id="prerequisites">Prerequisites</h3>

<p>Before running the example, ensure you have:</p>
<ol>
  <li><strong>API Key</strong>: A valid Mistral AI API key stored in the <code class="language-plaintext highlighter-rouge">MISTRAL_API_KEY</code> environment variable.</li>
  <li><strong>Dependencies</strong>: The required Python packages installed. You can install them with:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>requests numpy
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="example-code">Example Code</h3>

<p>Here’s a complete Python script that generates embeddings for three sentences and calculates their pairwise similarities:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">call_mistral_embeddings_api</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s">"mistral-embed"</span><span class="p">):</span>
    <span class="s">"""
    Call the Mistral AI embeddings API to generate embeddings for a list of texts.
    
    Args:
        texts (list): List of strings to embed.
        model (str): The embedding model to use (default: "mistral-embed").
    
    Returns:
        list: List of embedding vectors, or None if the request fails.
    """</span>
    <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"MISTRAL_API_KEY"</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Error: MISTRAL_API_KEY environment variable not set."</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
    
    <span class="n">url</span> <span class="o">=</span> <span class="s">"https://api.mistral.ai/v1/embeddings"</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"Content-Type"</span><span class="p">:</span> <span class="s">"application/json"</span><span class="p">,</span>
        <span class="s">"Accept"</span><span class="p">:</span> <span class="s">"application/json"</span><span class="p">,</span>
        <span class="s">"Authorization"</span><span class="p">:</span> <span class="sa">f</span><span class="s">"Bearer </span><span class="si">{</span><span class="n">api_key</span><span class="si">}</span><span class="s">"</span>
    <span class="p">}</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"model"</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="s">"input"</span><span class="p">:</span> <span class="n">texts</span>
    <span class="p">}</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">response</span><span class="p">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="n">response_json</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">json</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">response_json</span> <span class="ow">and</span> <span class="s">"data"</span> <span class="ow">in</span> <span class="n">response_json</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="s">"embedding"</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">response_json</span><span class="p">[</span><span class="s">"data"</span><span class="p">]]</span>
            <span class="k">return</span> <span class="n">embeddings</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Mistral Embeddings API Error: Invalid response format: </span><span class="si">{</span><span class="n">response_json</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">None</span>
    <span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Mistral Embeddings API Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">e</span><span class="p">.</span><span class="n">response</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Response status code: </span><span class="si">{</span><span class="n">e</span><span class="p">.</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Response content: </span><span class="si">{</span><span class="n">e</span><span class="p">.</span><span class="n">response</span><span class="p">.</span><span class="n">text</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">calculate_similarity</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">):</span>
    <span class="s">"""
    Calculate the similarity between two embeddings using dot product.
    
    Args:
        emb1 (list): First embedding vector.
        emb2 (list): Second embedding vector.
    
    Returns:
        float: Similarity score (dot product, equivalent to cosine similarity for normalized vectors).
    """</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="c1"># Example texts to embed
</span>    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"I love programming in Python."</span><span class="p">,</span>
        <span class="s">"Python is a great programming language."</span><span class="p">,</span>
        <span class="s">"The weather is sunny today."</span>
    <span class="p">]</span>
    
    <span class="c1"># Generate embeddings
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">call_mistral_embeddings_api</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embeddings</span><span class="p">:</span>
        <span class="c1"># Print embedding dimension
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Embedding dimension: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        
        <span class="c1"># Calculate pairwise similarities
</span>        <span class="n">sim_12</span> <span class="o">=</span> <span class="n">calculate_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">sim_13</span> <span class="o">=</span> <span class="n">calculate_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">sim_23</span> <span class="o">=</span> <span class="n">calculate_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        
        <span class="c1"># Display results
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Similarity Results:"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Text 1: '</span><span class="si">{</span><span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Text 2: '</span><span class="si">{</span><span class="n">texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Text 3: '</span><span class="si">{</span><span class="n">texts</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Similarity between Text 1 and Text 2: </span><span class="si">{</span><span class="n">sim_12</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similarity between Text 1 and Text 3: </span><span class="si">{</span><span class="n">sim_13</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similarity between Text 2 and Text 3: </span><span class="si">{</span><span class="n">sim_23</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="how-to-run">How to Run</h3>

<ol>
  <li><strong>Set the API Key</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">MISTRAL_API_KEY</span><span class="o">=</span><span class="s2">"your_api_key_here"</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Save and Execute</strong>:
Save the script (e.g., as <code class="language-plaintext highlighter-rouge">embedding_example.py</code>) and run it:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python embedding_example.py
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="expected-output">Expected Output</h3>

<p>Assuming the API call succeeds, you’ll see output like this (exact values depend on the embeddings returned):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Embedding dimension: 1024

Similarity Results:
Text 1: 'I love programming in Python.'
Text 2: 'Python is a great programming language.'
Text 3: 'The weather is sunny today.'

Similarity between Text 1 and Text 2: 0.9200
Similarity between Text 1 and Text 3: 0.6500
Similarity between Text 2 and Text 3: 0.6700
</code></pre></div></div>

<h3 id="explanation">Explanation</h3>

<ul>
  <li>
    <p><strong>API Endpoint</strong>: The <code class="language-plaintext highlighter-rouge">call_mistral_embeddings_api</code> function sends a POST request to <code class="language-plaintext highlighter-rouge">https://api.mistral.ai/v1/embeddings</code>, passing a list of texts and the <code class="language-plaintext highlighter-rouge">"mistral-embed"</code> model. The API returns a JSON response containing embeddings under the <code class="language-plaintext highlighter-rouge">"data"</code> key.</p>
  </li>
  <li>
    <p><strong>Embeddings</strong>: Each embedding is a 1024-dimensional vector (as per Mistral’s documentation), representing the semantic content of the input text. The embeddings are normalized to a norm of 1.</p>
  </li>
  <li>
    <p><strong>Similarity Calculation</strong>: Since the embeddings are normalized, the dot product (<code class="language-plaintext highlighter-rouge">np.dot</code>) between two embeddings equals their cosine similarity. Higher values indicate greater semantic similarity:</p>
    <ul>
      <li><strong>Text 1 and Text 2</strong>: Both are about programming in Python, so their similarity should be high (e.g., ~0.92).</li>
      <li><strong>Text 1 and Text 3</strong>: One is about programming, the other about weather, so their similarity should be lower (e.g., ~0.65).</li>
      <li><strong>Text 2 and Text 3</strong>: Similar pattern, lower similarity due to differing topics.</li>
    </ul>
  </li>
</ul>

<h3 id="additional-use-cases">Additional Use Cases</h3>

<p>Embeddings from Mistral AI can be applied beyond similarity detection:</p>
<ul>
  <li><strong>Clustering</strong>: Group similar texts using algorithms like K-means (requires <code class="language-plaintext highlighter-rouge">scikit-learn</code>).
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Search</strong>: Embed a query and find the most similar text:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query</span> <span class="o">=</span> <span class="s">"Tell me about Python."</span>
<span class="n">query_emb</span> <span class="o">=</span> <span class="n">call_mistral_embeddings_api</span><span class="p">([</span><span class="n">query</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="p">[</span><span class="n">calculate_similarity</span><span class="p">(</span><span class="n">query_emb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span> <span class="k">for</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">]</span>
<span class="n">most_similar</span> <span class="o">=</span> <span class="n">texts</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">similarities</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Most similar text: '</span><span class="si">{</span><span class="n">most_similar</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Classification</strong>: Use embeddings as features in a machine learning model.</li>
</ul>

<h3 id="notes">Notes</h3>

<ul>
  <li><strong>Model</strong>: The <code class="language-plaintext highlighter-rouge">"mistral-embed"</code> model is optimized for embeddings. Check the <a href="https://docs.mistral.ai/capabilities/embeddings/">documentation</a> for updates or additional models.</li>
  <li><strong>Batching</strong>: The API processes multiple texts in one call, improving efficiency.</li>
  <li><strong>Advanced Applications</strong>: For large-scale similarity search, integrate with vector databases like Faiss or Milvus.</li>
</ul>

<p>This example provides a practical introduction to using Mistral AI embeddings, adaptable to the structure of the provided chat completion code by changing the endpoint and data format.</p>


  <hr>

  <div class="button-container">
    <a href="/" class="button left-button">Back</a>


    

    
    
    <a href="/donate-en" class="button right-button">Donate</a>
  </div>
</main>

<script src="/assets/js/dark-mode.js"></script>
<script src="/assets/js/audio.js" defer></script>
<script src="/assets/js/pdf.js" defer></script>
<script>
    document.getElementById('languageSelect').addEventListener('change', function() {
        var selectedValue = this.value;
        if (selectedValue) {
            window.location.href = selectedValue;
        }
    });
</script>

</body>

</html>
